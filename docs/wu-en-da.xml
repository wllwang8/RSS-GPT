<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
<channel>
<title>吴恩达的知乎动态</title>
<link>https://www.zhihu.com/people/wu-en-da-89/activities</link>

<item>
<title>吴恩达发表了文章: 吴恩达来信：编写更优提示</title>
<link>https://zhuanlan.zhihu.com/p/698748054</link>
<guid>https://zhuanlan.zhihu.com/p/698748054</guid>
<content:encoded><![CDATA[
<div> 关键词: Google, OpenAI, GPT-4o, 输入上下文窗口, 多模态token

总结:<br><br>文章介绍了近期Google和OpenAI的最新进展，包括Gemini Pro 1.5输入上下文窗口扩大到2百万tokens，以及OpenAI发布的GPT-4o，能够更快地生成tokens和接受多模态token。文章提到开发人员在构建应用程序时需要更详细的提示和更长的输入上下文窗口。作者建议通过逐步迭代的方式完善提示，及时根据输出结果调整提示内容。此外，文章还介绍了一些新的短期课程，帮助开发人员学习如何构建多智能体系统和多模态搜索。 <div>
<p>Dear friends,</p><p>In the last couple of days, Google announced a doubling of Gemini Pro 1.5's input context window from 1 million to 2 million tokens, and OpenAI released GPT-4o, which generates tokens 2x faster and 50% cheaper than GPT-4 Turbo and natively accepts and generates multimodal tokens. I view these developments as the latest in an 18-month trend. Given the improvements we've seen, best practices for developers have changed as well.</p><p>Since the launch of ChatGPT in November 2022, with key milestones that include the releases of GPT-4, Gemini 1.5 Pro, Claude 3 Opus, and Llama 3-70B, many model providers have improved their capabilities in two important ways: (i) reasoning, which allows LLMs to think through complex concepts and and follow complex instructions; and (ii) longer input context windows. </p><p>The reasoning capability of GPT-4 and other advanced models makes them quite good at interpreting complex prompts with detailed instructions. Many people are used to dashing off a quick, 1- to 2-sentence query to an LLM. In contrast, when building applications, I see sophisticated teams frequently writing prompts that might be 1 to 2 pages long (my teams call them “mega-prompts”) that provide complex instructions to specify in detail how we’d like an LLM to perform a task. I still see teams not going far enough in terms of writing detailed instructions. For an example of a moderately lengthy prompt, check out <a class=" wrap external" href="https://twitter.com/AmandaAskell/status/1765207842993434880?utm_campaign=The%20Batch&amp;utm_source=hs_email&amp;utm_medium=email&amp;_hsenc=p2ANqtz-_PU4gmbfJN9_gBrzLMkZheDB1ROQnQWYv9cSxeMK53CO9ix0aYRLcabOd6v3xmmbHcM7HE" rel="nofollow noreferrer" target="_blank">Claude 3’s system prompt</a>. It’s detailed and gives clear guidance on how Claude should behave.</p><p>This is a very different style of prompting than we typically use with LLMs’ web user interfaces, where we might dash off a quick query and, if the response is unsatisfactory, clarify what we want through repeated conversational turns with the chatbot.</p><p>Further, the increasing length of input context windows has added another technique to the developer’s toolkit. GPT-3 kicked off a lot of research on few-shot in-context learning. For example, if you’re using an LLM for text classification, you might give a handful — say 1 to 5 examples — of text snippets and their class labels, so that it can use those examples to generalize to additional texts. However, with longer input context windows — GPT-4o accepts 128,000 input tokens, Claude 3 Opus 200,000 tokens, and Gemini 1.5 Pro 1 million tokens (2 million just announced in a limited preview) — LLMs aren’t limited to a handful of examples. With <a class=" wrap external" href="https://arxiv.org/abs/2404.11018?utm_campaign=The%20Batch&amp;utm_source=hs_email&amp;utm_medium=email&amp;_hsenc=p2ANqtz-_PU4gmbfJN9_gBrzLMkZheDB1ROQnQWYv9cSxeMK53CO9ix0aYRLcabOd6v3xmmbHcM7HE" rel="nofollow noreferrer" target="_blank">many-shot learning</a>, developers can give dozens, even hundreds of examples in the prompt, and this works better than few-shot learning. </p><p>When building complex workflows, I see developers getting good results with this process: </p><ul><li>Write quick, simple prompts and see how it does.</li><li>Based on where the output falls short, flesh out the prompt iteratively. This often leads to a longer, more detailed, prompt, perhaps even a mega-prompt.</li><li>If that’s still insufficient, consider few-shot or many-shot learning (if applicable) or, less frequently, fine-tuning.</li><li>If that still doesn’t yield the results you need, break down the task into subtasks and apply an agentic workflow.</li></ul><p>I hope a process like this will help you build applications more easily. If you’re interested in taking a deeper dive into prompting strategies, I recommend the Medprompt <a class=" wrap external" href="https://arxiv.org/abs/2311.16452?utm_campaign=The%20Batch&amp;utm_source=hs_email&amp;utm_medium=email&amp;_hsenc=p2ANqtz-_PU4gmbfJN9_gBrzLMkZheDB1ROQnQWYv9cSxeMK53CO9ix0aYRLcabOd6v3xmmbHcM7HE" rel="nofollow noreferrer" target="_blank">paper</a>, which lays out a complex set of prompting strategies that can lead to very good results.</p><p>Keep learning!</p><p>Andrew </p><p>P.S. Two new short courses:</p><ul><li>“Multi AI Agent Systems with crewAI” taught by crewAI Founder and CEO João Moura: Learn to take a complex task and break it into subtasks for a team of specialized agents. You’ll learn how to design agent roles, goals, and tool sets, and decide how the agents collaborate (such as which agents can delegate to other agents). You'll see how a multi-agent system can carry out research, write an article, perform financial analysis, or plan an event. Architecting multi-agent systems requires a new mode of thinking that's more like managing a team than chatting with LLMs. <a class=" wrap external" href="https://www.deeplearning.ai/short-courses/ai-agent-workflows-with-crewai/?utm_campaign=The%20Batch&amp;utm_source=hs_email&amp;utm_medium=email&amp;_hsenc=p2ANqtz-_PU4gmbfJN9_gBrzLMkZheDB1ROQnQWYv9cSxeMK53CO9ix0aYRLcabOd6v3xmmbHcM7HE" rel="nofollow noreferrer" target="_blank">Sign up here!</a></li><li>“Building Multimodal Search and RAG” taught by Weaviate's Sebastian Witalec: In this course, you'll create RAG systems that reason over contextual information across text, images and video. You will learn how to train multimodal embedding models to map similar data to nearby vectors, so as to carry out semantic search across multiple modalities, and learn about visual instruction tuning to add image capabilities to large language models. <a class=" wrap external" href="https://www.deeplearning.ai/short-courses/building-multimodal-search-and-rag/?utm_campaign=The%20Batch&amp;utm_source=hs_email&amp;utm_medium=email&amp;_hsenc=p2ANqtz-_PU4gmbfJN9_gBrzLMkZheDB1ROQnQWYv9cSxeMK53CO9ix0aYRLcabOd6v3xmmbHcM7HE" rel="nofollow noreferrer" target="_blank">Sign up here!</a></li></ul><hr /><p>亲爱的朋友们,<br /><br />在过去的几天里，谷歌宣布将Gemini Pro 1.5的输入上下文窗口从100万增加到200万；OpenAI发布了GPT - 4o，它生成token的速度比GPT-4 Turbo快2倍，价格却便宜50%，并且可以在本地接受和生成多模式token。我认为这些是近18个月发展趋势中的最新进展。鉴于我们所看到的改进，开发人员的最佳实践也发生了变化。<br /><br />自ChatGPT于2022年11月推出以来，包括发布GPT-4, Gemini 1.5 Pro, Claude 3 Opus和Llama 3- 70b在内的关键里程碑，许多模型提供商在两个重要方面提高了他们的能力:(i)推理，这使得LLM能够思考复杂的概念并遵循复杂的指令；(ii)更长的输入上下文窗口。<br /><br />GPT-4和其他高级模型的推理能力使它们能够很好地解释带有详细说明的复杂提示。许多人习惯于匆匆向LLM提出一两句简短的问题。相反，在构建应用程序时，我看到经验丰富的团队经常编写可能长达1到2页的提示（我的团队称之为“超级提示”），这些提示提供了复杂的指令，以详细指定我们希望LLM如何执行任务。我仍感觉团队在编写详细说明方面做得不够。<br /><br />这与我们通常在LLM的网络用户界面中使用的提示风格非常不同，在LLM的网络用户界面中，我们可能会匆忙进行快速查询，如果回答不令人满意，就会通过与聊天机器人的反复对话来澄清我们想要的是什么。<br /><br />此外，输入上下文长度的增加为开发人员的工具包增加了另一种技术。GPT-3开启了大量关于少量情境学习的研究。例如，如果你正在使用LLM进行文本分类，比如提供少量（例如1到5个样本）文本片段及其类标签，以便它可以使用这些样本泛化到其他文本。然而，随着更长的输入上下文窗口的出现——GPT - 4o接受128,000个输入token，Claude 3 Opus接受200,000个token，Gemini 1.5 Pro接受100万个token（能接受200万个token的版本刚刚在有限预览中宣布）——LLM不再局限于少数例样本。通过进行many-shot learning，开发人员可以在提示框中给出几十个，甚至几百个样本，这比使用few-shot learning学习更有效。<br /><br />当构建复杂的工作流时，我看到开发人员通过这个过程获得了很好的结果:<br />● 写一些快速、简单的提示，看看效果如何。<br />● 根据输出不足的地方，迭代地充实提示符。这通常会产生一个更长的、更详细的提示，甚至可能是一个超级提示。<br />● 如果这仍然不够，考虑few-shot 或 many-shot learning（如果适用），或者不太频繁的微调。<br />● 如果这仍然不能产生你需要的结果，那么将任务分解为子任务并应用一个智能体工作流。<br /><br />我希望这样的过程能够帮助各位更轻松地构建应用程序。如果你有兴趣深入了解提示策略，我推荐Medprompt的论文，它给出了一套复杂的提示策略，可以带来非常好的结果。<br /><br />请不断学习！<br />吴恩达<br /><br />两个新的短期课程:<br />● crewAI创始人兼首席执行官João Moura教授的“crewAI Multi AI Agent Systems with crewAI”：学会接受一项复杂的任务，并将其分解为一组专业智能体的子任务。你将学习如何设计智能体角色、目标和工具集，并决定智能体如何协作（例如哪些智能体可以委托给其他智能体）。你将看到多智能体系统如何进行研究、撰写文章、执行财务分析或计划事件。构建多智能体系统需要一种新的思维模式，这种模式更像是管理一个团队，而不是与LLM聊天。请点击下方“阅读愿望”注册学习！<br />● “Building Multimodal Search and RAG”由Weaviate的Sebastian Witalec教授：在本课程中，你将学习创建RAG系统，对文本、图像和视频的上下文信息进行推理。你将学习如何训练多模态嵌入模型，将相似的数据映射到附近的向量，从而跨多模态进行语义搜索，并学习视觉指令调优，以便为大型语言模型添加图像功能。<br />请<a class=" wrap external" href="https://www.deeplearning.ai/short-courses/building-multimodal-search-and-rag/?utm_campaign=The%20Batch&amp;utm_source=hs_email&amp;utm_medium=email&amp;_hsenc=p2ANqtz-_PU4gmbfJN9_gBrzLMkZheDB1ROQnQWYv9cSxeMK53CO9ix0aYRLcabOd6v3xmmbHcM7HE" rel="nofollow noreferrer" target="_blank">点</a>注册学习！<br /> </p>
]]></content:encoded>
<pubDate>Mon, 20 May 2024 04:18:17 GMT</pubDate>
<pubDate>Mon, 20 May 2024 04:18:17 GMT</pubDate>
</item>

<item>
<title>吴恩达发表了文章: 吴恩达来信：警惕针对开源的不良言论</title>
<link>https://zhuanlan.zhihu.com/p/696753428</link>
<guid>https://zhuanlan.zhihu.com/p/696753428</guid>
<content:encoded><![CDATA[
<div> 关键词: 开源社群, 人工智能监管, 国家安全, 科技创新, 短期课程

总结:<br /><br />本文讨论了在美国国会大厦的人工智能和监管事件中，开源社群在抵制限制创新方面取得的进展以及开源对国家安全的影响。随着对立论者的论点逐渐变化，人工智能的监管需求逐渐清晰，应着重关注人工智能应用的监管。作者呼吁全球各地的监管机构更好地理解人工智能，多进行对话。此外，作者宣布了两门新的短期课程，涉及智能体工作流和量化技术，为人们学习人工智能提供了新的机会。通过对话和教育，希望能够促进人工智能的发展并保护开源社群的利益。 <div>
<p>Dear friends,</p><p>Last week, I spoke about AI and regulation at the U.S. Capitol at an event that was attended by legislative and business leaders. I’m encouraged by the progress the open source community has made fending off regulations that would have stifled innovation. But opponents of open source are continuing to shift their arguments, with the latest worries centering on open source's impact on national security. I hope we’ll all keep protecting open source! </p><p>Based on my conversations with legislators, I’m encouraged by the progress the U.S. federal government has made getting a realistic grasp of AI’s risks. To be clear, guardrails are needed. But they should be applied to AI applications, not to general-purpose AI technology. </p><p>Nonetheless, as I <a class=" wrap external" href="https://www.deeplearning.ai/the-batch/keep-open-source-free/?utm_campaign=The%20Batch&amp;utm_source=hs_email&amp;utm_medium=email&amp;_hsenc=p2ANqtz-8LNS8DF2FAurlzNv-TFTZKbJ0jKgiLC0wmKt8MgCKBZQFvWmuJJuwXqSWNb-qAt3KNkO8m" rel="nofollow noreferrer" target="_blank">wrote</a> previously, some companies are eager to limit open source, possibly to protect the value of massive investments they’ve made in proprietary models and to deter competitors. It has been fascinating to watch their arguments change over time.</p><p>For instance, about 12 months ago, the Center For AI Safety’s “<a class=" wrap external" href="https://www.safe.ai/work/statement-on-ai-risk?utm_campaign=The%20Batch&amp;utm_source=hs_email&amp;utm_medium=email&amp;_hsenc=p2ANqtz-8LNS8DF2FAurlzNv-TFTZKbJ0jKgiLC0wmKt8MgCKBZQFvWmuJJuwXqSWNb-qAt3KNkO8m" rel="nofollow noreferrer" target="_blank">Statement on AI Risk</a>” warned that AI could cause human extinction and stoked fears of AI taking over. This alarmed leaders in Washington. But many people in AI pointed out that this dystopian science-fiction scenario has little basis in reality. About six months later, when I <a class=" wrap external" href="https://www.deeplearning.ai/the-batch/ai-doomsday-scenarios-and-how-to-guard-against-them/?utm_campaign=The%20Batch&amp;utm_source=hs_email&amp;utm_medium=email&amp;_hsenc=p2ANqtz-8LNS8DF2FAurlzNv-TFTZKbJ0jKgiLC0wmKt8MgCKBZQFvWmuJJuwXqSWNb-qAt3KNkO8m" rel="nofollow noreferrer" target="_blank">testified</a> at the U.S. Senate’s AI Insight forum, legislators no longer worried much about an AI takeover.</p><p>Then the opponents of open source shifted gears. Their leading argument shifted to the risk of AI helping to create bioweapons. Soon afterward, <a class=" wrap external" href="https://openai.com/index/building-an-early-warning-system-for-llm-aided-biological-threat-creation?utm_campaign=The%20Batch&amp;utm_source=hs_email&amp;utm_medium=email&amp;_hsenc=p2ANqtz-8LNS8DF2FAurlzNv-TFTZKbJ0jKgiLC0wmKt8MgCKBZQFvWmuJJuwXqSWNb-qAt3KNkO8m" rel="nofollow noreferrer" target="_blank">OpenAI</a> and <a class=" wrap external" href="https://www.rand.org/news/press/2024/01/25.html?utm_campaign=The%20Batch&amp;utm_source=hs_email&amp;utm_medium=email&amp;_hsenc=p2ANqtz-8LNS8DF2FAurlzNv-TFTZKbJ0jKgiLC0wmKt8MgCKBZQFvWmuJJuwXqSWNb-qAt3KNkO8m" rel="nofollow noreferrer" target="_blank">RAND</a> showed that current AI does not significantly increase the ability of malefactors to build bioweapons. This fear of AI-enabled bioweapons has diminished. To be sure, the possibility that bad actors could use bioweapons — with or without AI — remains a topic of great international concern. </p><p>The latest argument for blocking open source AI has shifted to national security. AI is useful for both economic competition and warfare, and open source opponents say the U.S. should make sure its adversaries don’t have access to the latest foundation models. While I don’t want authoritarian governments to use AI, particularly to wage unjust wars, the LLM cat is out of the bag, and authoritarian countries will fill the vacuum if democratic nations limit access. When, some day, a child somewhere asks an AI system questions about democracy, the role of a free press, or the function of an independent judiciary in preserving the rule of law, I would like the AI to reflect democratic values rather than favor authoritarian leaders’ goals over, say, human rights. </p><p>I came away from Washington optimistic about the progress we’ve made. A  year ago, legislators seemed to me to spend 80% of their time talking about guardrails for AI and 20% about investing in innovation. I was delighted that the ratio has flipped, and there was far more talk of investing in innovation.</p><p>Looking beyond the U.S. federal government, there are many jurisdictions globally. Unfortunately, arguments in favor of  regulations that would stifle AI development continue to proliferate. But I’ve learned from my trips to Washington and other nations’ capitals that talking to regulators does have an impact. If you get a chance to talk to a regulator at any level, I hope you’ll do what you can to help governments better understand AI. </p><p>Keep learning,<br />Andrew</p><p>P.S. Two new short courses!</p><ul><li>I’m thrilled to announce our first short course focused on agentic workflows: “<a class=" wrap external" href="https://www.deeplearning.ai/short-courses/building-agentic-rag-with-llamaindex/?utm_campaign=The%20Batch&amp;utm_source=hs_email&amp;utm_medium=email&amp;_hsenc=p2ANqtz-9rsco1t53WrNq8oTk-wX4HAtWVypFA6VFpC58cihdUR6gTq4FHeMdGGzcO3FCdxtzMHScF" rel="nofollow noreferrer" target="_blank">Building Agentic RAG with LlamaIndex</a>,” taught by LlamaIndex CEO Jerry Liu. This covers an important shift in RAG. Rather than having a developer write explicit routines to retrieve information to feed into an LLM’s context, we can build a RAG agent that has access to tools to retrieve information. This lets it decide what information to fetch, and lets it answer more complex questions using multi-step reasoning.</li><li>Additionally, I’m delighted to launch “<a class=" wrap external" href="https://www.deeplearning.ai/short-courses/quantization-in-depth/?utm_campaign=The%20Batch&amp;utm_source=hs_email&amp;utm_medium=email&amp;_hsenc=p2ANqtz-9rsco1t53WrNq8oTk-wX4HAtWVypFA6VFpC58cihdUR6gTq4FHeMdGGzcO3FCdxtzMHScF" rel="nofollow noreferrer" target="_blank">Quantization in Depth</a>,” taught by Hugging Face’s Marc Sun and Younes Belkada. Quantization is a key technique for making large models accessible. You’ll learn about implementing linear quantization variants, quantizing at different granularities, and compressing deep learning models to 8-bit and 2-bit precision.</li></ul><hr /><p>亲爱的朋友们,<br /><br />上周，我在美国国会大厦的一个活动上谈到了人工智能和监管，这项活动有立法和商业领袖参加。我对开源社群所取得的进步感到鼓舞，这些进步是为了抵制那些可能会扼杀创新的法规。但是，开源的反对者也在持续更新他们的观点，最近的担忧集中在开源对国家安全的影响上。我希望我们都能继续保护开源！<br /><br />根据我与立法者的对话，我对美国联邦政府在现实把握人工智能风险方面取得的进展感到鼓舞。需要明确的是，设置护栏是必要的。但它们应该被应用于人工智能应用，而不是通用的人工智能技术。<br /><br />尽管如此，正如我之前所写的，一些公司渴望限制开源，可能是为了保护他们在专有模型上进行的大量投资的价值，并阻挡竞争对手。看着他们的论点随着时间的推移而变化是一件很有趣的事情。<br /><br />例如，大约12个月前，人工智能安全中心(Center For AI Safety)发布的《人工智能风险声明》(Statement on AI Risk) 警告称，人工智能可能导致人类灭绝，并引发了人们对人工智能接管人类的恐惧。这给华盛顿的领导人敲响了警钟。但许多人工智能领域的从业人士指出，这种反乌托邦式的科幻场景在现实中几乎没有依据。大约六个月后，当我在美国参议院的人工智能洞察论坛上为此作证时，立法者不再担心人工智能的“接管”。<br /><br />之后，开源的反对者改变了立场。他们的主要论点转向了人工智能帮助制造生物武器的风险。不久之后，OpenAI和RAND 公司表明，目前的人工智能并没有显著提高犯罪分子制造生物武器的能力。这种对人工智能生物武器的恐惧已经减弱。可以肯定的是，作恶者使用生物武器的可能性——不管有没有人工智能——仍然是国际社会高度关注的一个话题。<br /><br />阻止开源人工智能的最新争论已经转向了国家安全。人工智能在经济竞争和战争中都能发挥作用。开源的反对者表示，美国应该确保其对手无法获得最新的基础模型。虽然我不希望专制政府使用人工智能，特别是发动非正义的战争，但LLM已经问世，如果民主国家限制其使用，专制国家将填补空白。当有一天，某个地方的孩子向人工智能系统询问有关民主、新闻自由的作用或独立司法在维护法治方面的作用时，我希望人工智能能够反映民主价值观，而不是偏袒专制领导人的目标，比如人权。<br /><br />离开华盛顿时，我对我们取得的进展持乐观态度。一年前，在我看来，立法者似乎耗费了80%的时间谈论人工智能的护栏，仅有20%的时间谈论投资创新。令我高兴的是，这一比例发生了逆转，有关投资创新的讨论也多了很多。<br /><br />除了美国联邦政府，全球还有许多司法管辖区。不幸的是，支持限制人工智能发展的法规的论点持续激增。但我从访问华盛顿和他国首都的经历中了解到，与监管机构交流确实有影响。如果你有机会与任何级别的监管机构交谈，我希望你能尽你所能帮助政府更好地理解人工智能。<br /><br />请不断学习,<br />吴恩达<br /> <br />附注：两门新的短期课程现已上线！<br />我很高兴地宣布，我们发布了第一个专注于智能体工作流的短期课程: "Building Agentic RAG with LlamaIndex"，由LlamaIndex首席执行官Jerry Liu教授。这门课程涵盖了RAG的一个重要转变。相较于让开发人员编写显式例程来检索信息以提供给LLM的上下文，我们可以构建具有访问工具来检索信息功能的RAG智能体。这让它可以决定获取什么信息，并使用多步推理来回答更复杂的问题。<br /></p><p>此外，我很高兴推出"Quantization in Depth"课程，由Hugging Face公司的 Marc Sun 和 Younes Belkada教授。量化是使大型模型易于访问的关键技术。你将学习如何实现线性量化变量、不同粒度的量化，以及将深度学习模型压缩到8位和2位精度。</p>
]]></content:encoded>
<pubDate>Thu, 09 May 2024 05:21:45 GMT</pubDate>
</item>
<item>
<title>吴恩达发表了文章: 吴恩达来信：构建可以自我学习的模型</title>
<link>https://zhuanlan.zhihu.com/p/695774709</link>
<guid>https://zhuanlan.zhihu.com/p/695774709</guid>
<content:encoded><![CDATA[
<div> LLM, agentic workflows, synthetic data, token generation, training data  
<br />  
总结:  
LLM通过智能体工作流产生高质量输出，可用于训练自身；生成大量token用于数据预训练；使用设计模式优化LLM数据生成；智能体工作流开辟合成数据新机会；控制视觉模型技巧。 <div>
<p>Dear friends,</p><p>Inexpensive token generation and agentic workflows for large language models (LLMs) open up intriguing new possibilities for training LLMs on synthetic data. Pretraining an LLM on its own directly generated responses to prompts doesn't help. But if an agentic workflow implemented with the LLM results in higher quality output than the LLM can generate directly, then training on that output becomes potentially useful.</p><p>Just as humans can learn from their own thinking, perhaps LLMs can, too. For example, imagine a math student who is learning to write mathematical proofs. By solving a few problems — even without external input — they can reflect on what does and doesn’t work and, through practice, learn how to more quickly generate good proofs. </p><p>Broadly, LLM training involves (i) pretraining (learning from unlabeled text data to predict the next word) followed by (ii) instruction fine-tuning (learning to follow instructions) and (iii) RLHF/DPO tuning to align the LLM’s output to human values. Step (i) requires many orders of magnitude more data than the other steps. For example, <a class=" wrap external" href="https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md?utm_campaign=The%20Batch&amp;utm_source=hs_email&amp;utm_medium=email&amp;_hsenc=p2ANqtz-8k0LiZQvRWFPDGgDt43tNF902ROx3dTDBEvtdF-XpX81iwHOkMt0-y9vAGM94bcVF8ZSYc" rel="nofollow noreferrer" target="_blank">Llama 3</a> was pretrained on over 15 trillion tokens, and LLM developers are still hungry for more data. Where can we get more text to train on? </p><p>Many developers train smaller models directly on the output of larger models, so a smaller model learns to mimic a larger model’s behavior on a particular task. However, an LLM can’t learn much by training on data it generated directly, just like a supervised learning algorithm can’t learn from trying to predict labels it generated by itself. Indeed, training a model repeatedly on the output of an earlier version of itself can result in <a class=" wrap external" href="https://www.deeplearning.ai/the-batch/study-reveals-serious-defects-in-models-trained-on-their-own-content/?utm_campaign=The%20Batch&amp;utm_source=hs_email&amp;utm_medium=email&amp;_hsenc=p2ANqtz-8k0LiZQvRWFPDGgDt43tNF902ROx3dTDBEvtdF-XpX81iwHOkMt0-y9vAGM94bcVF8ZSYc" rel="nofollow noreferrer" target="_blank">model collapse</a>. </p><p>However, an LLM wrapped in an <a class=" wrap external" href="https://www.deeplearning.ai/the-batch/how-agents-can-improve-llm-performance/?utm_campaign=The%20Batch&amp;utm_source=hs_email&amp;utm_medium=email&amp;_hsenc=p2ANqtz-8k0LiZQvRWFPDGgDt43tNF902ROx3dTDBEvtdF-XpX81iwHOkMt0-y9vAGM94bcVF8ZSYc" rel="nofollow noreferrer" target="_blank">agentic workflow</a> may produce higher-quality output than it can generate directly. In this case, the LLM’s higher-quality output might be useful as pretraining data for the LLM itself. </p><p>Efforts like these have precedents:</p><ul><li>When using  reinforcement learning to play a game like chess, a model might learn a function that evaluates board positions. If we apply game tree search along with a low-accuracy evaluation function, the model can come up with more accurate evaluations. Then we can train that evaluation function to mimic these more accurate values.</li><li>In the alignment step, Anthropic’s <a class=" wrap external" href="https://arxiv.org/abs/2212.08073?utm_campaign=The%20Batch&amp;utm_source=hs_email&amp;utm_medium=email&amp;_hsenc=p2ANqtz-8k0LiZQvRWFPDGgDt43tNF902ROx3dTDBEvtdF-XpX81iwHOkMt0-y9vAGM94bcVF8ZSYc" rel="nofollow noreferrer" target="_blank">constitutional AI</a> method uses RLAIF (RL from AI Feedback) to judge the quality of LLM outputs, substituting feedback generated by an AI model for human feedback. </li></ul><p>A significant barrier to using LLMs prompted via agentic workflows to produce their own training data is the cost of generating tokens. Say we want to generate 1 trillion tokens to extend a pre-existing training dataset. Currently, at publicly announced prices, generating 1 trillion tokens using GPT-4-turbo ($30 per million output tokens), Claude 3 Opus ($75), Gemini 1.5 Pro ($21), and Llama-3-70B on Groq ($0.79) would cost, respectively, $30M, $75M, $21M and $790K. Of course, an agentic workflow that uses a design pattern like <a class=" wrap external" href="https://www.deeplearning.ai/the-batch/agentic-design-patterns-part-2-reflection/?utm_campaign=The%20Batch&amp;utm_source=hs_email&amp;utm_medium=email&amp;_hsenc=p2ANqtz-8k0LiZQvRWFPDGgDt43tNF902ROx3dTDBEvtdF-XpX81iwHOkMt0-y9vAGM94bcVF8ZSYc" rel="nofollow noreferrer" target="_blank">Reflection</a> would require generating more than one token per token that we would use as training data. But budgets for training cutting-edge LLMs easily surpass $100M, so spending a few million dollars more for data to boost performance is quite feasible.</p><p>That’s why I believe agentic workflows will open up intriguing new opportunities for high-quality synthetic data generation. </p><p>Keep learning!</p><p>Andrew</p><p>P.S. In “Prompt Engineering for Vision Models,” taught by Abby Morgan, Jacques Verré, and Caleb Kaiser of Comet, you’ll learn how to prompt and fine-tune a variety of vision models for image generation, image editing, object detection, and segmentation. For example, you’ll use OWL-ViT to detect an object you describe in a text prompt, pass the bounding box to SAM to create a segmentation mask, and feed the mask into Stable Diffusion with a text prompt to replace the original object with a new one. Controlling vision models can be tricky, and this course will teach you the techniques to control their output. Get started <a class=" wrap external" href="https://www.deeplearning.ai/short-courses/prompt-engineering-for-vision-models/?utm_campaign=The%20Batch&amp;utm_source=hs_email&amp;utm_medium=email&amp;_hsenc=p2ANqtz-8k0LiZQvRWFPDGgDt43tNF902ROx3dTDBEvtdF-XpX81iwHOkMt0-y9vAGM94bcVF8ZSYc" rel="nofollow noreferrer" target="_blank">here</a>!</p><hr /><p>亲爱的朋友们,<br /><br />大型语言模型 (LLM) 的低价token生成和智能体工作流为在合成数据上训练LLM开辟了有趣的新可能性。在LLM直接生成的对提示的响应上进行预训练是没有帮助的。但是，如果使用LLM实现的智能体工作流产生的输出质量高于LLM直接生成的输出质量，那么对该输出进行训练就可能变得有用。<br /><br />就像人类可以从自己的思考中学习一样，LLM或许也可以。例如，想象一个学数学的学生学习写数学证明的过程。通过解决一些问题——即使没有外部输入——他们可以反思什么可行，什么不可行，并通过实践学习如何更快生成好的证明。<br /><br />从广义上讲，LLM训练包括(i)预训练（从未标记的文本数据中学习以预测下一个单词），然后是(ii)指令微调（学习遵循指令）和(iii) RLHF/DPO调优以使LLM的输出与人类价值观保持一致。步骤(i)需要比其他步骤多用到许多数量级的数据。例如，Llama 3在超过15万亿个token上进行了预训练，而LLM开发人员仍然渴望更多的数据。我们从哪里可以得到更多的文本来进行训练？<br /><br />许多开发人员直接在大型模型的输出上训练较小的模型，因此较小的模型学习模仿较大模型在特定任务上的行为。然而，LLM无法通过训练它直接生成的数据来学习很多东西，就像监督学习算法无法通过尝试预测它自己生成的标签来学习一样。事实上，在早期版本的输出上反复训练模型可能会导致模型崩溃。<br /><br />然而，在智能体工作流中运行的LLM可能产生比它直接生成的更高质量的输出。在这种情况下，LLM的高质量输出可能对LLM本身的预训练数据有用。<br /><br />这样的努力是有先例的:<br />● 当使用强化学习来玩像国际象棋这样的游戏时，模型可能会学习一个评估棋盘位置的函数。如果我们将游戏树搜索与低精度评估函数结合使用，则模型可以得出更准确的评估。然后我们可以训练评估函数来模拟这些更精确的值。<br />● 在对齐步骤中，Anthropic的constitutional AI（宪法AI）方法使用RLAIF（来自AI反馈的RL）来判断LLM输出的质量，并用AI模型生成的反馈代替人类反馈。<br /><br />通过智能体工作流提示使用LLM生成自己的训练数据的一个重大阻碍是生成token的成本。假设我们想要生成1万亿个token来扩展已有的训练数据集。目前，按照公开发布的价格，使用GPT-4-turbo（输出每百万token花费30美元）、Claude 3 Opus（75美元）、Gemini 1.5 Pro（21美元）和Llama-3-70B在Groq（0.79美元）上生成1万亿哥token将分别花费3000万美元、7500万美元、2100万美元和79万美元。当然，使用像Reflection（反思）这样的设计模式的智能体工作流需要为每个token生成多个token，我们将这些token用作训练数据。但训练顶尖LLM的预算很容易超过1亿美元，因此，再花几百万美元购买数据以提高性能是完全可行的。<br /><br />这就是为什么我相信智能体工作流将为高质量的合成数据生成开辟有趣的新机会。<br /><br />请不断学习！<br />吴恩达<br /><br />P.S.“Prompt Engineering for Vision Models”课程由来自Comet的Abby Morgan, Jacques Verré和Caleb Kaiser 共同教授。在这门课程中，您将学习如何为图像生成、图像编辑、对象检测和分割进行模型的提示和微调。例如，您将使用OWL-ViT来检测您在文本提示符中描述的对象，将边界框传递给SAM以创建分割掩码，并将掩码与文本提示符一起提供给Stable Diffusion以使用新对象替换原始对象。控制视觉模型可能很棘手，本课程将教您控制其输出的技术。请<a class=" wrap external" href="https://www.deeplearning.ai/short-courses/prompt-engineering-for-vision-models/?utm_campaign=The%20Batch&amp;utm_source=hs_email&amp;utm_medium=email&amp;_hsenc=p2ANqtz-8k0LiZQvRWFPDGgDt43tNF902ROx3dTDBEvtdF-XpX81iwHOkMt0-y9vAGM94bcVF8ZSYc" rel="nofollow noreferrer" target="_blank">点此</a>开始学习<br /> <br /> </p>
]]></content:encoded>
<pubDate>Fri, 03 May 2024 03:40:55 GMT</pubDate>
</item>
<item>
<title>吴恩达发表了文章: 吴恩达来信：智能体设计模式3：工具使用</title>
<link>https://zhuanlan.zhihu.com/p/690698468</link>
<guid>https://zhuanlan.zhihu.com/p/690698468</guid>
<content:encoded><![CDATA[
<p>Dear friends,</p><p>Tool use, in which an LLM is given functions it can request to call for gathering information, taking action, or manipulating data, is a key design pattern of <a class=" wrap external" href="https://www.deeplearning.ai/the-batch/how-agents-can-improve-llm-performance/?utm_campaign=The%20Batch&amp;utm_source=hs_email&amp;utm_medium=email&amp;_hsenc=p2ANqtz--9ARMthd09q0ABUi-abo6BH62BLbcwPo13LrXs9hUezs-L050Ay7b_rHdWuRIqBVOD6k_S" rel="nofollow noreferrer" target="_blank">AI agentic workflows</a>. You may be familiar with LLM-based systems that can perform a web search or execute code. Indeed, some of the large, consumer-facing LLMs already incorporate these features. But tool use goes well beyond these examples. <br /><br />If you prompt an online LLM-based chat system, “What is the best coffee maker according to reviewers?”, it might decide to carry out a web search and download one or more web pages to gain context. Early on, LLM developers realized that relying only on a pre-trained transformer to generate output tokens is limiting, and that giving an LLM a tool for web search lets it do much more. With such a tool, an LLM is either fine-tuned or prompted (perhaps with few-shot prompting) to generate a special string like {tool: web-search, query: "coffee maker reviews"} to request calling a search engine. (The exact format of the string depends on the implementation.) A post-processing step then looks for strings like these, calls the web search function with the relevant parameters when it finds one, and passes the result back to the LLM as additional input context for further processing. </p><p>Similarly, if you ask, “If I invest $100 at compound 7% interest for 12 years, what do I have at the end?”, rather than trying to generate the answer directly using a transformer network — which is unlikely to result in the right answer — the LLM might use a code execution tool to run a Python command to compute 100 * (1+0.07)**12 to get the right answer. The LLM might generate a string like this: {tool: python-interpreter, code: "100 * (1+0.07)**12"}. </p><p>But tool use in agentic workflows now goes much further. Developers are using functions to search different sources (web, Wikipedia, arXiv, etc.), to interface with productivity tools (send email, read/write calendar entries, etc.), generate or interpret images, and much more. We can prompt an LLM using context that gives detailed descriptions of many functions. These descriptions might include a text description of what the function does plus details of what arguments the function expects. And we’d expect the LLM to automatically choose the right function to call to do a job. Further, systems are being built in which the LLM has access to hundreds of tools. In such settings, there might be too many functions at your disposal to put all of them into the LLM context, so you might use heuristics to pick the most relevant subset to include in the LLM context at the current step of processing. This technique, which is described in the Gorilla paper cited below, is reminiscent of how, if there is too much text to include as context, retrieval augmented generation (RAG) systems offer heuristics for picking a subset of the text to include. </p><p>Early in the history of LLMs, before widespread availability of large multimodal models (LMMs)  like LLaVa, GPT-4V, and Gemini, LLMs could not process images directly, so a lot of work on tool use was carried out by the computer vision community. At that time, the only way for an LLM-based system to manipulate an image was by calling a function to, say, carry out object recognition or some other function on it. Since then, practices for tool use have exploded. GPT-4’s function calling capability, released in the middle of last year, was a significant step toward general-purpose tool use. Since then, more and more LLMs are being developed to similarly be facile with tool use. </p><p>If you’re interested in learning more about tool use, I recommend: </p><ul><li>“<a class=" wrap external" href="https://arxiv.org/abs/2305.15334?utm_campaign=The%20Batch&amp;utm_source=hs_email&amp;utm_medium=email&amp;_hsenc=p2ANqtz--9ARMthd09q0ABUi-abo6BH62BLbcwPo13LrXs9hUezs-L050Ay7b_rHdWuRIqBVOD6k_S" rel="nofollow noreferrer" target="_blank">Gorilla: Large Language Model Connected with Massive APIs</a>,” Patil et al. (2023)</li><li>“<a class=" wrap external" href="https://arxiv.org/abs/2303.11381?utm_campaign=The%20Batch&amp;utm_source=hs_email&amp;utm_medium=email&amp;_hsenc=p2ANqtz--9ARMthd09q0ABUi-abo6BH62BLbcwPo13LrXs9hUezs-L050Ay7b_rHdWuRIqBVOD6k_S" rel="nofollow noreferrer" target="_blank">MM-REACT: Prompting ChatGPT for Multimodal Reasoning and Action</a>,” Yang et al. (2023)</li><li>“<a class=" wrap external" href="https://arxiv.org/abs/2401.17464?utm_campaign=The%20Batch&amp;utm_source=hs_email&amp;utm_medium=email&amp;_hsenc=p2ANqtz--9ARMthd09q0ABUi-abo6BH62BLbcwPo13LrXs9hUezs-L050Ay7b_rHdWuRIqBVOD6k_S" rel="nofollow noreferrer" target="_blank">Efficient Tool Use with Chain-of-Abstraction Reasoning</a>,” Gao et al. (2024)   </li></ul><p>Both Tool Use and Reflection, which I described in last week’s <a class=" wrap external" href="https://www.deeplearning.ai/the-batch/agentic-design-patterns-part-2-reflection/?utm_campaign=The%20Batch&amp;utm_source=hs_email&amp;utm_medium=email&amp;_hsenc=p2ANqtz--9ARMthd09q0ABUi-abo6BH62BLbcwPo13LrXs9hUezs-L050Ay7b_rHdWuRIqBVOD6k_S" rel="nofollow noreferrer" target="_blank">letter</a>, are design patterns that I can get to work fairly reliably on my applications — both are capabilities well worth learning about. In future letters, I’ll describe the Planning and Multi-agent collaboration design patterns. They allow AI agents to do much more but are less mature, less predictable — albeit very exciting — technologies. <br /><br />Keep learning!</p><p>Andrew</p><p><a class=" wrap external" href="https://www.deeplearning.ai/the-batch/how-agents-can-improve-llm-performance/?ref=dl-staging-website.ghost.io" rel="nofollow noreferrer" target="_blank">Read "Agentic Design Patterns Part 1: Four AI agent strategies that improve GPT-4 and GPT-3.5 performance"</a></p><p><a class=" wrap external" href="https://www.deeplearning.ai/the-batch/agentic-design-patterns-part-2-reflection/?ref=dl-staging-website.ghost.io" rel="nofollow noreferrer" target="_blank">Read "Agentic Design Patterns Part 2: Reflection"</a></p><p>P.S. Learn to carry out red-teaming attacks against your own LLM-based applications to spot and patch vulnerabilities! In our new short course, “Red Teaming LLM Applications,” Matteo Dora and Luca Martial of LLM testing company Giskard teach how to simulate malicious actions to discover vulnerabilities and improve security. We start with prompt injection, which can trick an LLM into bypassing safeguards to reveal private information or say something inappropriate. There is no one-size-fits-all approach to security, but this course will help you identify some scenarios to protect against.</p><p>We believe that widespread knowledge of red-teaming capabilities will result in greater transparency and safer LLM-based systems. However, we ask you to use the skills you gain from this course ethically.</p><p><a class=" wrap external" href="https://www.deeplearning.ai/short-courses/red-teaming-llm-applications/?utm_campaign=The%20Batch&amp;utm_source=hs_email&amp;utm_medium=email&amp;_hsenc=p2ANqtz--9ARMthd09q0ABUi-abo6BH62BLbcwPo13LrXs9hUezs-L050Ay7b_rHdWuRIqBVOD6k_S" rel="nofollow noreferrer" target="_blank">Sign up here</a></p><hr /><p>亲爱的朋友们,<br /><br />工具使用 (Tool use) 是AI智能体工作流的关键设计模式，在工具的使用中，LLM被赋予可以请求调用的功能，从而收集信息、采取行动或操纵数据。你可能熟悉可以执行web搜索或执行代码的基于LLM的系统。事实上，一些面向消费者的大型LLM已经整合了这些功能。但是工具的使用远远超出了这些例子。<br /><br />如果你对一个基于LLM的在线聊天系统进行提示，“根据评论者的评价，最好的咖啡机是什么?”，该系统可能会决定进行网络搜索并下载一个或多个网页以获取上下文。早期时候，LLM的开发人员意识到仅依靠预训练过的transformer来生成输出token是有限的，而向LLM提供网络搜索工具可以让它做得更多。有了这样的工具，LLM要么被微调，要么被提示（可能只有很少的提示）生成一个特殊的字符串，比如{tool: web-search, query: "coffee maker reviews"}，以请求调用搜索引擎。（字符串的确切格式取决于实现。）然后，一个后处理步骤会查找这样的字符串，在找到一个字符串时调用带有相关参数的web搜索函数，并将结果作为额外的输入上下文传递回LLM以进行进一步处理。<br /><br />同样，如果提问“如果我以7%的复利投资100美元，12年后最终会拿到多少钱?”，而不是试图直接使用transformer网络生成答案（这不太可能产生正确的答案），LLM可能使用代码执行工具运行Python命令来计算100 *(1+0.07)**12以获得正确答案。可能会生成这样的字符串:{tool: python-interpreter, code: "100 *(1+0.07)**12"}。<br /><br />但是在智能体工作流中的工具使用现在取得了更大发展了。开发人员正在使用函数来搜索不同的资源（web, Wikipedia, arXiv等），与生产力工具（发送电子邮件，读写日历条目等）进行交互，生成或解释图像等等。我们可以使用给出许多函数详细描述的上下文来提示LLM。这些描述可能包括函数功能的文本描述以及函数期望的参数的详细信息。我们希望LLM能够自动选择正确的函数来完成工作。此外，正在构建的系统中，LLM可以访问数百种工具。在这种设置中，可能有太多的函数可供使用，无法将它们全部放入LLM上下文中，因此你可以使用启发法选择最相关的子集，以便在当前处理步骤中将其包含在LLM上下文中。下面引用的Gorilla论文中介绍了这种技术，它让人想起，如果有太多的文本要作为上下文包含，检索增强生成 (RAG) 系统如何提供启发法来选择要包含的文本子集。<br /><br />在LLM发展的早期，在LLaVa、GPT-4V和Gemini等大型多模态模型 (LMM) 广泛应用之前，LLM不能直接处理图像，因此许多关于工具使用的工作是由计算机视觉社群进行的。当时，基于LLM的系统操作图像的唯一方法是调用函数来执行对象识别或其他函数。从那时起，工具使用的实践开始了爆炸式增长。GPT-4的函数调用功能于去年年中发布，是迈向通用工具的重要一步。从那时起，越来越多的LLM被开发成同样易于使用工具。<br /><br />如果你有兴趣了解更多关于工具使用的知识，建议阅读以下资料:<br />● “Gorilla: Large Language Model Connected with Massive APIs,”Patil et al. (2023)<br />● “MM-REACT: Prompting ChatGPT for Multimodal Reasoning and Action,”，Yanget et al. (2023)<br />● “Efficient Tool Use with Chain-of-Abstraction Reasoning”，Gao et al. (2024)<br /><br />我在上周的来信中描述的工具使用和反思 (Reflection) 都是可以在我的应用程序上表现相当可靠地设计模式——两者都是非常值得学习的功能。在以后的来信中，我将描述规划 (Planning) 和多智能体协作设计模式。它们允许人工智能代理做更多的事情，但不太成熟，不太可预测(尽管非常令人兴奋)的技术。<br /><br />请不断学习！<br />吴恩达<br /><br />阅读 "Agentic Design Patterns Part 1: Four AI agent strategies that improve GPT-4 and GPT-3.5 performance"<br />阅读 "Agentic Design Patterns Part 2: Reflection"<br /><br />附注:学习对你自己基于LLM的应用程序进行red-teaming攻击，以发现和修补漏洞！在我们新的短期课程“Red Teaming LLM Applications”中，LLM测试公司Giskard的Matteo Dora和Luca Martial教授如何模拟恶意行为来发现漏洞并提高安全性。我们从注入提示开始，它可以欺骗LLM绕过保护措施，泄露私人信息或说一些不恰当的话。没有放之四海而皆准的安全方法，但本课程将帮助你识别一些需要防范的场景。<br /><br />我们相信，对red-teaming能力的广泛了解将带来更高的透明度和更安全的基于LLM的系统。当然，我们要求你符合伦理道德地使用从这门课程中获得的技能。请<a class=" wrap external" href="https://www.deeplearning.ai/the-batch/issue-243/" rel="nofollow noreferrer" target="_blank">点此</a>注册学习~<br /> </p>
]]></content:encoded>
<pubDate>Thu, 04 Apr 2024 03:34:05 GMT</pubDate>
</item>
<item>
<title>吴恩达发表了文章: 吴恩达来信： 智能体设计模式2：Reflection</title>
<link>https://zhuanlan.zhihu.com/p/689492556</link>
<guid>https://zhuanlan.zhihu.com/p/689492556</guid>
<content:encoded><![CDATA[
<p>Dear friends,</p><p>Last week, I described four design patterns for AI agentic workflows that I believe will drive significant progress this year: Reflection, Tool use, Planning and Multi-agent collaboration. Instead of having an LLM generate its final output directly, an agentic workflow prompts the LLM multiple times, giving it opportunities to build step by step to higher-quality output. In this letter, I'd like to discuss Reflection. For a design pattern that’s relatively quick to implement, I've seen it lead to surprising performance gains. <br /><br />You may have had the experience of prompting ChatGPT/Claude/Gemini, receiving unsatisfactory output, delivering critical feedback to help the LLM improve its response, and then getting a better response. What if you automate the step of delivering critical feedback, so the model automatically criticizes its own output and improves its response? This is the crux of Reflection. <br /><br />Take the task of asking an LLM to write code. We can prompt it to generate the desired code directly to carry out some task X. After that, we can prompt it to reflect on its own output, perhaps as follows:</p><p><i>Here’s code intended for task X: [previously generated code]    </i><br /><i>Check the code carefully for correctness, style, and efficiency, and give constructive criticism for how to improve it.</i><br /><br />Sometimes this causes the LLM to spot problems and come up with constructive suggestions. Next, we can prompt the LLM with context including (i) the previously generated code and (ii) the constructive feedback and (iii) ask it to use the feedback to rewrite the code. This can lead to a better response. Repeating the criticism/rewrite process might yield further improvements. This self-reflection process allows the LLM to spot gaps and improve its output on a variety of tasks including producing code, writing text, and answering questions. </p><p>And we can go beyond self-reflection by giving the LLM tools that help evaluate its output; for example, running its code through a few unit tests to check whether it generates correct results on test cases or searching the web to double-check text output. Then it can reflect on any errors it found and come up with ideas for improvement.</p><p>Further, we can implement Reflection using a multi-agent framework. I've found it convenient to create two different agents, one prompted to generate good outputs and the other prompted to give constructive criticism of the first agent's output. The resulting discussion between the two agents leads to improved responses.</p><p>Reflection is a relatively basic type of agentic workflow, but I've been delighted by how much it improved my applications’ results in a few cases. I hope you will try it in your own work. If you’re interested in learning more about reflection, I recommend these papers:</p><ul><li>“<a class=" wrap external" href="https://arxiv.org/abs/2303.17651?utm_campaign=The%20Batch&amp;utm_source=hs_email&amp;utm_medium=email&amp;_hsenc=p2ANqtz-9dHVnW1I1bA3sPBbsikjT165Qez3QiiAssknCERwgki818YHG7PyHOQSgg-nxKDa0BuE7B" rel="nofollow noreferrer" target="_blank">Self-Refine: Iterative Refinement with Self-Feedback</a>,” Madaan et al., 2023</li><li>“<a class=" wrap external" href="https://arxiv.org/abs/2303.11366?utm_campaign=The%20Batch&amp;utm_source=hs_email&amp;utm_medium=email&amp;_hsenc=p2ANqtz-9dHVnW1I1bA3sPBbsikjT165Qez3QiiAssknCERwgki818YHG7PyHOQSgg-nxKDa0BuE7B" rel="nofollow noreferrer" target="_blank">Reflexion: Language Agents with Verbal Reinforcement Learning</a>,” Shinn et al., 2023</li><li>“<a class=" wrap external" href="https://arxiv.org/abs/2305.11738?utm_campaign=The%20Batch&amp;utm_source=hs_email&amp;utm_medium=email&amp;_hsenc=p2ANqtz-9dHVnW1I1bA3sPBbsikjT165Qez3QiiAssknCERwgki818YHG7PyHOQSgg-nxKDa0BuE7B" rel="nofollow noreferrer" target="_blank">CRITIC: Large Language Models Can Self-Correct with Tool-Interactive Critiquing</a>,” Gou et al., 2024</li></ul><p>I’ll discuss the other agentic design patterns in future letters.</p><p>Keep learning!</p><p>Andrew </p><p>P.S. New JavaScript short course! Learn to build full-stack web applications that use RAG in “JavaScript RAG Web Apps with LlamaIndex,” taught by Laurie Voss, VP of Developer Relations at LlamaIndex and co-founder of npm. </p><ul><li>Build a RAG application for querying your own data.</li><li>Develop tools that interact with multiple data sources and use an agent to autonomously select the right tool for a given query.</li><li>Create a full-stack web app step by step that lets you chat with your data. </li><li>Dig further into production-ready techniques like how to persist your data, so you don’t need to reindex constantly.</li></ul><p><a class=" wrap external" href="https://www.deeplearning.ai/short-courses/javascript-rag-web-apps-with-llamaindex?utm_campaign=The%20Batch&amp;utm_source=hs_email&amp;utm_medium=email&amp;_hsenc=p2ANqtz-9dHVnW1I1bA3sPBbsikjT165Qez3QiiAssknCERwgki818YHG7PyHOQSgg-nxKDa0BuE7B" rel="nofollow noreferrer" target="_blank">Sign up here</a>!</p><hr /><p class="ztext-empty-paragraph"><br /></p><p>亲爱的朋友们,<br /><br />上周，我介绍了AI智能体工作流的四种设计模式，我相信它们将在今年推动重大进展：Reflection, Tool use, Planning and Multi-agent collaboration。智能体工作流不是让LLM直接生成最终输出，而是多次提示LLM，使其有机会逐步构建更高质量的输出。在本周的来信中，我想重点讨论一下Reflection（反思）。对于实现速度相对较快的设计模式，我已经看到它带来了惊人的性能提升效果。<br /><br />我们可能都有过这样的经历：提示ChatGPT/Claude/Gemini，得到不满意的输出，提供关键反馈以帮助LLM改进其响应，然后获得更好的响应。如果使用自动化交付关键反馈的步骤，让模型自动批评自己的输出并改进其响应，结果会怎样？这是Reflection的关键。<br /><br />以要求LLM编写代码为例。我们可以提示它直接生成所需的代码来执行某个任务x。之后，我们可以提示它反思自己的输出，可能如下所示:<br /><br />下面是任务X的代码：[之前生成的代码]<br />仔细检查代码的正确性、风格和效率，并对如何改进它提出建设性的批评。<br /><br />有时这会使LLM发现问题并提出建设性建议。接下来，我们可以用上下文提示LLM，包括(i)以前生成的代码和(ii)建设性的反馈和(iii)要求它使用反馈来重写代码。这可以带来更好的反应。重复批评/重写过程可能会产生进一步的改进。这种自我反思过程使LLM能够发现差距并改善其在各种任务上的输出，包括生成代码，编写文本和回答问题。<br /><br />我们可以通过给LLM提供工具来帮助评估其产出，从而超越自我反思；例如，通过几个单元测试来运行它的代码，以检查它是否在测试用例上生成正确的结果，或者搜索网页以检查文本输出。然后，它可以反思它发现的任何错误，并提出改进的想法。<br /><br />此外，我们可以使用多智能体框架实现Reflection。我发现创建两个不同的智能体很方便，一个提示生成良好的输出，另一个提示对第一个智能体的输出给出建设性的批评。两个智能体之间的讨论推动了改进的响应。<br /><br />Reflection是一种相对基本的智能体工作流类型，但我很高兴它在一些情况下改善了我的应用程序的结果。我希望你能在自己的工作中尝试一下。如果你有兴趣了解更多关于Reflection的知识，我推荐这些论文:<br />● “Self-Refine: Iterative Refinement with Self-Feedback,” Madaan et al., 2023<br />● “Reflexion: Language Agents with Verbal Reinforcement Learning,” Shinn et al., 2023<br />● “CRITIC: Large Language Models Can Self-Correct with Tool-Interactive Critiquing,” Gou et al., 2024<br /><br />我将在以后的来信中讨论其他智能体设计模式。<br /><br />请不断学习!<br />吴恩达<br /><br />P.S.新的JavaScript短期课程现已上线！在“JavaScript RAG web Apps with LlamaIndex”课程中学习使用RAG构建全栈web应用程序，该课程由LlamaIndex的开发者关系副总裁兼npm的联合创始人Laurie Voss教授。<br />● 构建一个用于查询自己的数据的RAG应用程序。<br />● 开发与多个数据源交互的工具，并使用智能体为给定查询自主选择正确的工具。<br />● 一步一步地创建一个全栈web应用程序，可以让你与数据聊天。<br />● 深入研究生产就绪 (production-ready)的技术，比如如何持久化数据，这样就不需要不断地重新索引了。<br /></p><p>请<a class=" wrap external" href="https://www.deeplearning.ai/short-courses/javascript-rag-web-apps-with-llamaindex?utm_campaign=The%20Batch&amp;utm_source=hs_email&amp;utm_medium=email&amp;_hsenc=p2ANqtz-9dHVnW1I1bA3sPBbsikjT165Qez3QiiAssknCERwgki818YHG7PyHOQSgg-nxKDa0BuE7B" rel="nofollow noreferrer" target="_blank">点此</a>注册学习~</p><p></p><p></p>
]]></content:encoded>
<pubDate>Thu, 28 Mar 2024 03:50:20 GMT</pubDate>
</item>
<item>
<title>吴恩达发表了文章: 吴恩达来信：智能体如何优化LLM性能</title>
<link>https://zhuanlan.zhihu.com/p/688226963</link>
<guid>https://zhuanlan.zhihu.com/p/688226963</guid>
<content:encoded><![CDATA[
<p>Dear friends,</p><p>I think AI agent workflows will drive massive AI progress this year — perhaps even more than the next generation of foundation models. This is an important trend, and I urge everyone who works in AI to pay attention to it.</p><p>Today, we mostly use LLMs in zero-shot mode, prompting a model to generate final output token by token without revising its work. This is akin to asking someone to compose an essay from start to finish, typing straight through with no backspacing allowed, and expecting a high-quality result. Despite the difficulty, LLMs do amazingly well at this task! </p><p>With an agent workflow, however, we can ask the LLM to iterate over a document many times. For example, it might take a sequence of steps such as:</p><ul><li>Plan an outline.</li><li>Decide what, if any, web searches are needed to gather more information.</li><li>Write a first draft.</li><li>Read over the first draft to spot unjustified arguments or extraneous information.</li><li>Revise the draft taking into account any weaknesses spotted.</li><li>And so on.</li></ul><p>This iterative process is critical for most human writers to write good text. With AI, such an iterative workflow yields much better results than writing in a single pass. </p><p><a class=" wrap external" href="https://www.cognition-labs.com/introducing-devin?utm_campaign=The%20Batch&amp;utm_source=hs_email&amp;utm_medium=email&amp;_hsenc=p2ANqtz--QGCoaEh42QGUTnfoPl5an-ds0dVuJeNXLRxVO4h72DKVH187SV0hJ06VkEN-DlSHanUDK" rel="nofollow noreferrer" target="_blank">Devin</a>’s splashy demo recently received a lot of social media buzz. My team has been closely following the evolution of AI that writes code. We analyzed results from a number of research teams, focusing on an algorithm’s ability to do well on the widely used HumanEval coding benchmark. You can see our findings in the diagram below. </p><p>GPT-3.5 (zero shot) was 48.1% correct. GPT-4 (zero shot) does better at 67.0%. However, the improvement from GPT-3.5 to GPT-4 is dwarfed by incorporating an iterative agent workflow. Indeed, wrapped in an agent loop, GPT-3.5 achieves up to 95.1%. </p><p>Open source agent tools and the academic literature on agents are proliferating, making this an exciting time but also a confusing one. To help put this work into perspective, I’d like to share a framework for categorizing design patterns for building agents. My team AI Fund is successfully using these patterns in many applications, and I hope you find them useful.</p><ul><li>Reflection: The LLM examines its own work to come up with ways to improve it. </li><li>Tool use: The LLM is given tools such as web search, code execution, or any other function to help it gather information, take action, or process data.</li><li>Planning: The LLM comes up with, and executes, a multistep plan to achieve a goal (for example, writing an outline for an essay, then doing online research, then writing a draft, and so on).</li><li>Multi-agent collaboration: More than one AI agent work together, splitting up tasks and discussing and debating ideas, to come up with better solutions than a single agent would.</li></ul><p>Next week, I’ll elaborate on these design patterns and offer suggested readings for each.</p><p>Keep learning!</p><p>Andrew</p><p>P.S. Build an optimized large language model (LLM) inference system from the ground up in our new short course “Efficiently Serving LLMs,” taught by Predibase CTO Travis Addair.</p><ul><li>Learn techniques like KV caching, continuous batching, and quantization to speed things up and optimize memory usage.</li><li>Benchmark LLM optimizations to explore the trade-offs between latency and serving many users at once.</li><li>Use low-rank adaptation (LoRA) to serve hundreds of custom fine-tuned models on a single device efficiently.</li></ul><p><a class=" wrap external" href="https://www.deeplearning.ai/short-courses/efficiently-serving-llms?utm_campaign=The%20Batch&amp;utm_source=hs_email&amp;utm_medium=email&amp;_hsenc=p2ANqtz--QGCoaEh42QGUTnfoPl5an-ds0dVuJeNXLRxVO4h72DKVH187SV0hJ06VkEN-DlSHanUDK" rel="nofollow noreferrer" target="_blank">Sign up now!</a></p><hr /><p>亲爱的朋友们,<br /><br />我认为AI智能体工作流将在今年推动人工智能的巨大进步——甚至可能超过下一代基础模型。这是一个重要的趋势，我希望所有从事人工智能工作的人都能关注它。<br /><br />目前，我们主要在zero-shot模式下使用LLM，提示模型在不修改其工作的情况下逐token生成最终输出。这类似于要求某人从头到尾写一篇文章，直接输入、不允许空白，并期望得到高质量的结果。尽管困难重重，但LLM在这项任务上表现非常好！<br /><br />然而，对于智能体工作流，我们可以要求LLM对文档进行多次迭代。例如，它可能采取一系列步骤，如:<br />● 拟定大纲。<br />● 如果需要的话，决定需要进行哪些网络搜索来收集更多信息。<br />● 撰写初稿。<br />● 通读初稿，找出不合理的论点或无关的信息。<br />● 根据找到的薄弱点修改初稿。<br />● 等等。<br /></p><p>这种反复的过程对于大多数作家写出好的文本是至关重要的。使用AI，这样的迭代工作流程产生的结果会比单次编写好得多。<br /><br />Devin引人注目的演示最近在社交媒体上引起了很多关注。我的团队一直在密切关注编写代码的人工智能的发展。我们分析了来自多个研究团队的结果，重点关注算法在广泛使用的HumanEval编程基准上表现良好的能力。你可以在下面的图表中看到我们的发现。<br /><br />GPT-3.5 (zero shot) 正确率为48.1%。GPT-4 (zero shot) 的表现更好，为67.0%。然而，从GPT-3.5到GPT-4的提升与合并迭代智能体工作流相比显得微不足道。实际上，在智能体循环里运行，GPT-3.5可以达到95.1%的正确率。<br /><br />开源智能体工具和关于智能体的学术文献正在激增，这是一个令人兴奋的时刻，但也是一个令人困惑的时刻。为了更好地理解这项工作，我想分享一个对搭建智能体的设计模式进行分类的框架。我的团队AI Fund在许多应用中成功地使用了这些模式，我希望你觉得它们有用。<br />● Reflection：LLM审查自己的工作，提出改进的方法。<br />● Tool use：LLM被赋予工具，如网络搜索，代码执行，或任何其他功能，以帮助它收集信息，采取行动，或处理数据。<br />● Planning：LLM提出并执行一个多步骤计划来实现一个目标（例如，编写一篇文章的大纲，然后进行在线研究、编写草稿，等等）。<br />● Multi-agent collaboration：多个AI智能体一起工作，分解任务、讨论和辩论想法，提出比单个智能体更好的解决方案。<br /></p><p>下周，我将详细介绍这些设计模式，并为每种模式提供建议阅读材料。<br /><br />请不断学习!<br />吴恩达<br /><br />附注：学习由Predibase首席技术官Travis Addair教授的新短期课程“Efficiently Serving LLMs”，从头开始构建优化后的LLM推理系统。<br />● 学习KV缓存、连续批处理和量化等技术来加快速度并优化内存使用。<br />● 基准LLM优化，以探索延迟和同时为许多用户服务之间的权衡。<br />● 使用低秩适配(low-rank adaptation , LoRA)在单个设备上高效地服务数百个自定义微调模型。<br />欢迎<a class=" wrap external" href="https://www.deeplearning.ai/short-courses/efficiently-serving-llms?utm_campaign=The%20Batch&amp;utm_source=hs_email&amp;utm_medium=email&amp;_hsenc=p2ANqtz--QGCoaEh42QGUTnfoPl5an-ds0dVuJeNXLRxVO4h72DKVH187SV0hJ06VkEN-DlSHanUDK" rel="nofollow noreferrer" target="_blank">点此</a>注册学习~</p><p></p>
]]></content:encoded>
<pubDate>Thu, 21 Mar 2024 04:00:17 GMT</pubDate>
</item>
<item>
<title>吴恩达发表了文章: 吴恩达来信：低数据重力下的发展</title>
<link>https://zhuanlan.zhihu.com/p/686957251</link>
<guid>https://zhuanlan.zhihu.com/p/686957251</guid>
<content:encoded><![CDATA[
<p><i>Dear friends,</i></p><p><i>I’ve noticed a trend in how generative AI applications are built that might affect both big companies and developers: The gravity of data is decreasing.</i></p><p><i>Data gravity is the idea, proposed by IT engineer Dave McCrory in 2010, that data, or activity around data, attracts and creates more data. With traditional software workloads, data gravity is strong. If you have terabytes of data stored in a particular cloud, the cost to transmit it elsewhere for processing is high. So many teams pick a cloud such as AWS, Azure, or Google Cloud and build on it.</i></p><p><i>However, for many generative AI applications, the cost of processing is much greater than the cost of transmission. This weakens data gravity because data is more weakly bound to the cloud provider or data center where it’s stored, so it’s more practical to build systems that send packets to different servers all over the internet. </i></p><p><i>Let’s say transmitting 1GB of data costs $0.10. 1GB of text might correspond to about 250 million inputs tokens (if we average four characters per token), which costs about $125 to process using the relatively inexpensive gpt-3.5-turbo-0125 model. (With gpt-4-0125-preview, the cost would be 20x higher.) The cost of processing the data is significantly higher than the cost of transmission. Also, given the computationally intensive nature of using an LLM to read and generate tokens, the latency is high enough that sending your text or image tokens across the internet usually doesn’t add that much further latency. </i></p><p><i>This means that, even if we’re building software primarily on a particular cloud provider, it’s still quite feasible to transmit LLM prompts to OpenAI, Anthropic, Anyscale, or Together.ai — or, for that matter, AWS, Azure, or Google Cloud — to get a response. The incentive to build only on a single, monolithic cloud platform is lower than before.  </i></p><p><i>This situation has implications for stakeholders:</i></p><ul><li><i>For developers, it means we’re increasingly assembling AI applications from lots of SaaS providers all across the internet, and stitching their services together.</i></li><li><i>For CIOs, it’s creating headaches in terms of managing where their data goes and how to maintain lists of trusted vendors.</i></li><li><i>For the big cloud companies, it’s changing the basis of competition, since the generative AI portions of their customer workloads look quite different from traditional software workloads.</i></li><li><i>For new tool developers, it’s creating new opportunities for users to use their services, even if they aren’t bundled into one of cloud environments.</i></li></ul><p><i>To be clear, many applications have large traditional software components (that serve up a websites, maintain databases, and so on) as well as new generative AI components (say, a chatbot built on top of the traditional infrastructure). My remarks here apply only to the generative AI portion, and the competitive dynamics of the traditional software components haven’t changed much.</i></p><p><i>Further, as new types of AI components emerge, I expect their gravity to evolve as well. For example, right now it appears reasonably easy to change LLM providers; if you’ve built a system on one LLM, it’s annoying but not impossible to switch to a different LLM provider. In comparison, shifting databases is much harder, and once you’ve stored a lot of data in one vector database, the complexity of migrating to a different one can be high. </i></p><p><i>The gravity of data has been a fundamental tenet of cloud computing, and a major factor of competition for many companies. Decreasing data gravity is decreasing is a complex, exciting trend that will affect many developers and businesses.</i></p><p><i>Keep learning!</i></p><p><i>Andrew</i></p><p><i>P.S. Our new short course “Knowledge Graphs for RAG” is now available, taught by Andreas Kollegger of Neo4! Knowledge graphs are a data structure that’s great at capturing complex relationships among data of multiple types. They can improve the context you pass to the LLM and the performance of your RAG applications by enabling more sophisticated retrieval of text than similarity search alone. In this course, you’ll build a knowledge graph from scratch and see how it improves chat applications by providing both text and graph data to an LLM. <a class=" wrap external" href="https://www.deeplearning.ai/short-courses/knowledge-graphs-rag/?utm_campaign=The%20Batch&amp;utm_source=hs_email&amp;utm_medium=email&amp;_hsenc=p2ANqtz-8inrvPcSiotzMx3NHE0OcmB185WpTlIzg8uKR9JkmAMTY7ElVDkpgDZKqiIJORN5C08z5y" rel="nofollow noreferrer" target="_blank">Sign up here</a>!</i></p><hr /><p>亲爱的朋友们,<br /><br />我注意到生成式人工智能应用程序的构建方式有一个趋势，可能会对大企业和开发人员造成影响：数据的重力正在下降。<br /><br />数据重力 (Data gravity) 是IT工程师Dave McCrory在2010年提出的一个概念，即数据或围绕数据的行为吸引并创造了更多的数据。对于传统的软件工作负载，数据重力很强。如果在某个特定的云端存储了TB量级的数据，那么将其传输到其他地方进行处理的成本就很高。因此，许多团队选择AWS、Azure或Google Cloud等云平台，并在其上进行构建。<br /><br />然而，对于许多生成式人工智能应用来说，处理的成本远远大于传输的成本。这削弱了数据重力，因为数据更少地绑定到存储数据的云提供商或数据中心，因此构建向互联网上的不同服务器发送数据包的系统就更为实际。<br /><br />假设传输1GB数据的成本为0.10美元。1GB的文本可能对应于大约2.5亿个输入令token（如果我们平均每个token四个字符），使用相对便宜的gpt-3.5-turbo-0125模型处理它的成本约为125美元。（如果使用gpt-4-0125-preview，处理成本将高出20倍。）处理数据的成本明显高于传输的成本。此外，考虑到使用LLM读取和生成token的计算密集性质，延迟足够高，通过互联网发送文本或图像token通常不会增加那么多的延迟。<br /><br />这意味着，即使我们主要在一个特定的云提供商上构建软件，将LLM提示传输到OpenAI, Anthropic, Anyscale或Together.ai——或就此而言，AWS, Azure或Google Cloud——来获得响应仍然是非常可行的。只在一个单一的、整体的云平台上构建的动机比以前低了。<br /><br />这种情况对利益相关方有以下影响:<br />● 对于开发人员来说，这意味着我们将越来越多地在互联网上的许多SaaS提供商那里组装人工智能应用程序，并将它们的服务拼接在一起。<br />● 对于公司的首席信息官来说，管理数据流向以及如何维护可信供应商列表是个令人头疼的问题。<br />● 对于大型云计算公司来说，该情况正在改变竞争的基础，因为他们的客户工作负载的生成式人工智能部分看起来与传统的软件工作负载大不相同。<br />● 对于新工具的开发人员来说，该情况为用户创造了使用他们服务的新机会，即使他们没有被捆绑到一个云环境中。<br /><br />需要明确的是，许多应用程序都有大型的传统软件组件（提供网站，维护数据库等）以及新的生成式AI组件（例如，建立在传统基础设施之上的聊天机器人）。我的想法只适用于生成AI部分，传统软件组件的竞争动态并没有太大变化。<br /><br />此外，随着新型AI组件的出现，我预计它们的重力也会发生变化。例如，现在更换LLM提供商似乎相当容易；如果你在一个LLM上构建了一个系统，那么切换到不同的LLM提供商是很烦人的，但并非不可能实现。相比之下，转移数据库就要困难得多，而且一旦在一个矢量数据库中存储了大量数据，那么迁移到另一个矢量数据库的复杂性可能会很高。<br /><br />数据重力一直是云计算的基本原则，也是许多公司竞争的主要因素。数据重力的下降是一个复杂而令人兴奋的趋势，它将影响许多开发人员和企业。<br /><br />请不断学习!<br />吴恩达<br /><br />附注：新的短期课程“Knowledge Graphs for RAG”（构建用于RAG的知识图谱）现已上线，由Neo4的Andreas Kollegger教授！知识图谱是一种数据结构，它非常擅长捕捉多种类型数据之间的复杂关系。通过支持比单独的相似度搜索更复杂的文本检索，它们可以改善传递给LLM的上下文和RAG应用程序的性能。在本课程中，你将从头开始构建知识图谱，并通过向LLM提供文本和图形数据来了解它是如何改进聊天应用程序的。请<a class=" wrap external" href="https://www.deeplearning.ai/short-courses/knowledge-graphs-rag/?utm_campaign=The%20Batch&amp;utm_source=hs_email&amp;utm_medium=email&amp;_hsenc=p2ANqtz-8inrvPcSiotzMx3NHE0OcmB185WpTlIzg8uKR9JkmAMTY7ElVDkpgDZKqiIJORN5C08z5y" rel="nofollow noreferrer" target="_blank">点此</a>注册学习~</p>
]]></content:encoded>
<pubDate>Thu, 14 Mar 2024 03:20:34 GMT</pubDate>
</item>
<item>
<title>吴恩达发表了文章: 吴恩达来信：AI智能体的黎明时刻</title>
<link>https://zhuanlan.zhihu.com/p/685840097</link>
<guid>https://zhuanlan.zhihu.com/p/685840097</guid>
<content:encoded><![CDATA[
<p>Dear friends,</p><p>Progress on LLM-based agents that can autonomously plan out and execute sequences of actions has been rapid, and I continue to see month-over-month improvements. Many projects attempt to take a task like “write a report on topic X” and autonomously take actions such as browsing the web to gather information to synthesize a report. <br /><br />AI agents can be designed to take many different types of actions. Research agents (like many projects built on <a class=" wrap external" href="https://github.com/Significant-Gravitas/AutoGPT?utm_campaign=The%20Batch&amp;utm_source=hs_email&amp;utm_medium=email&amp;_hsenc=p2ANqtz-87IEkio4ktkOH9VkmcWuq2BgtXjYHRZma0dP7QYPpRBg1l-KOxs3aG7c2Wnxp0D-BYC7sE" rel="nofollow noreferrer" target="_blank">AutoGPT</a>, <a class=" wrap external" href="https://github.com/assafelovic/gpt-researcher?utm_campaign=The%20Batch&amp;utm_source=hs_email&amp;utm_medium=email&amp;_hsenc=p2ANqtz-87IEkio4ktkOH9VkmcWuq2BgtXjYHRZma0dP7QYPpRBg1l-KOxs3aG7c2Wnxp0D-BYC7sE" rel="nofollow noreferrer" target="_blank">GPTresearcher</a>, or <a class=" wrap external" href="https://arxiv.org/abs/2402.14207?utm_campaign=The%20Batch&amp;utm_source=hs_email&amp;utm_medium=email&amp;_hsenc=p2ANqtz-87IEkio4ktkOH9VkmcWuq2BgtXjYHRZma0dP7QYPpRBg1l-KOxs3aG7c2Wnxp0D-BYC7sE" rel="nofollow noreferrer" target="_blank">STORM</a>) search the web and fetch web pages. A sales representative agent might dispatch a product to a user. An industrial automation agent might control a robot.</p><p>So far, I see agents that browse the web progressing much faster because the cost of experimentation is low, and this is key to rapid technical progress. It’s cheap to fetch a webpage, and if your agent chooses poorly and reads the wrong page, there’s little harm done. In comparison, sending a product or moving a physical robot are costly actions, which makes it hard to experiment rapidly. Similarly, agents that generate code (that you can run in a sandbox environment) are relatively cheap to run, leading to rapid experimentation and progress. <br /><br />Although today’s research agents, whose tasks are mainly to gather and synthesize information, are still in an early phase of development, I expect to see rapid improvements. ChatGPT, Bing Chat, and Gemini can already browse the web, but their online research tends to be limited; this helps them get back to users quickly. But I look forward to the next generation of agents that can spend minutes or perhaps hours doing deep research before getting back to you with an output. Such algorithms will be able to generate much better answers than models that fetch only one or two pages before returning an answer.<br /><br />Even when experimentation is quick, evaluation remains a bottleneck in development. If you can try out 10 algorithm variations quickly, how do you actually pick among them? Using an LLM to evaluate another LLM's output is common practice, but prompting an LLM to give very accurate and consistent evaluations of text output is a challenge. Any breakthroughs here will accelerate progress!</p><p>An exciting trend has been a move toward multi-agent systems. What if, instead of having only a single agent, we have one agent to do research and gather information, a second agent to analyze the research, and a third to write the final report? Each of these agents can be built on the same LLM using a different prompt that causes it to play a particular, assigned role. Another common design pattern is to have one agent write and a second agent work as a critic to give constructive feedback to the first agent to help it improve. This can result in much higher-quality output. Open-source frameworks like Microsoft’s <a class=" wrap external" href="https://microsoft.github.io/autogen/?utm_campaign=The%20Batch&amp;utm_source=hs_email&amp;utm_medium=email&amp;_hsenc=p2ANqtz-87IEkio4ktkOH9VkmcWuq2BgtXjYHRZma0dP7QYPpRBg1l-KOxs3aG7c2Wnxp0D-BYC7sE" rel="nofollow noreferrer" target="_blank">AutoGen</a>, <a class=" wrap external" href="https://crewai.net/?utm_campaign=The%20Batch&amp;utm_source=hs_email&amp;utm_medium=email&amp;_hsenc=p2ANqtz-87IEkio4ktkOH9VkmcWuq2BgtXjYHRZma0dP7QYPpRBg1l-KOxs3aG7c2Wnxp0D-BYC7sE" rel="nofollow noreferrer" target="_blank">Crew AI</a>, and <a class=" wrap external" href="https://python.langchain.com/docs/langgraph?utm_campaign=The%20Batch&amp;utm_source=hs_email&amp;utm_medium=email&amp;_hsenc=p2ANqtz-87IEkio4ktkOH9VkmcWuq2BgtXjYHRZma0dP7QYPpRBg1l-KOxs3aG7c2Wnxp0D-BYC7sE" rel="nofollow noreferrer" target="_blank">LangGraph</a> are making it easier for developers to program multiple agents that collaborate to get a task done. <br /><br />I’ve been playing with many agent systems myself, and I think they are a promising approach to architecting intelligent systems. A lot of progress has been made by scaling up LLMs, and this progress no doubt will continue. But big ideas are sometimes made up of many, many little ideas. (For example, you might arrive at an important mathematical theorem via lots of little derivation steps.) Today’s LLMs can reason and have lots of “little ideas” in the sense that they take in information and make basic inferences. <a class=" wrap external" href="https://arxiv.org/pdf/2201.11903.pdf?utm_campaign=The%20Batch&amp;utm_source=hs_email&amp;utm_medium=email&amp;_hsenc=p2ANqtz-87IEkio4ktkOH9VkmcWuq2BgtXjYHRZma0dP7QYPpRBg1l-KOxs3aG7c2Wnxp0D-BYC7sE" rel="nofollow noreferrer" target="_blank">Chain-of-thought prompting</a> shows that guiding an LLM to think step-by-step — that is, to string together many basic inferences — helps it to answer questions more accurately than asking it to leap to a conclusion without intermediate steps.</p><p>Agent programming models are a promising way to extend this principle significantly and guide LLMs to have lots of little ideas that collectively constitute bigger and more useful ideas. </p><p>Keep learning! <br />Andrew</p><p>P.S. New short course: “Open Source Models with Hugging Face,” taught by Maria Khalusova, Marc Sun, and Younes Belkada! Hugging Face has been a game changer by letting you quickly grab any of hundreds of thousands of already-trained open source models to assemble into new applications. This course teaches you best practices for building this way, including how to search and choose among models. You’ll learn to use the Transformers library and walk through multiple models for text, audio, and image processing, including zero-shot image segmentation, zero-shot audio classification, and speech recognition. You’ll also learn to use multimodal models for visual question answering, image search, and image captioning. Finally, you’ll learn how to demo what you build locally, on the cloud, or via an API using Gradio and Hugging Face Spaces. <a class=" wrap external" href="https://www.deeplearning.ai/short-courses/open-source-models-hugging-face/?utm_campaign=The%20Batch&amp;utm_source=hs_email&amp;utm_medium=email&amp;_hsenc=p2ANqtz-87IEkio4ktkOH9VkmcWuq2BgtXjYHRZma0dP7QYPpRBg1l-KOxs3aG7c2Wnxp0D-BYC7sE" rel="nofollow noreferrer" target="_blank">Please sign up here</a> </p><hr /><p class="ztext-empty-paragraph"><br /></p><p>亲爱的朋友们,<br /><br />基于LLM的智能体可以自主规划和执行一系列动作，其发展非常迅速，还能持续看到每个月的进步。许多项目试图完成“写一篇关于X主题的报告”这样的任务，并自主地采取浏览网页等行动来收集信息以合成报告。<br /><br />AI智能体可以被设计成采取许多不同类型的行动。研究智能体（如许多建立在AutoGPT, GPTresearcher或STORM上的项目）会搜索网络并获取网页。销售代表智能体可以将产品分发给用户。工业自动化智能体可以控制机器人。<br /><br />到目前为止，我看到浏览web的智能体进展得更快，因为实验成本低，这是实现快速技术进步的关键。获取一个网页很便宜，如果你的智能体做出不好的选择，读错了页面，也没什么大不了的。相比之下，派发产品或移动实体机器人是成本高昂的动作，这使得快速实验变得困难。类似地，生成代码的智能体（可以在沙盒环境中运行）的运行成本相对较低，从而带来快速的实验和进展。<br /><br />虽然今天的研究智能体的主要任务是收集和综合信息，仍处于早期发展阶段，但我期待看到快速的改进。ChatGPT、必应聊天和Gemini已经可以浏览网页，但他们的在线研究往往是有限的；这有助于他们快速回到用户身边。但我期待着下一代的智能体，它们可以花几分钟甚至几个小时进行深入研究，然后向你返回一个输出。这样的算法能够生成比只获取一到两页就返回答案的模型更好的答案。<br /><br />即使实验过程很快，评估仍然是开发的瓶颈。如果你可以快速尝试10种不同的算法，你该如何从中做出选择呢？使用LLM来评估另一个LLM的输出是一种常见的做法，但促使LLM对文本输出给出非常准确和一致的评估是一项挑战。这些方面的任何突破都将加速进步！<br /><br />一个令人兴奋的趋势是朝着多智能体系统 (multi-agent systems) 的方向发展。如果不是只有一个智能体，而是有一个智能体做研究和收集信息，第二个智能体分析研究，第三个智能体撰写最终报告呢？这些智能体中的每一个都可以在同一个LLM上构建，使用不同的提示符，使其发挥指定的特定作用。另一个常见的设计模式是让一个智能体写作，另一个智能体作为评论家，向第一个智能体提供建设性的反馈，以帮助其改进。这可以产生更高质量的输出。诸如微软的AutoGen, Crew AI和LangGraph这样的开源框架使开发人员更容易编程多个智能体来协作完成任务。<br /><br />我自己也测试过许多智能体系统，我认为它们是构建智能系统的一种很有前途的方法。通过扩大LLM的规模已经取得了很多进展，毫无疑问，这种发展将继续下去。但伟大的想法有时是由很多很多小想法组成的。（例如，你可能通过许多小的推导步骤得出一个重要的数学定理。）如今的LLM可以执行推理，并且有很多“小想法”，因为它们可以接受信息并做出基本的推断。思维链提示表明，引导LLM循序渐进地思考——将许多基本推论串在一起——比要求它在没有中间步骤的情况下直接得出结论更能准确地回答问题。<br /><br />智能体编程模型是一种很有前途的方式，可以显著扩展这一原则，并指导LLM拥有许多小想法，这些小想法会共同构成更大、更有用的想法。<br /><br />请不断学习!<br />吴恩达<br /><br />附注:新的短期课程:“Open Source Models with Hugging Face”，由Maria Khalusova, Marc Sun和Younes Belkada教授！Hugging Face已经改变了游戏规则，它可以让你快速从数十万个已经训练有素的开源模型中挑选任何一个，组装成新的应用程序。本课程将教你构建这种方式的最佳实践，包括如何搜索和选择模型。你将学习使用transformer库，并通过多个模型进行文本、音频和图像处理，包括零样本图像分割、零样本音频分类和语音识别。你还将学习使用多模态模型进行视觉问题回答，图像搜索和图像字幕。最后，你将学习如何演示在本地，云端或通过使用Gradio和Hugging Face Spaces调用API构建的内容。请<a class=" wrap external" href="https://www.deeplearning.ai/short-courses/open-source-models-hugging-face/?utm_campaign=The%20Batch&amp;utm_source=hs_email&amp;utm_medium=email&amp;_hsenc=p2ANqtz-87IEkio4ktkOH9VkmcWuq2BgtXjYHRZma0dP7QYPpRBg1l-KOxs3aG7c2Wnxp0D-BYC7sE" rel="nofollow noreferrer" target="_blank">点此</a>进行注册。</p>
]]></content:encoded>
<pubDate>Thu, 07 Mar 2024 12:01:06 GMT</pubDate>
</item>
</channel>
</rss>