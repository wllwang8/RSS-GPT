<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
<channel>
<title>数据学习的知乎动态</title>
<link>https://www.zhihu.com/people/datalearner/activities</link>


<item>
<title>数据学习赞同了回答: 小米和华为比到底有什么区别？</title>
<link>https://www.zhihu.com/question/356544088/answer/2968443600</link>
<guid>https://www.zhihu.com/question/356544088/answer/2968443600</guid>
<content:encoded><![CDATA[
<div> 关键词: 小米, 华为, 面试, 专业, 诚恳

总结:<br /><br />
小米面试体验专业而诚恳，面试官谈及公司现状，重点在手机性能和创新方面遇到瓶颈，研发方向偏向智选品牌；华为面试较为不尽如人意，技术部门问题边角无关痒，对造车进度和研究方向支支吾吾，似乎缺乏清晰的定位。小米展现出对困难的艰难对抗态度，华为印象偏向组装厂贴牌企业，面试体验略显不足。整体体验展现小米专业性和诚恳态度，华为靠贴牌组装的形象。 <div>
<p>不请自来，soc设计博士，上周末面试了华为，今天面试了小米。 </p><p> 首先是小米，主管单独面试，过程略过，最后和主管聊了一会儿，主管很诚恳的告诉我经过小米投资上百亿造车，确实对手机业务有影响。手机性能和创新都遇到瓶颈，所以退而求其次只能多出智选品牌（本人相关专业，这么描述肯定非常准确），因此研发并不困难，希望我进去后的研发方向是怎么多给手机起名字，从而降低研发费用。因为是春招尾声，没有工程师岗位了，只有博后岗位，所以最后还是放弃了。总体感觉，首先是很专业，虽然主管不是完全我的这个方向，但是问的问题非常一针见血，其次是很诚恳，关于小米的现状并不避讳谈及。也看得出小米也在尽力对抗目前的困难，虽然确实很艰难。  </p><p>好了，说回上周面试的华为，下午三个面试官，都是技术部门的。自我介绍结束后，问了我几个问题，不能说很不专业吧，但是都是边边角角无关痛痒的角度。因为华为我面的是电动车的芯片工程师，所以我问了一下华为造车进度，那边首先是夸夸其谈了一顿，我问细节的时候又支支吾吾说不出来，最后委婉告诉我差不多是贴牌组装的。然后我问了一下如果入职华为，研究方向是什么，是车BU还是海思，结果又是一顿支支吾吾，最后和我说可能没有研究环境，进去是和外包公司进行对接。我真的一直以为华为是和苹果对标的科技公司，今天才知道华为完完全全是一个组装厂，全靠贴牌，不知道是不是电子产品届的南极人。 </p><p> 以上是我个人面试两个公司的感受，因为没有进去工作，所以公司氛围环境一概不知，只是针对两次面试体验有感而发。</p>
]]></content:encoded>
<pubDate>Sat, 25 May 2024 09:15:56 GMT</pubDate>
</item>
<item>
<title>数据学习赞同了回答: 现在（2024年5月）是不是解放台湾最好的时机？</title>
<link>https://www.zhihu.com/question/655104271/answer/3500028208</link>
<guid>https://www.zhihu.com/question/655104271/answer/3500028208</guid>
<content:encoded><![CDATA[
<div> 军事常识，战争中需要的毁伤火力，数十万量级的轰炸才有决定性影响，隐身战机扔精确制导炸弹是主要攻击方式。<br />
海上作战需要考虑海峡宽度、地雷、水雷、重机枪等因素，跨海作战难度高，需要更多兵力。<br />
海湾战争和俄乌战争案例，都展示了陆地作战和跨海攻击的巨大差异。海上进攻容易遭受对方飞机、舰船、潜艇攻击，损失巨大。<br />
最后，跨海攻击风险高，难以保证制海权、制空权，攻占狭窄海峡对手都需要持续战斗才能成功。<br />
总结: 军事行动中，海上作战跨海攻击要面临制空权、制海权、攻防等多方面困难，需要巨大兵力和战略勇气。 <div>
<p>纯粹的讲解基础军事常识，剩下的事情你们自己判断。</p><p>首先，那些张口就是什么“导弹洗地”“远程火箭弹洗地”的都是典型的军盲，直接可以离开，不用再往下看了。</p><p>军盲最大的问题在于对战争中所需要的毁伤火力完全没有概念，总是幻想发射几千几万枚常规导弹能起作用。还有所谓三百公里的远程火箭弹，那玩意儿其实就是短程导弹！只是换了个叫法而已，付出的价格也是短程导弹的价格，十分鸡肋，只有军盲才会吹。俄罗斯打到现在发射了上万枚各型导弹，影响基本就是0，听个响而已。因为现实战争中，需要的轰炸毁伤火力，都是以数十万计，500公斤级炸弹，扔几十万颗以上才能对战争有决定性影响，中短程导弹战斗部不过三五百公斤而已，想对战争有决定性影响，也得发射几十万枚，但这显然不现实。这还仅仅只是影响，想取得最后的胜利，数量要继续往上翻。所以在现实中，都是靠隐身战机扔精确制导炸弹，<span class="nolink">精准度</span>高、威力大、量大还便宜，可以真正的炸残敌人，影响战争走向。</p><p>有很多武器看似不起眼，实则很恐怖，例如地雷、水雷、重机枪。海峡平均宽度200公里，岛上能登陆的地点就那么几个，几十年来地上地下都是永久的防御工事。地上有雷达天上有卫星，大规模的跨海作战，集结的第一时间就会被发现，但凡真有点风吹草动，立刻就会在海里布满水雷，滩头布满地雷，现在的驱逐舰都是皮薄馅大，碰到一颗水雷，直接就废了。像俄军防御乌军的进攻，8成靠地雷，剩下2成靠榴弹炮和重机枪，什么匕首、伊斯坎德尔、图160之类的玩意儿，从开战到现在起到的作用基本是0。用火炮覆盖滩头，射击诸元都是现成的。暗堡里重机枪火力非常突然猛烈，冷不丁一串子弹打过来，覆盖一片区域，来不及躲避也没地方躲。</p><p>至于什么围而不打围困几个月一两年之类的说法，完全没有常识，围困不等于拿着大喇叭告诉全世界要开打了吗？让美日充分做准备，况且能怎么围困？到时候联合国再一投票，什么人道主义运输之类的，美日立刻就站在国际舆论的制高点，自己只会搞的极其被动，还不如直接进攻。</p><p>先空袭几个月那就更扯了，最不怕的就是单纯的空袭，单纯的空袭那不更是拿着大喇叭告诉全世界开打了，一样是给美日充分的准备时间，并且支援防空更简单。</p><p>打仗不存在投机取巧，除了开战后就能快速上岛别无他法，只有这一条路有胜算，剩下的什么先围困几个月、先空袭几个月之类的，都是在拖延时间，时间一旦被拉长，后果不堪设想，压根就不可能这样搞。</p><p>海湾战争期间，以美国为首的联军，陆海军没打到伊拉克境内，全在科威特境内打，空军绝大部分也是轰炸科威特境内伊军，少部分轰炸了伊拉克境内目标，海湾战争99%是在科威特境内打。科威特领土面积只有1.7万平方公里，跟呼和浩特市一样大。</p><p>当时伊拉克号称有百万大军，但任何国家的军队都分精锐和杂牌。伊拉克是派了20万精锐到科威特，剩下的80万杂牌一直在国内。美军也是拿出自己的50万精锐加上其他国家的20万，组成70万联军。</p><p>海湾战争是以美军为主的70万联军，在呼和浩特市这么大地方，打20万伊军。还得加上美军武器对伊军的领先程度，再加上飞机、军舰先炸了40多天，然后才发起总攻。</p><p>当时伊军还很完整，以联军对伊军的差距，在一个市大的地方，进攻一方70万，防守一方20万，这才是战争的标准规模。</p><p>而这，还都是在接壤的陆地上，不需要横跨200公里的大海。</p><p>但是，以联军70万的兵力，在那么大优势的情况下，把伊军赶出科威特以后，依然不敢贸然打到伊拉克境内。这些兵力贸然进攻一个43万平方公里的国家还是不保险，所以萨达姆屈服以后，美国也是见好就收。</p><p>海湾战争，在科威特的10万伊军精锐被消灭，另外10万也被打残，国内剩下的都是没有战斗力的杂牌。之后美国熬了伊拉克十几年，各种制裁，没事还得轰炸一下，同时“里里外外”做足准备工作，在伊军战斗力、萨达姆的统治力、军心民心都跌到谷底的时候，才又出动20万部队直扑首都巴格达，还是打一个市。</p><p>打下巴格达，萨达姆就倒台了。</p><p>海湾战争时期美军打伊军是降维打击，十几年后，美军再打伊军是降两维，但依然要20万精锐才能打下一个市。</p><p>越南领土面积相当于中国一个省，他的市相当于村镇。对越自卫反击战仅仅只是跨过省界，扫荡一下附近村镇，交战面积不到越南面积的二十分之一，快打快撤一个月，就这也得二三十万的部队。</p><p>俄军总计140万，和海、空、火箭再分一下，陆军只剩下30万人。结果俄罗斯吐血拿出一大半陆军，十几二十万企图去吞并一个60多万平方公里4000多万人口的国家，给乌克兰全国去军事化，令人匪夷所思，乌克兰面积是科威特的35倍，比伊拉克、越南、法国、德国都大。</p><p>十万人二十万人？够干啥的呀？居然企图吞并整个乌克兰，大帝从开始的第一步就走错了，注定要失败。</p><p>战争初期，俄军参战的都是精锐，但跟乌军比并不占优势。双方品质差不多，乌军几十万，俄罗斯想吞并乌克兰就得最低出150万陆军，这还是建立在陆地上接壤。美国在科威特打伊拉克都得组织70万联军，俄罗斯打乌克兰，150万多吗？而俄罗斯的操作匪夷所思，双方水平差不多，乌军40万，他派20万企图去吞并人家。结果打了两年多，俄军的精锐早被消耗没了，连坦克都消耗没了，自己先去军事化了。现在的主要成分是后方紧急抓来的罪犯和文盲，严重缺乏训练和装备。乌军有西方援助的武器和训练，品质要高于俄军。</p><p>现在俄军再想打下整个乌克兰，必须靠人数来弥补品质的差距，得翻3倍：300万起步。在高效的现代武器面前，落后、缺乏武器的一方，300万其实并不够。</p><p>哪怕现在想夺取乌克兰一个州，一个标准市，保守估计也得50万往上才有可能。</p><p>俄罗斯和乌克兰陆上接壤相连，俄罗斯号称世界第二军事强国，打乌克兰的难度系数属于新手简单模式，即便是如此简单的模式，也照样打不赢。</p><p>以上的这些案例，全都是在接壤的陆地上，不需要横渡200公里的海峡。</p><p>如果要横渡200公里的海峡，攻占一个3.6万平方公里，两千多万人口，几十万常备军，经济发达的地方，需要多少兵力？</p><p>跨海作战的难度，跟陆地上接壤作战相比，难度有着本质的提升。</p><p>二战初期，德军横扫西欧，打英军玩一样，但他不敢跨海进攻英国本土。跨海作战的难度、风险是超乎想象的。</p><p>现在俄军已经被击毙40万，一场战争，炮弹的数量都是以千万计。乌军每天要消耗几千发炮弹，这个数合情合理。问题是已经打了两年多。按每天5000发炮弹算，两年总计得打360万发炮弹。折合下来平均9发炮弹才能打死1名俄军。</p><p>还有子弹、手榴弹、火箭弹、导弹、地雷，这些加一起才击毙40万俄军。</p><p>俄乌在地面打，平均下来，乌克兰大概是10发炮弹、几万发子弹、几十颗手榴弹、几发火箭弹+导弹+几颗地雷才能打死1名俄军。这还得是乌克兰很节省的情况下，换成美军得再加个零。</p><p>但跨海作战不一样。</p><p>一艘登陆舰里面装一个营的兵，对手来两发鱼雷，击沉一艘船就是消灭几百名士兵。登陆舰运的是少数打前锋的部队，大部队得用民船运。1艘民用货船撞上水雷，挨两颗炸弹，几千人会被瞬间团灭。一艘潜艇发射10枚鱼雷，击沉5艘运兵船，消灭1万人，相当于10万发炮弹，几亿发子弹，几十万颗手榴弹的战果。</p><p>先头部队上去了，人家飞机、军舰、潜艇把你后勤补给的船打沉，把运输后续部队船打沉，前方没弹药、没粮食，没援兵，只能坐等被消灭。</p><p>跨海作战极其凶险，难度极高。</p><p>英吉利海峡最窄处只有34公里，人甚至都能游过去。但在没有绝对的把握，没有绝对的制海权、制空权的情况下，狂暴如元首，凶猛如德军，依然不敢打。</p><p>海峡平均宽度差不多200公里，最窄处也有130公里，登陆舰、运输船走完海峡，再卸船，差不多要将近一天，这中间非常容易遭到攻击，一旦被击中，损失非常大。</p>
]]></content:encoded>
<pubDate>Sat, 25 May 2024 09:03:51 GMT</pubDate>
</item>
<item>
<title>数据学习赞同了回答: 张朝阳是一个怎样的人？</title>
<link>https://www.zhihu.com/question/19573482/answer/2812163541</link>
<guid>https://www.zhihu.com/question/19573482/answer/2812163541</guid>
<content:encoded><![CDATA[
<div> 老张、搜狐、真事、管理、文化
<br />
老张是搜狐创始人，有许多真事儿展现了他的个性，虽然不擅长管理，但打造了独特的企业文化，搜狐是一家宽松氛围的公司。老张偏爱自由，喜欢与明星合唱、组织员工爬山等活动。他对百度等竞争对手有看法，重视家庭关系，父母从小教育有方。尽管有些决策被批评，但老张毕生致力于搜狐的发展，独特的价值观和企业文化无法复制，搜狐的品牌影响力和员工忠诚度也得到认可。总的来说，老张是一位有个性和包容心的企业家，他为搜狐公司树立了独特的企业形象和文化。 
<br /><br />总结: <div>
<p>说几件老张的真事儿</p><p>1、清华科技园的所有写字楼包括搜狐大厦都是禁烟的，一天早上有人举报某层男厕所有人抽烟，保安给堵门口了，让里面的人出来，过了一会儿，老张非常淡定的从里面出来了...</p><p>2、一天在楼梯间闲聊，就听楼上一个人吹着口哨，点着雪茄往下走，当老张进过我们的时候，他也不吹哨了，我们也不说话了，下了两层，双方继续...</p><p>3、有保安看见老张从办公室出来了，赶紧给他按了一下下楼的电梯，老张看了他一眼，走向了另外一个电梯...</p><figure><img class="origin_image zh-lightbox-thumb lazy" src="https://pic1.zhimg.com/v2-56ef9ce114a08366cb2a111feedc1b54_1440w.jpg" /></figure><p>4、在一次高峰时间乘电梯，跟老张挤在一起了，我努力再努力，试图支撑一下不要碰到老板，两秒钟后，老板毫不留情的挤我挤到喘不上来气...</p><p>5、汪小菲跟大S结婚前两天，老张打算换一个新手机，也就是后来偷拍婚礼的那个手机，想把通讯录2000多人倒到新手机里，当时的程序员估计没有经历过这种压力测试，换机软件崩溃了，老张把搜狗输入法的大哥们叫去弄了半天，最终接受了搞不定的结果（搜狐那么多人，手动输入也搞定了），带着遗憾揣着两个手机去参加婚礼了</p><p>6、老张很少参加产品技术团队的年会，他喜欢参加搜狐娱乐的年会</p><p>7、搜狐运动会，老张经常自己开着小跑去赛场，有一年跑了个第四，你仔细品</p><p>8、老张跟员工交流的时候，你容易联想到马，懂得自然懂</p><figure><img class="origin_image zh-lightbox-thumb lazy" src="https://pic4.zhimg.com/v2-5e895e11599c000fe8789984c7ac8787_1440w.jpg" /></figure><p>9、老张喜欢说，当年李彦宏来面试，后来自己成立百度的故事，还喜欢大骂周鸿祎是流氓</p><p>10、老张是很佛系的人，必须说他不擅长管理，不擅长用人，绝对有知识分子的范儿，无论是搜狐微博还是白社会都是自己在卖力招呼，后来曾很痛心地说，做白社会是他这一生最二逼的事情。</p><p>11、尽管成绩平平，市值不高，但老张造就的搜狐，还有搜狐的这种独一无二的企业文化，是任何一家互联网公司都无法模仿的，一个企业的价值观有时候不一定跟钱有关，搜狐是我见过的最宽松的企业，每天楼下有大批喝茶聊天的员工，每个月20号提前发这个月的工资</p><p>12、当年老张的父母住在地坛公园附近，去看病都是很客气的跟老张的司机打电话。有一次搜狐搞活动，本来要去接他们，但后来老两口自己坐公交去的。从父母这一辈，给的教育和树立的榜样就不一样</p><p>13、老张喜欢跟各种明星一起唱歌，也热衷做公益，还喜欢组织高管爬山，曾经有一次爬山回来之后，又专门派人去山上捡回他们当时来不及清走的垃圾</p><p>14、他的优秀基因已经有人继承了，操心的和不放心的可以省心了</p><p>15、搜狐前台，哦，抱歉，这个话题不能聊</p><p><b>很怀念那段日子，狐尾一点红，离人眼中血...</b></p><p><b>祝福搜狐，祝福张朝阳！</b></p><p class="ztext-empty-paragraph"><br /></p><p><b>解答你有关互联网圈的疑问~</b></p><a></a><p class="ztext-empty-paragraph"><br /></p><figure><img class="origin_image zh-lightbox-thumb lazy" src="https://pic3.zhimg.com/v2-b97c04e62d519da3bd13ede40b06928a_1440w.jpg" /></figure><p></p>
]]></content:encoded>
<pubDate>Tue, 21 May 2024 09:32:55 GMT</pubDate>
</item>
<item>
<title>数据学习发表了文章: 一文总结OpenAI新发布的GPT-4o的能力和信息，免费ChatGPT用户也可以使用的GPT-4模型，开发者接口价格下降一半，数学推理评测结果大幅提升，...</title>
<link>https://zhuanlan.zhihu.com/p/697591062</link>
<guid>https://zhuanlan.zhihu.com/p/697591062</guid>
<content:encoded><![CDATA[
<div> GPT-4o, 模型更新, 多模态支持, 性能提升, 数学推理, 延迟降低
<br />
<br />
总结:文章介绍了OpenAI发布的最新基础模型GPT-4o，强调了其多模态支持和性能提升。GPT-4o相比于之前的版本在数学推理能力和响应速度上有显著提高。该模型在处理英语文本和代码方面表现优异，同时运行速度更快，成本更低。另外，GPT-4o在视觉和音频理解方面也有明显的优势，使得人机交互更加自然流畅。在不同评测数据集上，GPT-4o取得了令人瞩目的成绩，特别是在数学推理和编程方面，表现卓越。目前，GPT-4o已经开放使用，免费用户可以使用，Plus用户有更多的使用数量，开发者则可以享受更快且价格更低的API接口。 <div>
<p>本文原文来自DataLearnerAI官方网站：</p><a class=" wrap external" href="https://www.datalearner.com/blog/1051715644737472" rel="nofollow noreferrer" target="_blank">OpenAI发布的GPT-4o能力总结，数学推理能力超过所有模型，价格下降一半！</a><hr /><p>OpenAI在GPT-4发布一年之后再次更新其基础模型，发布最新的GPT-4o模型，其中o代表的是omni，即“全能”的意思。GPT-4o相比较此前最大的升级是对多模态的支持以及性能的提升，特别是数学推理能力有大幅提高。GPT-4o在各方面比GPT-4更强，但是速度更快，开发者接口的价格则只有一半！</p><figure><img class="origin_image zh-lightbox-thumb" src="https://pic2.zhimg.com/v2-0e2614c6c94d89a865b773776a3b8859_1440w.jpg" /></figure><p class="ztext-empty-paragraph"><br /></p><p class="ztext-empty-paragraph"><br /></p><ul><li><a class=" wrap external" href="https://www.datalearner.com/blog/1051715644737472#GPT-4o%E8%83%BD%E5%8A%9B%E4%BB%8B%E7%BB%8D" rel="nofollow noreferrer" target="_blank">GPT-4o能力介绍</a></li><li><a class=" wrap external" href="https://www.datalearner.com/blog/1051715644737472#GPT-4o%E7%9A%84%E7%BB%BC%E5%90%88%E8%AF%84%E6%B5%8B%E7%BB%93%E6%9E%9C" rel="nofollow noreferrer" target="_blank">GPT-4o的综合评测结果</a></li><li><a class=" wrap external" href="https://www.datalearner.com/blog/1051715644737472#GPT-4o%E8%83%8C%E5%90%8E%E7%9A%84%E6%A8%A1%E5%9E%8B%E6%8A%80%E6%9C%AF" rel="nofollow noreferrer" target="_blank">GPT-4o背后的模型技术</a></li><li><a class=" wrap external" href="https://www.datalearner.com/blog/1051715644737472#GPT-4o%E7%9A%84%E5%BC%80%E6%94%BE%E4%BD%BF%E7%94%A8" rel="nofollow noreferrer" target="_blank">GPT-4o的开放使用</a></li></ul><p class="ztext-empty-paragraph"><br /></p><h3><b>GPT-4o能力介绍</b></h3><p>GPT-4o的特点和优势总结如下：</p><ol><li><b>多模态输入输出</b>：GPT-4o能够接受文本、音频和图像的任意组合作为输入，并生成文本、音频和图像的任意组合作为输出。这意味着它可以更自然地与人进行交互，不仅限于文字交流。</li><li><b>响应速度</b>：GPT-4o对音频输入的响应时间可以快到232毫秒，平均为320毫秒，这与人类在对话中的响应时间相近，提供了更即时的互动体验。在GPT-4o之前的模型，GPT-3.5的语音模式响应约2.8秒，而GPT-4的语音响应约5.4秒，可以看到语音速度响应大幅提高！</li><li><b>性能和成本优势</b>：在处理英语文本和代码方面，GPT-4o的性能与GPT-4 Turbo相当，但在处理非英语文本时有显著改进。同时，它的运行速度更快，使用API的成本降低了50%。</li><li><b>视觉和音频理解能力</b>：与现有模型相比，GPT-4o在视觉和音频理解方面有更好的表现，这意味着它在处理图像和声音时更加准确和有效。</li></ol><p>综上所述，GPT-4o是一种多模态、快速、高效且成本更低的模型，特别在视觉和音频处理方面表现优越，使人机交互更加自然流畅。</p><h3><b>GPT-4o的综合评测结果</b></h3><p>官方发布了GPT-4o在不同评测数据集的结果，其中MMLU评分88.7分，是截止目前为止，作为综合大模型最高的得分。而MATH数学得分76.6，大幅提高，MATH作为数学推理能力测评，一种都非常困难。此前，最高得分是Claude Opus的60.1分，也就是说GPT-4o在MATH数学推理上至少比当前市场上最好的模型提高27.5%！应该说非常强悍。</p><figure><img class="origin_image zh-lightbox-thumb" src="https://pic4.zhimg.com/v2-b750b7bd3b30b0abc0f03e51086ff8e3_1440w.jpg" /></figure><p class="ztext-empty-paragraph"><br /></p><p>数据来源：<a class=" external" href="https://www.datalearner.com/ai-models/leaderboard/datalearner-llm-leaderboard" rel="nofollow noreferrer" target="_blank"><span class="invisible">https://www.</span><span class="visible">datalearner.com/ai-mode</span><span class="invisible">ls/leaderboard/datalearner-llm-leaderboard</span><span class="ellipsis"></span></a></p><p>而作为一个综合大模型，GPT-4o在编程的评测结果上也大幅提升。Human Eval的评测得分90.5分，在GPT-4基础上继续提高了5分。也是目前全球所有综合大模型以及编程大模型水平得分最高的一个。</p><figure><img class="origin_image zh-lightbox-thumb" src="https://pic1.zhimg.com/v2-ac661aefa3763f9a5c9bec49dea5830c_1440w.jpg" /></figure><p class="ztext-empty-paragraph"><br /></p><p>数据来源：<a class=" external" href="https://www.datalearner.com/ai-models/leaderboard/datalearner-llm-coding-leaderboard" rel="nofollow noreferrer" target="_blank"><span class="invisible">https://www.</span><span class="visible">datalearner.com/ai-mode</span><span class="invisible">ls/leaderboard/datalearner-llm-coding-leaderboard</span><span class="ellipsis"></span></a></p><p>除了语言相关的评测大幅提升外。GPT-4o在多模态能力也有大幅提高。其中自动语音识别ASR（Auto Speech Recognition）部分比此前Whipser-V3-Large有了明显提升，错误识别率大幅下降：</p><figure><img class="origin_image zh-lightbox-thumb" src="https://pic3.zhimg.com/v2-d2fd3f8480c7a8358dcf429fe686262a_1440w.jpg" /></figure><p class="ztext-empty-paragraph"><br /></p><h3><b>GPT-4o背后的模型技术</b></h3><p>一如既往，没有任何信息。唯一官方透露的信息是，GPT-4o是一个端到端的跨文本、视觉、语音的模型。因此，所有的输入都在一个网络中进行。</p><h3><b>GPT-4o的开放使用</b></h3><p>目前，官网和APP都可以使用这个模型。免费用户也可以使用！Plus用户有5倍信息的使用数量！</p><p>对于开发者来说，GPT-4o的API接口快2倍，价格只有一半！</p><p>原文来自DataLearnerAI官方网站：<a class=" wrap external" href="https://www.datalearner.com/blog/1051715644737472" rel="nofollow noreferrer" target="_blank">OpenAI发布的GPT-4o能力总结，数学推理能力超过所有模型，价格下降一半！ | 数据学习者官方网站(Datalearner)</a></p>
]]></content:encoded>
<pubDate>Tue, 14 May 2024 00:17:04 GMT</pubDate>
</item>
<item>
<title>数据学习回答了问题: 如何评价 OpenAI 2024 Spring 发布的支持实时语音对话的模型 GPT-4O?</title>
<link>https://www.zhihu.com/question/655916303/answer/3497584753</link>
<guid>https://www.zhihu.com/question/655916303/answer/3497584753</guid>
<content:encoded><![CDATA[
<div> OpenAI、GPT-4o、多模态、性能提升、成本优势
<br />
<br />
总结：OpenAI在GPT-4发布一年后推出最新的GPT-4o模型，强调其支持多模态输入输出、响应速度快，处理效率高且成本低。GPT-4o在数学推理能力、视觉和音频理解方面表现优异，综合评测结果也显示其在MMLU和MATH等评测数据集上得分高，推动了综合大模型和编程大模型的发展。此外，GPT-4o的API接口快速且价格便宜，开放使用范围广泛，对开发者具有吸引力。 <div>
<p>本文原文来自DataLearnerAI官方网站：</p><a class=" wrap external" href="https://www.datalearner.com/blog/1051715644737472" rel="nofollow noreferrer" target="_blank">OpenAI发布的GPT-4o能力总结，数学推理能力超过所有模型，价格下降一半！</a><hr /><p>OpenAI在GPT-4发布一年之后再次更新其基础模型，发布最新的GPT-4o模型，其中o代表的是omni，即“全能”的意思。GPT-4o相比较此前最大的升级是对多模态的支持以及性能的提升，特别是数学推理能力有大幅提高。GPT-4o在各方面比GPT-4更强，但是速度更快，开发者接口的价格则只有一半！</p><figure><img class="origin_image zh-lightbox-thumb lazy" src="https://pic2.zhimg.com/v2-0e2614c6c94d89a865b773776a3b8859_1440w.jpg" /></figure><p class="ztext-empty-paragraph"><br /></p><p class="ztext-empty-paragraph"><br /></p><ul><li><a class=" wrap external" href="https://www.datalearner.com/blog/1051715644737472#GPT-4o%E8%83%BD%E5%8A%9B%E4%BB%8B%E7%BB%8D" rel="nofollow noreferrer" target="_blank">GPT-4o能力介绍</a></li><li><a class=" wrap external" href="https://www.datalearner.com/blog/1051715644737472#GPT-4o%E7%9A%84%E7%BB%BC%E5%90%88%E8%AF%84%E6%B5%8B%E7%BB%93%E6%9E%9C" rel="nofollow noreferrer" target="_blank">GPT-4o的综合评测结果</a></li><li><a class=" wrap external" href="https://www.datalearner.com/blog/1051715644737472#GPT-4o%E8%83%8C%E5%90%8E%E7%9A%84%E6%A8%A1%E5%9E%8B%E6%8A%80%E6%9C%AF" rel="nofollow noreferrer" target="_blank">GPT-4o背后的模型技术</a></li><li><a class=" wrap external" href="https://www.datalearner.com/blog/1051715644737472#GPT-4o%E7%9A%84%E5%BC%80%E6%94%BE%E4%BD%BF%E7%94%A8" rel="nofollow noreferrer" target="_blank">GPT-4o的开放使用</a></li></ul><p class="ztext-empty-paragraph"><br /></p><h3><b>GPT-4o能力介绍</b></h3><p>GPT-4o的特点和优势总结如下：</p><ol><li><b>多模态输入输出</b>：GPT-4o能够接受文本、音频和图像的任意组合作为输入，并生成文本、音频和图像的任意组合作为输出。这意味着它可以更自然地与人进行交互，不仅限于文字交流。</li><li><b>响应速度</b>：GPT-4o对音频输入的响应时间可以快到232毫秒，平均为320毫秒，这与人类在对话中的响应时间相近，提供了更即时的互动体验。在GPT-4o之前的模型，GPT-3.5的语音模式响应约2.8秒，而GPT-4的语音响应约5.4秒，可以看到语音速度响应大幅提高！</li><li><b>性能和成本优势</b>：在处理英语文本和代码方面，GPT-4o的性能与GPT-4 Turbo相当，但在处理非英语文本时有显著改进。同时，它的运行速度更快，使用API的成本降低了50%。</li><li><b>视觉和音频理解能力</b>：与现有模型相比，GPT-4o在视觉和音频理解方面有更好的表现，这意味着它在处理图像和声音时更加准确和有效。</li></ol><p>综上所述，GPT-4o是一种多模态、快速、高效且成本更低的模型，特别在视觉和音频处理方面表现优越，使人机交互更加自然流畅。</p><h3><b>GPT-4o的综合评测结果</b></h3><p>官方发布了GPT-4o在不同评测数据集的结果，其中MMLU评分88.7分，是截止目前为止，作为综合大模型最高的得分。而MATH数学得分76.6，大幅提高，MATH作为数学推理能力测评，一种都非常困难。此前，最高得分是Claude Opus的60.1分，也就是说GPT-4o在MATH数学推理上至少比当前市场上最好的模型提高27.5%！应该说非常强悍。</p><figure><img class="origin_image zh-lightbox-thumb lazy" src="https://pic4.zhimg.com/v2-b750b7bd3b30b0abc0f03e51086ff8e3_1440w.jpg" /></figure><p class="ztext-empty-paragraph"><br /></p><p>数据来源：<a class=" external" href="https://www.datalearner.com/ai-models/leaderboard/datalearner-llm-leaderboard" rel="nofollow noreferrer" target="_blank"><span class="invisible">https://www.</span><span class="visible">datalearner.com/ai-mode</span><span class="invisible">ls/leaderboard/datalearner-llm-leaderboard</span><span class="ellipsis"></span></a></p><p>而作为一个综合大模型，GPT-4o在编程的评测结果上也大幅提升。Human Eval的评测得分90.5分，在GPT-4基础上继续提高了5分。也是目前全球所有综合大模型以及编程大模型水平得分最高的一个。</p><figure><img class="origin_image zh-lightbox-thumb lazy" src="https://pic1.zhimg.com/v2-ac661aefa3763f9a5c9bec49dea5830c_1440w.jpg" /></figure><p class="ztext-empty-paragraph"><br /></p><p>数据来源：<a class=" external" href="https://www.datalearner.com/ai-models/leaderboard/datalearner-llm-coding-leaderboard" rel="nofollow noreferrer" target="_blank"><span class="invisible">https://www.</span><span class="visible">datalearner.com/ai-mode</span><span class="invisible">ls/leaderboard/datalearner-llm-coding-leaderboard</span><span class="ellipsis"></span></a></p><p>除了语言相关的评测大幅提升外。GPT-4o在多模态能力也有大幅提高。其中自动语音识别ASR（Auto Speech Recognition）部分比此前Whipser-V3-Large有了明显提升，错误识别率大幅下降：</p><figure><img class="origin_image zh-lightbox-thumb lazy" src="https://pic3.zhimg.com/v2-d2fd3f8480c7a8358dcf429fe686262a_1440w.jpg" /></figure><p class="ztext-empty-paragraph"><br /></p><h3><b>GPT-4o背后的模型技术</b></h3><p>一如既往，没有任何信息。唯一官方透露的信息是，GPT-4o是一个端到端的跨文本、视觉、语音的模型。因此，所有的输入都在一个网络中进行。</p><h3><b>GPT-4o的开放使用</b></h3><p>目前，官网和APP都可以使用这个模型。免费用户也可以使用！Plus用户有5倍信息的使用数量！</p><p>对于开发者来说，GPT-4o的API接口快2倍，价格只有一半！</p><p>原文来自DataLearnerAI官方网站：<a class=" wrap external" href="https://www.datalearner.com/blog/1051715644737472" rel="nofollow noreferrer" target="_blank">OpenAI发布的GPT-4o能力总结，数学推理能力超过所有模型，价格下降一半！ | 数据学习者官方网站(Datalearner)</a></p>
]]></content:encoded>
<pubDate>Tue, 14 May 2024 00:17:04 GMT</pubDate>
</item>
<item>
<title>数据学习赞同了回答: 为什么互联网公司很少用oppovivo的手机呢？</title>
<link>https://www.zhihu.com/question/475086611/answer/2033570791</link>
<guid>https://www.zhihu.com/question/475086611/answer/2033570791</guid>
<content:encoded><![CDATA[
<div> 用户画像、腾讯、oppo、vivo、互联网公司
<br /><br />总结:腾讯的大数据显示，oppo和vivo用户主要分布在三四线城市和农村，学历较低，收入不高，女性用户占比较高。这四个特点与互联网公司从业者的情况存在冲突，因此互联网公司很少见oppo和vivo用户。根据酷安社区的例子，即使是绿厂员工也不一定会选择用自家手机，更不用说其他互联网企业了。 <div>
<p>这个原因很简单，腾讯曾经出过一期手机用户画像大数据。</p><figure><img class="origin_image zh-lightbox-thumb lazy" src="https://pic4.zhimg.com/v2-5ea4905c376de5775a8e35db906a3fd7_1440w.jpg" /></figure><p>其中，oppo用户的画像是这样的。</p><figure><img class="origin_image zh-lightbox-thumb lazy" src="https://pic1.zhimg.com/v2-7d5a6e61051e8652c8004648fb550708_1440w.jpg" /></figure><p>vivo用户画像是这样的。</p><figure><img class="origin_image zh-lightbox-thumb lazy" src="https://pic1.zhimg.com/v2-2c31c141769d9c51ea40d282e36302d8_1440w.jpg" /></figure><p class="ztext-empty-paragraph"><br /></p><p class="ztext-empty-paragraph"><br /></p><p>从上面可以看出，oppo vivo用户有以下四个特征:</p><p>1.主要分布在三四线城市和农村。</p><p>2，学历较低，大学本科以上学历只有5%左右，而友商苹果华为小米均在10%以上。</p><p>3，收入较低，月收入8000以上的仅有5%。</p><p>4.女性用户占比远大于男性用户占比。</p><p>很明显，互联网公司从业者和oppovivo用户的这四个特点完全冲突。三四线农村比较少互联网企业，本科以下学历从事互联网基本属于扯淡，互联网从业者也不至于月收入8000以下，互联网工作者较少女性。</p><p>所以说，互联网公司很少见oppovivo，是因为oppovivo用户普遍存在于三四线农村学历较低，能够胜任互联网工作的比较稀少。</p><p class="ztext-empty-paragraph"><br /></p><p class="ztext-empty-paragraph"><br /></p><p class="ztext-empty-paragraph"><br /></p><p class="ztext-empty-paragraph"><br /></p><p>ps:酷安社区也有几个绿厂员工，有一个用的三星note10+,有两个用的iphone12系列，还有一个用的小米11ultra，只有一个一加工程师坚持用自家手机一加8pro。连自家员工都做不到用自家手机，何况是别的互联网企业呢？</p>
]]></content:encoded>
<pubDate>Mon, 13 May 2024 01:19:21 GMT</pubDate>
</item>
<item>
<title>数据学习赞同了回答: 如何看待美国商务部将一批中国量子研究机构如中科大、物理所等加入「实体清单」？</title>
<link>https://www.zhihu.com/question/655547056/answer/3494661796</link>
<guid>https://www.zhihu.com/question/655547056/answer/3494661796</guid>
<content:encoded><![CDATA[
<div> onedrive, 学校, 数据备份, 科研, 云存储服务
<br /><br />总结:
学校中的数据备份工作已经全面展开，各个学校在疯狂备份数据，其中提到了学校并不推荐使用onedrive，因为学校有自己的云存储服务并有每人几TB的空间。虽然onedrive是免费提供的，但科研数据一般不会存储在onedrive中。学校可能仍在维护onedrive的原因是因为它能提供大量的免费容量，对于一些学生来说，免费的云存储服务很有吸引力。 <div>
<p>制裁的第一波影响已来</p><p>全校都在疯狂备份数据</p><p>Σ(ŎдŎ|||)ﾉﾉ</p><figure><img class="origin_image zh-lightbox-thumb lazy" src="https://pic4.zhimg.com/v2-954f5c115cadf2d7bbd922845aa1ec9f_1440w.jpg" /></figure><p class="ztext-empty-paragraph"><br /></p><hr /><p>我来补充一下，onedrive不是学校官方提倡大家使用的，学校应该也没有为onedrive付过费。onedrive的教育版是<b>微软免费提供的</b>，各个学校只要愿意维护，应该都有。除此之外，<b>科大有自己的云存储服务</b>，每人也有几TB空间。</p><p>那为啥科大网络中心还在维护onedrive嘞，单纯是因为免费量大啊。各位有所不知，onedrive教育版之前每人有5TB容量，且完全免费，并且多端同步很爽啊，给穷学生们存存照片、存存视频啥的挺好的，毕竟国产的云存储服务很多学生用不起啊，免费的羊毛当然是能薅则薅了。(˃ ⌑ ˂ഃ )</p><p>不用担心科研数据啦，不会存onedrive的，科大再穷，各个课题组存储服务器还是买得起的。</p>
]]></content:encoded>
<pubDate>Mon, 13 May 2024 01:13:56 GMT</pubDate>
</item>
<item>
<title>数据学习发表了文章: 阿里开源截止目前为止参数规模最大的Qwen1.5-110B模型：MMLU评测接近Llama-3-70B，略超Mixtral-8×22B！</title>
<link>https://zhuanlan.zhihu.com/p/694895756</link>
<guid>https://zhuanlan.zhihu.com/p/694895756</guid>
<content:encoded><![CDATA[
<h2><b>本文原文来自DataLearnerAI官方网站：</b></h2><a class=" wrap external" href="https://www.datalearner.com/blog/1051714140775766" rel="nofollow noreferrer" target="_blank">阿里开源截止目前为止参数规模最大的Qwen1.5-110B模型：MMLU评测接近Llama-3-70B，略超Mixtral-8×22B！</a><p>Qwen1.5系列是阿里开源的一系列大语言模型，也是目前为止最强开源模型之一。Qwen1.5是Qwen2的beta版本，此前开源的模型最大参数规模都是720亿，和第一代模型一样。就在刚刚，阿里开源了1100亿参数规模的Qwen1.5-110B模型。评测结果显示MMLU略超Llama3-70B和Mixtral-8×22B。DataLearnerAI实测结果，<b>相比Qwen1.5-72B模型来说，Qwen1.5-110B模型复杂任务的逻辑提升比较明显！</b></p><h3><b>Qwen1.5-110B模型简介</b></h3><p>在开源大模型领域，最大的模型参数规模通常不会超过700亿参数规模。最近2个月，国外开源的DBRX、Mixtral-8×22B-MoE是最新的超过1000亿参数规模的模型。而国内此前开源领域最大的参数模型是720亿参数规模的Qwen1.5-72B规模和650亿参数的深圳元象科技开源的XVERSE-65B。</p><p>这次阿里开源的1100亿参数规模的<b>Qwen1.5-110B模型是截止目前为止国内开源模型中参数规模最大的模型</b>。Qwen1.5-110B模型与其它Qwen1.5系列模型架构一致。采用了分组查询注意力机制，因此推理效率很高。该模型最高支持32K上下文，并且支持多语言，包括英文、中文、法语、西班牙语、德语、俄语、韩语、日文等。</p><p>按照1100亿参数估计，<b>Qwen1.5-110B模型半精度的推理显存需要220GB</b>。</p><p>Qwen1.5-110B模型开源的版本包含基座模型和Chat优化版本，可以说诚意满满！</p><h3><b>Qwen1.5-110B模型的评测结果</b></h3><p>根据官方公布的评测结果，Qwen1.5-110B模型的评测结果略略超过Llama-3-70B和Mixtral-8×22B。也比Qwen1.5-72B模型本身更强，这几个模型的评测结果对比如下：</p><table><tbody><tr><th>模型列表</th><th>Qwen1.5-110B</th><th>Qwen1.5-72B</th><th>Llama-3-70B</th><th>Mixtral-8x22B</th></tr><tr><td>MMLU</td><td>80.4</td><td>77.5</td><td>79.5</td><td>77.8</td></tr><tr><td>TheoremQA</td><td>34.9</td><td>29.3</td><td>32.0</td><td>35.9</td></tr><tr><td>GPQA</td><td>35.9</td><td>36.3</td><td>36.4</td><td>34.3</td></tr><tr><td>Hellaswag</td><td>87.5</td><td>86.0</td><td>88.0</td><td>88.7</td></tr><tr><td>BBH</td><td>74.8</td><td>65.5</td><td>76.6</td><td>69.2</td></tr><tr><td>ARC-C</td><td>69.6</td><td>65.9</td><td>68.8</td><td>70.7</td></tr><tr><td>GSM8K</td><td>85.4</td><td>79.5</td><td>79.2</td><td>78.6</td></tr><tr><td>MATH</td><td>49.6</td><td>34.1</td><td>41.0</td><td>41.7</td></tr><tr><td>HumanEval</td><td>52.4</td><td>41.5</td><td>45.7</td><td>45.1</td></tr><tr><td>MBPP</td><td>58.1</td><td>53.4</td><td>55.1</td><td>71.2</td></tr></tbody></table><p>从上面的对比结果看，<b>Qwen1.5-110B模型在综合理解（MMLU）、数学推理（GSM8K和MATH）方面得分比Llama-3-70B略高一点点，是几个模型中最强的</b>。而在复杂推理任务ARC-C上则略低于Mixtral-8×22B模型。在编程测试HumanEval得分则是远超另几个模型，而MBPP编程测试上则低于Mixtral-8×22B模型。从这个评测结果看，Qwen1.5-110B模型应该是与全球最强的开源模型可以一拼。</p><p>在DataLearnerAI收集的全球大模型排行榜中，Qwen1.5-110B模型的评测结果非常靠前：</p><figure><img class="origin_image zh-lightbox-thumb" src="https://pic1.zhimg.com/v2-8bac07d31ac76fdee4627594e2b9e82c_1440w.jpg" /></figure><p class="ztext-empty-paragraph"><br /></p><p>数据来源：<a class=" external" href="https://www.datalearner.com/ai-models/leaderboard/datalearner-llm-leaderboard" rel="nofollow noreferrer" target="_blank"><span class="invisible">https://www.</span><span class="visible">datalearner.com/ai-mode</span><span class="invisible">ls/leaderboard/datalearner-llm-leaderboard</span><span class="ellipsis"></span></a></p><p>这是按照MMLU排序的结果，也是除了Llama3-70B-Instruct模型外最强的开源模型。</p><h3><b>Qwen1.5-110B模型实测结果</b></h3><p>官方在HF上放了演示链接，我们用一个实例测试了Qwen1.5-110B和Qwen1.5-72B，模型逻辑方面Qwen1.5-110B模型明显更好，答案非常准确：</p><figure><img class="origin_image zh-lightbox-thumb" src="https://pic4.zhimg.com/v2-2843bffcfec34202c125e4dcfdbe807b_1440w.jpg" /></figure><p class="ztext-empty-paragraph"><br /></p><figure><img class="origin_image zh-lightbox-thumb" src="https://pic2.zhimg.com/v2-7af782ea1a4ad4b0b966da1a32033b69_1440w.jpg" /></figure><p class="ztext-empty-paragraph"><br /></p><p>模型的开源地址和演示地址可以参考DataLearnerAI的模型信息卡：<br />Qwen1.5-110B：<a class=" external" href="https://www.datalearner.com/ai-models/pretrained-models/Qwen1_5-110B" rel="nofollow noreferrer" target="_blank"><span class="invisible">https://www.</span><span class="visible">datalearner.com/ai-mode</span><span class="invisible">ls/pretrained-models/Qwen1_5-110B</span><span class="ellipsis"></span></a><br />Qwen1.5-110B-Chat：<a class=" external" href="https://www.datalearner.com/ai-models/pretrained-models/Qwen1_5-110B-Chat" rel="nofollow noreferrer" target="_blank"><span class="invisible">https://www.</span><span class="visible">datalearner.com/ai-mode</span><span class="invisible">ls/pretrained-models/Qwen1_5-110B-Chat</span><span class="ellipsis"></span></a><br />Qwen1.5-72B：<a class=" external" href="https://www.datalearner.com/ai-models/pretrained-models/Qwen1_5-72B-Chat" rel="nofollow noreferrer" target="_blank"><span class="invisible">https://www.</span><span class="visible">datalearner.com/ai-mode</span><span class="invisible">ls/pretrained-models/Qwen1_5-72B-Chat</span><span class="ellipsis"></span></a></p>
]]></content:encoded>
<pubDate>Sat, 27 Apr 2024 11:34:27 GMT</pubDate>
</item>
<item>
<title>数据学习赞同了回答: 体制内越级汇报的危害有哪些？</title>
<link>https://www.zhihu.com/question/359501955/answer/2017343834</link>
<guid>https://www.zhihu.com/question/359501955/answer/2017343834</guid>
<content:encoded><![CDATA[
<p><b>就单单讲疫情武汉封城的事情吧，从中看看从市、省、中央的决策是怎样的。</b></p><p>我记得原来看过一篇文章讲的是武汉封城的前后决策过程，大概是这样的：</p><p>武汉医院接收了不明肺炎患者以后，院方汇报给市疾控中心，疾控中心上报市卫健委，市卫健委汇报给市委市政府，这个时候应该都是属于不了解肺炎的具体情况，然后市卫健委和市委市政府一起向湖北省卫健委和省委省政府汇报了，但是也都是都不了解具体的情况和该怎么处理。这时候省卫健委就向国家疾控中心汇报，说发现了不明肺炎。</p><p>这时候大概是1月19号20号那两天？国家疾控中心就派了专家组（钟南山李兰娟高福那些人）来武汉实地调查，发现人传人的现象以后回北京（注意是要回北京，不是在湖北，因为发现人传人的现象是十分严重的公共卫生事件，而且武汉这么大的一个城市，中部枢纽，副省级市，封城需要中央批准，省一级根本做不了主）马上报给国家卫健委，国家卫健委开会汇总以后，专家组和国家卫健委主任（正部级）一起向分管医疗卫生的国务院副总理汇报（好像是孙）。这时候武汉的肺炎已经算是中央介入的大事件了，这还不算，接下来是专家组、卫健委主任、分管副总理一起参会，向总理汇报了这个情况。我记得总理还说了一句“谢谢你们的工作，这对我们后续决策有非常大的帮助。”（开始暗示中央的直接介入）接下来应该是总理向zz局常委会汇报了这个情况（中国最高决策层），后来的事情大家都知道了，中央决定武汉封城，zz局常委会开会，在全国部署抗疫工作。</p><p>从发现病毒到武汉封城全国抗疫，汇报都是一层一层逐级上报的，不可能存在越级上报的情况，越到高层这个现象越显著。你想想，如果是武汉市委直接报送zz局常委会，市政府直接报送国务院，不是先追查病毒的事情，中央会先问责为什么你们绕过省委来汇报？省委也会问责为什么对不明肺炎这个情况完全不了解，反过来被中央骂了一通才知道？越级上报最糟的后果就是造成上级对下级或者下下级的情况不了解，无法有效做出决策。体制内分管部门这么多，每个部门都是对具体的事务负责的，有成熟的经验和应对措施去处理，越级上报对上对下都是不科学、不负责的行为。当然有一些非常紧急的事务除外。</p><p>这里不是洗白什么的……面对新冠这种全球性的卫生事件，刚开始谁都不了解情况，也只能一层一层报送上去，你总不能说某个市的医院收治了几个情况不明的患者就需要惊动国务院吧。后来也是事情越来越严重病例越来越多，中央才介入的。</p>
]]></content:encoded>
<pubDate>Wed, 24 Apr 2024 07:31:22 GMT</pubDate>
</item>
<item>
<title>数据学习发表了文章: 使用Gradio配合transformers的text streamer实现Llama3-8B-Instruct的网页聊天机器人，流式输出</title>
<link>https://zhuanlan.zhihu.com/p/693990819</link>
<guid>https://zhuanlan.zhihu.com/p/693990819</guid>
<content:encoded><![CDATA[
<p>DataLearnerAI官网新增了一个大模型教程模块，提供了当前主流大模型的部署教程，目的是在一个py文件中完成大模型的部署启动。目前已经上架了MiniCPM、多模态Yi-VL6-B和Llama3-8B三个模型的部署教程，三个教程均在一个py文件中实现，并可以通过gradio的chatbot完成web交互。所有的代码也都开源在github中。这些代码大多数内部测试使用，现在共享给大家，欢迎交流。同时我们也在仙宫云上发布了镜像（仙宫云提供按分钟租赁4090显卡，所以适合前期的学习测试，性价比很高，DataLearnerAI部分模型功能也是在此测试完成的），有模型部署学习和测试的童鞋可以关注交流。后续会陆续上架新的模型。</p><p>DataLearnerAI大模型部署教程地址：<a class=" wrap external" href="https://www.datalearner.com/llm-tutorials/pretrained-model-tutorials" rel="nofollow noreferrer" target="_blank">AI大模型手把手部署教程！ | 数据学习(DataLearner)</a></p><p>仙宫云的4090租赁可以参考：<a class=" wrap external" href="https://www.datalearner.com/blog/1051701266330292" rel="nofollow noreferrer" target="_blank">推荐一个国内可以按分钟计费的4090显卡租用公有云，一个小时24GB显存的4090只需要2.37元——仙宫云</a></p><h3>基本信息</h3><p>原教程地址： </p><a class=" wrap external" href="https://www.datalearner.com/llm-tutorials/pretrained-model-tutorials/llama3-8b-instruct-deployment-with-4090-in-one-py-file" rel="nofollow noreferrer" target="_blank">使用Gradio配合transformers的text streamer实现Llama3-8B-Instruct的网页聊天机器人，流式输出</a><p>模型发布时间： 2024-04-18</p><p> 模型发布机构： Facebook AI研究实验室</p><p> 模型信息详情：<a class=" wrap external" href="https://www.datalearner.com/ai-models/pretrained-models/Llama3-8B-Instruct" rel="nofollow noreferrer" target="_blank"> https://www.datalearner.com/ai-models/pretrained-models/Llama3-8B-Instruct</a></p><p> 代码GitHub地址：<a class=" wrap external" href="https://github.com/DataLearnerAI/LLMPractice/tree/main/llm_code/llama3" rel="nofollow noreferrer" target="_blank"> https://github.com/DataLearnerAI/LLMPractice/tree/main/llm_code/llama3</a></p><p> 仙宫云一键镜像：<a class=" wrap external" href="https://www.xiangongyun.com/image/detail/613fcb84-d94d-4b42-8584-e18aa7c92964" rel="nofollow noreferrer" target="_blank"> https://www.xiangongyun.com/image/detail/613fcb84-d94d-4b42-8584-e18aa7c92964</a></p><p>我们与GPU租赁服务商仙宫云合作，为您提供一键部署的便捷服务。上述镜像链接，您可以直接点击部署即可运行。 仙宫云提供按分钟租赁4090显卡（24GB）显存，因此可以运行120亿参数规模及以下的大语言模型（fp16精度），关于仙宫云的介绍参考：<a class=" wrap external" href="https://www.datalearner.com/blog/1051701266330292" rel="nofollow noreferrer" target="_blank">推荐一个国内可以按分钟计费的4090显卡租用公有云，一个小时24GB显存的4090只需要2.37元——仙宫云</a></p><p>注：通过DataLearnerAI专用邀请链接注册会有额外的3元即共8元的赠送额度（自己注册仅有5元额度）：<a class=" external" href="https://www.xiangongyun.com/register/6WTXZM" rel="nofollow noreferrer" target="_blank"><span class="invisible">https://www.</span><span class="visible">xiangongyun.com/registe</span><span class="invisible">r/6WTXZM</span><span class="ellipsis"></span></a></p><p><b>Llama3-8B-Instruct 部署教程简介</b><br /><br />Llama3系列是MetaAI最新开源的大语言模型，相比第二代模型，它的性能提升很高。其中700亿参数版本在上线2天后就在ChatBot Arena大模型匿名竞技场获得了2700多个匿名投票，得分超过此前最强的开源模型Command R+，登顶开源模型第一。相比较其它模型，Llama3在更多的数据训练，其模型架构也有了新的变化。关于Llama3系列模型的信息可以参考DataLearnerAI的介绍：<a class=" external" href="https://www.datalearner.com/blog/1051713454866102" rel="nofollow noreferrer" target="_blank"><span class="invisible">https://www.</span><span class="visible">datalearner.com/blog/10</span><span class="invisible">51713454866102</span><span class="ellipsis"></span></a><br /></p><figure><img class="origin_image zh-lightbox-thumb" src="https://pic3.zhimg.com/v2-8f28f743d7da20da60d3c20eefb0ad86_1440w.jpg" /></figure><p><br />本教程的主要目的是通过Gradio提供的ChatBot组件做web版的大模型聊天页面原型。然后通过transformers库提供TextStreamer工具实现流式输出。所有的代码均只在一个py文件中实现，搭建好环境之后，直接运行脚本即可绑定服务到80端口，实现web版本的Llama3的聊天应用。<br />注意，这里的Llama3-8B-Instruct模型本身需要约15GB显存才可以推理，如果你本身已经有资源可以自己尝试部署。本教程的模型文件和代码已经打包成镜像，上架仙宫云的镜像市场，想一键体验或者低成本学习可以参考我们的仙宫云合作信息。<br /></p><ul><li><a class=" wrap external" href="https://www.datalearner.com/llm-tutorials/pretrained-model-tutorials/llama3-8b-instruct-deployment-with-4090-in-one-py-file#Llama3%E6%A8%A1%E5%9E%8B%E4%B8%8B%E8%BD%BD" rel="nofollow noreferrer" target="_blank">Llama3模型下载</a></li><li><a class=" wrap external" href="https://www.datalearner.com/llm-tutorials/pretrained-model-tutorials/llama3-8b-instruct-deployment-with-4090-in-one-py-file#%E6%AD%A5%E9%AA%A40%EF%BC%9A%E5%AE%89%E8%A3%85Llama3-8B-Instruct%E7%9A%84%E4%BE%9D%E8%B5%96" rel="nofollow noreferrer" target="_blank">步骤0：安装Llama3-8B-Instruct的依赖</a></li><li><a class=" wrap external" href="https://www.datalearner.com/llm-tutorials/pretrained-model-tutorials/llama3-8b-instruct-deployment-with-4090-in-one-py-file#%E6%AD%A5%E9%AA%A41%EF%BC%9A%E5%AE%9A%E4%B9%89%E5%85%A8%E5%B1%80%E5%8F%98%E9%87%8F" rel="nofollow noreferrer" target="_blank">步骤1：定义全局变量</a></li><li><a class=" wrap external" href="https://www.datalearner.com/llm-tutorials/pretrained-model-tutorials/llama3-8b-instruct-deployment-with-4090-in-one-py-file#%E6%AD%A5%E9%AA%A42%EF%BC%9A%E5%88%9D%E5%A7%8B%E5%8C%96Llama3-8B-Instruct%E6%A8%A1%E5%9E%8B" rel="nofollow noreferrer" target="_blank">步骤2：初始化Llama3-8B-Instruct模型</a></li><li><a class=" wrap external" href="https://www.datalearner.com/llm-tutorials/pretrained-model-tutorials/llama3-8b-instruct-deployment-with-4090-in-one-py-file#%E6%AD%A5%E9%AA%A43%EF%BC%9A%E5%9F%BA%E4%BA%8EGradio%E5%88%9B%E5%BB%BA%E8%81%8A%E5%A4%A9%E6%9C%BA%E5%99%A8%E4%BA%BA" rel="nofollow noreferrer" target="_blank">步骤3：基于Gradio创建聊天机器人</a></li><li><a class=" wrap external" href="https://www.datalearner.com/llm-tutorials/pretrained-model-tutorials/llama3-8b-instruct-deployment-with-4090-in-one-py-file#%E6%AD%A5%E9%AA%A44%EF%BC%9A%E5%A2%9E%E5%8A%A0%E7%94%A8transformers%E5%BA%93%E6%8E%A8%E7%90%86%E5%9B%9E%E5%A4%8D%E7%94%A8%E6%88%B7%E6%B6%88%E6%81%AF%E7%9A%84%E5%86%85%E5%AE%B9" rel="nofollow noreferrer" target="_blank">步骤4：增加用transformers库推理回复用户消息的内容</a></li><li><a class=" wrap external" href="https://www.datalearner.com/llm-tutorials/pretrained-model-tutorials/llama3-8b-instruct-deployment-with-4090-in-one-py-file#%E6%AD%A5%E9%AA%A45%EF%BC%9A%E7%BB%91%E5%AE%9AGradio%E5%BA%94%E7%94%A8%E5%88%B080%E7%AB%AF%E5%8F%A3%EF%BC%8C%E8%BF%90%E8%A1%8Cweb%E8%AE%BF%E9%97%AE" rel="nofollow noreferrer" target="_blank">步骤5：绑定Gradio应用到80端口，运行web访问</a></li><li><a class=" wrap external" href="https://www.datalearner.com/llm-tutorials/pretrained-model-tutorials/llama3-8b-instruct-deployment-with-4090-in-one-py-file#%E5%9C%A8%E4%BB%99%E5%AE%AB%E4%BA%91%E9%95%9C%E5%83%8F%E4%B8%AD%E8%BF%90%E8%A1%8C" rel="nofollow noreferrer" target="_blank">在仙宫云镜像中运行</a></li></ul><p><br /><b>Llama3模型下载</b><br />首先需要注意的是，我们需要自己去申请拿到Llama3的预训练结果。目前看，尽量不要用个人邮箱申请，速度很快。DataLearnerAI的童鞋用企业邮箱注册申请，官网申请秒批，在HuggingFace申请也只需要几分钟即可。<br />另外，需要注意的是，GitHub下载脚本下载需要审批通过的链接。HuggingFace上审批通过后需要配合access token下载。这部分可以用git命令也可以用HuggingFace的transformers库在python中下载认证。我们推荐自己手动下载。<br /><b>步骤0：安装Llama3-8B-Instruct的依赖</b><br />Llama3模型本身比较新，采用了一些比较新的架构。我们这里考虑使用transformers库做推理，同时使用accelerate加速，并最终使用gradio创建聊天界面，因此，在安装了torch的cuda版本前提下还需要安装上述三个库（升级到新版本）：<br /></p><div class="highlight"><pre><code class="language-text">pip install -U transformers
pip install -U accelerate
pip install -U gradio</code></pre></div><p><br /><b>步骤1：定义全局变量</b><br />为了初始化模型，以及设置一些Gradio聊天的界面头像，我们先定义几个全局变量，便于后续使用。<br /></p><div class="highlight"><pre><code class="language-text">bot_avatar = "/home/datalearner/dl_logo_rect.png" # 聊天机器人头像位置
user_avatar = "/home/datalearner/user_avatar.png" # 用户头像位置
model_path = "/home/datalearner/Meta-Llama-3-8B-Instruct" # 已下载的模型位置

# 存储全局的历史对话记录，Llama3支持系统prompt，所以这里默认设置！
llama3_chat_history = [
 {"role": "system", "content": "You are a helpful assistant trained by MetaAI! But you are running with DataLearnerAI Code."}
]

# 初始化所有变量，用于载入模型
tokenizer = None
streamer = None
model = None
terminators = None</code></pre></div><p><br />这里需要注意2点：一个是llama3_chat_history，这个变量用来保存历史对话信息，尽管Gradio的Chatbot组件有内置的变量chat_history管理对话记录。但是最新的模型的历史消息一般都包含了系统指令、不同角色区分等。所以我们用一个新的变量保存这些历史记录。也因为引入了这个变量，在清理历史消息的时候也需要增加清理机制。另一个变量是terminators，是模型停止输出的标记。在Llama3中，结束标记是它们自己设置的，这个变量如果没有或者设置错误，即使是Insturct指令优化模型也会出现不断生成内容的现象，也就是说模型不知道什么时候该停止生成。<br /><b>步骤2：初始化Llama3-8B-Instruct模型</b><br />我们单独定义一个函数，用来初始化并加载Llama3-8B-Instruct模型，只需要启动时候初始化，后续对话直接使用即可：<br /></p><div class="highlight"><pre><code class="language-text">def init_model():
 """初始化模型，载入本地模型
    """
 global tokenizer, model, streamer, terminators
    tokenizer = AutoTokenizer.from_pretrained(
        model_path, local_files_only=True)

    model = AutoModelForCausalLM.from_pretrained(
        model_path,
        torch_dtype=torch.float16,
        device_map=device,
        trust_remote_code=True
 )

    terminators = [
        tokenizer.eos_token_id,
        tokenizer.convert_tokens_to_ids("&lt;|eot_id|&gt;")
 ]

    streamer = TextIteratorStreamer(
        tokenizer,
        skip_prompt=True,
        skip_special_tokens=True
 )

初始化方法很简单，与常规的transformers写法一致，只是注意Llama3独有的停止标记即可。
步骤3：基于Gradio创建聊天机器人
接下来我们就可以使用Gradio来创建聊天机器人了。大致的代码框架如下：


with gr.Blocks() as demo:
 # step1: 载入模型
    init_model()

 # step2: 初始化gradio的chatbot应用，并添加按钮等信息
    chatbot = gr.Chatbot(
        height=900,
        avatar_images=(user_avatar, bot_avatar)
 )
    msg = gr.Textbox()
    clear = gr.ClearButton([msg, chatbot])

 # 清楚历史记录
 def clear_history():
 global llama3_chat_history
        llama3_chat_history = []

 # 用于回复的方法
 def respond(message, chat_history):
 """用Llama3回复用户消息的处理，后面展开"""

 # 点击清楚按钮，触发历史记录清楚
    clear.click(clear_history)
    msg.submit(respond, [msg, chatbot], [msg, chatbot])</code></pre></div><p><br />这是非常常规的Gradio聊天机器人应用写法。除了清楚历史记录外，其它都与官方代码一致。这里加的的清楚历史记录是在最后<code>clear.click</code>时候触发的，clear是清楚历史对话的按钮，我们前面说了，我们引入了自定义变量，那就要这里记住清楚。<br />接下来我们来加入推理代码即可，即代码中<code>用于回复的方法</code>注释部分的具体内容。<br /><b>步骤4：增加用transformers库推理回复用户消息的内容</b><br />这部分就是拿到用户的输入之后，怎么用流式方法推理。在步骤2中，我们已经初始化了Llama3-8B-Instruct模型相关的变量。那么流式推理的代码主要如下：<br /></p><div class="highlight"><pre><code class="language-text">def respond(message, chat_history):

 # 引入全局变量
 global llama3_chat_history, tokenizer, model, streamer

 # 拼接对话历史
        llama3_chat_history.append({"role": "user", "content": message})

 # 使用Llama3自带的聊天模板，格式化对话记录
        history_str = tokenizer.apply_chat_template(
            llama3_chat_history,
            tokenize=False,
            add_generation_prompt=True
 )

 # 对历史记录进行tokenization
        inputs = tokenizer(history_str, return_tensors='pt').to(device)

 # 这个历史记录是Gradio的Chatbot自带的变量，用来控制页面显示逻辑的，我们必须也要对齐操作，保证页面展示正常
        chat_history.append([message, ""])

 # 拼接推理参数
        generation_kwargs = dict(
 **inputs,
            streamer=streamer,
            max_new_tokens=4096,
            num_beams=1,
            do_sample=True,
            top_p=0.8,
            temperature=0.3,
            eos_token_id=terminators
 )

 # 启动线程，用以监控流失输出结果
        thread = Thread(target=model.generate, kwargs=generation_kwargs)
        thread.start()

 # 循环推理streamer，每次得到新增的推理部分都附到历史记录末尾，Gradio会监控这个变量在页面展示
 for new_text in streamer:
            chat_history[-1][1] += new_text
 yield "", chat_history

 # 所有的输出完毕之后，我们自己的历史记录也要更新，把模型输出的完整结果加进来。
        llama3_chat_history.append(
 {"role": "assistant", "content": chat_history[-1][1]}
 )</code></pre></div><p><br />可以看到，这部分内容是最多的。但也不是很复杂。把前面所有内容拼接好就是完整的代码了。这个代码放到一个py脚本中，后面直接执行这个脚本即可。唯一需要注意的是安装的transformers等依赖的版本，以及要提前把模型文件下载好。<br /><b>步骤5：绑定Gradio应用到80端口，运行web访问</b><br />这一步很简单，常规操作，但是没有就无法启动了：<br /></p><div class="highlight"><pre><code class="language-text">if __name__ == "__main__":
    demo.launch(server_name="0.0.0.0", server_port=80)</code></pre></div><p><br /><b>在仙宫云镜像中运行</b><br />最后，我们简单总结一下如何运行。首先，这里的运行并不是一定要在仙宫云进行。只要你在超过15G显卡的机器上就可以跑了。<br />我们的代码文件和模型文件都放在了<code>/home/datalearner</code>目录下的<code>run_llama3_gradio.py</code>，所以，我们执行如下命令即可：<br /></p><div class="highlight"><pre><code class="language-text">cd /home/datalearner
python run_llama3_gradio.py</code></pre></div><p><br />接下来可以看到如下界面提示：<br /></p><figure><img class="origin_image zh-lightbox-thumb" src="https://pic4.zhimg.com/v2-b8f92ae826f0e04c0c3f1d5b19894d4b_1440w.jpg" /></figure><p>这个提示说明成功拉起模型，并启动了gradio的界面。通过运行<code>nvidia-smi</code>命令也可以看到4090的显存已经消耗了15GB左右。说明半精度的Llama3-8B-Instruct已经拉起来了。<br /></p><figure><img class="origin_image zh-lightbox-thumb" src="https://pic4.zhimg.com/v2-c963fb77f83cb853de203d5ef1cbaebf_1440w.jpg" /></figure><p><br />启动完成之后，本地可以通过http:/127.0.0.1 访问，如果是仙宫云，则直接可以通过外网访问。可以通过仙宫云控制台访问web地址：<br /></p><figure><img class="origin_image zh-lightbox-thumb" src="https://pic4.zhimg.com/v2-ebe1d3ead8468f4b8e4e0e1771957353_1440w.jpg" /></figure><p><br />Llama3-8B-Instruct简单测试：<br /></p><figure><img class="origin_image zh-lightbox-thumb" src="https://pic1.zhimg.com/v2-b4b01ad54c7242bce6304fe47ca2c5c8_1440w.jpg" /></figure><p class="ztext-empty-paragraph"><br /></p><p>访问教程原地址：<a class=" wrap external" href="https://www.datalearner.com/llm-tutorials/pretrained-model-tutorials/llama3-8b-instruct-deployment-with-4090-in-one-py-file" rel="nofollow noreferrer" target="_blank">使用Gradio配合transformers的text streamer实现Llama3-8B-Instruct的网页聊天机器人，流式输出 | 数据学习(DataLearner)</a></p>
]]></content:encoded>
<pubDate>Tue, 23 Apr 2024 00:58:14 GMT</pubDate>
</item>
<item>
<title>数据学习发表了文章: Llama3相比较前两代的模型（Llama1和Llama2）有哪些升级？几张图简单总结Llama3的训练成本、训练时间、模型架构升级等情况</title>
<link>https://zhuanlan.zhihu.com/p/693742469</link>
<guid>https://zhuanlan.zhihu.com/p/693742469</guid>
<content:encoded><![CDATA[
<p>本文原文来自DataLearnerAI官方网站：</p><a class=" wrap external" href="https://www.datalearner.com/blog/1051713702716647" rel="nofollow noreferrer" target="_blank">Llama3相比较前两代的模型（Llama1和Llama2）有哪些升级？几张图简单总结Llama3的训练成本、训练时间、模型架构升级等情况</a><hr /><p>Llama3是MetaAI开源的最新一代大语言模型。一发布就引起了全球AI大模型领域的广泛关注。这是MetaAI开源的第三代大语言模型，也是当前最强的开源模型。但相比较第一代和第二代的Llama模型，Llama3的升级之处有哪些？本文以图表的方式总结Llama3的升级之处。</p><figure><img class="origin_image zh-lightbox-thumb" src="https://pic3.zhimg.com/v2-9fd84180a853e0853b989432ee0248d6_1440w.jpg" /></figure><p class="ztext-empty-paragraph"><br /></p><p class="ztext-empty-paragraph"><br /></p><ul><li><a class=" wrap external" href="https://www.datalearner.com/blog/1051713702716647#Llama3%E7%B3%BB%E5%88%97%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%80%BB%E7%BB%93" rel="nofollow noreferrer" target="_blank">Llama3系列模型的总结</a></li><li><a class=" wrap external" href="https://www.datalearner.com/blog/1051713702716647#Llama3%E6%A8%A1%E5%9E%8B%E6%9E%B6%E6%9E%84%E7%9A%84%E5%8D%87%E7%BA%A7" rel="nofollow noreferrer" target="_blank">Llama3模型架构的升级</a></li><ul><li><a class=" wrap external" href="https://www.datalearner.com/blog/1051713702716647#Llama%E7%B3%BB%E5%88%97%E7%9A%84%E4%B8%8A%E4%B8%8B%E6%96%87%E9%95%BF%E5%BA%A6%E4%B8%80%E7%9B%B4%E5%9C%A8%E5%A2%9E%E9%95%BF" rel="nofollow noreferrer" target="_blank">Llama系列的上下文长度一直在增长</a></li><li><a class=" wrap external" href="https://www.datalearner.com/blog/1051713702716647#Llama3%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%AF%8D%E6%B1%87%E8%A1%A8%E5%A4%A7%E5%B9%85%E6%8F%90%E9%AB%98" rel="nofollow noreferrer" target="_blank">Llama3模型的词汇表大幅提高</a></li></ul><li><a class=" wrap external" href="https://www.datalearner.com/blog/1051713702716647#Llama3%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B%E7%9A%84%E5%8D%87%E7%BA%A7" rel="nofollow noreferrer" target="_blank">Llama3模型的训练过程的升级</a></li><ul><li><a class=" wrap external" href="https://www.datalearner.com/blog/1051713702716647#Llama3%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%AE%AD%E7%BB%83%E6%95%B0%E6%8D%AE%E5%A4%A7%E5%B9%85%E5%A2%9E%E9%95%BF" rel="nofollow noreferrer" target="_blank">Llama3模型的训练数据大幅增长</a></li><li><a class=" wrap external" href="https://www.datalearner.com/blog/1051713702716647#Llama3%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%AE%AD%E7%BB%83%E6%97%B6%E9%95%BF%E5%A4%A7%E5%B9%85%E5%A2%9E%E5%8A%A0" rel="nofollow noreferrer" target="_blank">Llama3模型的训练时长大幅增加</a></li></ul><li><a class=" wrap external" href="https://www.datalearner.com/blog/1051713702716647#Llama3%E7%9A%84%E8%AF%84%E6%B5%8B%E5%9F%BA%E5%87%86%E5%A4%A7%E5%B9%85%E6%8F%90%E9%AB%98" rel="nofollow noreferrer" target="_blank">Llama3的评测基准大幅提高</a></li><li><a class=" wrap external" href="https://www.datalearner.com/blog/1051713702716647#Llama3%E6%80%BB%E7%BB%93" rel="nofollow noreferrer" target="_blank">Llama3总结</a></li></ul><h3><b>Llama3系列模型的总结</b></h3><p>第一代和第二代的Llama模型都包含了四个不同参数规模的版本，其中最小的模型参数规模在70亿，往上分别有130亿、340亿和700亿（第一代最高的是650亿）。而此次发布的第三代Llama3模型，目前公开的只有80亿参数规模版本和700亿版本。而根据透露，最高的参数版本是4000亿参数规模的模型。只是目前还在训练中。</p><p>下面我们就用图表的形式说明本次Llama3的升级细节。</p><h3><b>Llama3模型架构的升级</b></h3><p>首先是模型架构相关的升级。目前，官方没有公开Llama3的技术报告或者论文细节，在官方博客中只给出了一些简单的指标。</p><p>关于Llama3的模型架构，应该是没有本质变化，官方的说法是：</p><blockquote><i>根据我们的设计理念，我们在 Llama 3 中选择了一个相对标准的纯解码器（decoder-only）变压器架构。</i></blockquote><p>因此，模型架构基本没变，但是增加了Group Query Attention（分组查询注意力，GQA），这项技术最大的特点是可以<b>加速推理</b>，这也是我们实测中感受到的，Llama3-8B-Instruct在4090上的速度飞快！</p><p>剩余的模型相关的架构，这里我们对比的是上下文长度和词汇表。</p><h3><b>Llama系列的上下文长度一直在增长</b></h3><p>上下文长度每一代都翻倍了，在Llama3中，训练的时候用的就是8K上下文：</p><figure><img class="origin_image zh-lightbox-thumb" src="https://pic3.zhimg.com/v2-331f89b187e26b6abf4671589ef4561e_1440w.jpg" /></figure><p class="ztext-empty-paragraph"><br /></p><p>从上图可以看到，Llama三代模型的上下文长度分别是2K、4K和8K，虽然<b>Llama3训练是8K上下文</b>，但是按照目前业界的技术，应该是可以继续拓展到更长上下文的。而官方也说过，未来Llama3会有更长上下文的版本。</p><h3><b>Llama3模型的词汇表大幅提高</b></h3><p>在模型架构中另一个值得注意的是词汇表的大幅提高。在Llama1和Llama2中，MetaAI的词汇表都是32K大小，这可能与前两代模型的训练数据差距不大有关。而第三代的<b>Llama3模型的词汇表大小变为128K</b>，也就是说它的tokenizer有了较大的变化。</p><figure><img class="origin_image zh-lightbox-thumb" src="https://pic1.zhimg.com/v2-27bcfdfcd60136eb9c39c94e33174df4_1440w.jpg" /></figure><p class="ztext-empty-paragraph"><br /></p><p>更大的词汇表意味着更强的语义表达能力，也是支持更多语言的一个基础。</p><h3><b>Llama3模型的训练过程的升级</b></h3><p>训练过程的升级我们主要看训练时间和训练数据的变化。此前，业界一直说大模型的训练成本在下降。但是从Llama3的变化看，下降的是单位训练成本，但是大模型总的训练成本其实在大幅增长。</p><h3><b>Llama3模型的训练数据大幅增长</b></h3><p>Llama3的训练数据达到了15万亿，比第一代和第二代Llama模型加在一起还多好几倍。如下图所示，第一代的小一点的模型训练数据是1万亿tokens，而较大的650亿规模的模型训练数据是1.4万亿tokens。到了第二代Llama2系列，训练数据都增长到了2万亿tokens。</p><figure><img class="origin_image zh-lightbox-thumb" src="https://pic1.zhimg.com/v2-6aa9f7baad624cc343514e4238e94a84_1440w.jpg" /></figure><p class="ztext-empty-paragraph"><br /></p><p>可以看到，第三代Llama3训练数据大幅增加，几乎也是目前市场上训练数据最多的一个模型了。这里有一点也值得提一下，DeepMind发表过一个非常著名的论文，就是关于Chinchilla模型的论文，里面提到了训练数据对大模型性能的影响。根据论文发现的规律，<b>80亿参数规模的模型，用2000亿tokens数据集训练可以获得最佳性能，但是MetaAI发现，80亿参数规模的模型训练数据增长到15万亿tokens依然可以获得log线性增长！</b>因此，数据可以说依然是王道。</p><h3><b>Llama3模型的训练时长大幅增加</b></h3><p>Meta公司一直是全球拥有显卡最多的公司。在Llama1论文发布的时候，大家就发现，Meta训练Llama1模型可能花费了几百万上千万美金。原因是650亿参数的Llama1模型训练了102万个GPU小时，按照公有云A100租赁的价格打折计算，这个成本也是几百万美金。</p><p>到了Llama3模型这里，训练成本的增长更为恐怖，<b>Llama3-8B模型的训练时长比650亿参数规模的Llama1模型还长</b>。结果如下：</p><figure><img class="origin_image zh-lightbox-thumb" src="https://pic1.zhimg.com/v2-86bf0007b31daa3b780c0e1aec3c76ec_1440w.jpg" /></figure><p class="ztext-empty-paragraph"><br /></p><p>上图对比的是Llama系列70亿参数规模模型和700亿参数规模模型的训练时长，单位是百万个GPU小时。忽略Llama3可能是H100的显卡，这个训练时长增长太恐怖了。而这些也是Llama3训练成本非常高的原因之一。<b>以700亿参数规模的Llama3-70B为例，训练时长是640万个GPU小时。以AWS的p4d.24xlarge实例计算，包含8个A100，按需付费8卡是32.77美元一个小时，640万个GPU小时是80万台这样的机器，按需付费的价格就是80万×32.77美元=2621.6万美元</b>，假设MetaAI自己用自己的硬件，成本是一半的话，训练700亿参数规模的Llama3-70B就是1300多万美元，十分之一的话那就是262万美元！成本十分昂贵！</p><h3><b>Llama3的评测基准大幅提高</b></h3><p>最后，我们用图表展示一下三代不同Llama系列在综合理解评测基准MMLU、数学推理GSM8K以及代码能力HumanEval的评测结果。不用说，Llama3相比Llama2的提升应该是比Llama2相比Llama1的提升要高的多的。</p><figure><img class="origin_image zh-lightbox-thumb" src="https://pic1.zhimg.com/v2-3a8b1cb4f9699cac10899377721d8fcc_1440w.jpg" /></figure><p class="ztext-empty-paragraph"><br /></p><figure><img class="origin_image zh-lightbox-thumb" src="https://pic1.zhimg.com/v2-12697a2bb31a6bb7faa3519494d6a710_1440w.jpg" /></figure><p class="ztext-empty-paragraph"><br /></p><figure><img class="origin_image zh-lightbox-thumb" src="https://pic2.zhimg.com/v2-2130abb22291491feef26465fea31489_1440w.jpg" /></figure><p class="ztext-empty-paragraph"><br /></p><p>由于Llama1没有公布GSM8K和HumanEval的评测结果，所以没有展示了。</p><h3><b>Llama3总结</b></h3><p>Llama3-70B模型目前已经是Chatbot Arena大模型匿名评分最高的开源模型了。在英文的分项测试甚至超过了Claude-Opus模型，十分强悍。</p><figure><img class="origin_image zh-lightbox-thumb" src="https://pic1.zhimg.com/v2-f39733ba37ffb7c09f06dc23927058d0_1440w.jpg" /></figure><p class="ztext-empty-paragraph"><br /></p><p>数据来源：<a class=" external" href="https://www.datalearner.com/ai-models/leaderboard/lm-sys-chat-bot-arena-leaderboard" rel="nofollow noreferrer" target="_blank"><span class="invisible">https://www.</span><span class="visible">datalearner.com/ai-mode</span><span class="invisible">ls/leaderboard/lm-sys-chat-bot-arena-leaderboard</span><span class="ellipsis"></span></a></p><p>未来，如果4000亿参数规模的Llama3-400B也开源的话，那无疑是给闭源模型企业一颗巨大的炸弹。</p><p>Llama3的详细介绍参考：<a class=" external" href="https://www.datalearner.com/blog/1051713454866102" rel="nofollow noreferrer" target="_blank"><span class="invisible">https://www.</span><span class="visible">datalearner.com/blog/10</span><span class="invisible">51713454866102</span><span class="ellipsis"></span></a><br />Llama3在大模型匿名竞技场的得分排行榜：<a class=" external" href="https://www.datalearner.com/ai-models/leaderboard/lm-sys-chat-bot-arena-leaderboard" rel="nofollow noreferrer" target="_blank"><span class="invisible">https://www.</span><span class="visible">datalearner.com/ai-mode</span><span class="invisible">ls/leaderboard/lm-sys-chat-bot-arena-leaderboard</span><span class="ellipsis"></span></a><br />Llama3手动部署教程：<a class=" external" href="https://www.datalearner.com/llm-tutorials/pretrained-model-tutorials/llama3-8b-instruct-deployment-with-4090-in-one-py-file" rel="nofollow noreferrer" target="_blank"><span class="invisible">https://www.</span><span class="visible">datalearner.com/llm-tut</span><span class="invisible">orials/pretrained-model-tutorials/llama3-8b-instruct-deployment-with-4090-in-one-py-file</span><span class="ellipsis"></span></a></p>
]]></content:encoded>
<pubDate>Sun, 21 Apr 2024 12:54:33 GMT</pubDate>
</item>
<item>
<title>数据学习回答了问题: 如何看待 Meta 发布 Llama3，并将推出 400B+ 版本？</title>
<link>https://www.zhihu.com/question/653373334/answer/3470986557</link>
<guid>https://www.zhihu.com/question/653373334/answer/3470986557</guid>
<content:encoded><![CDATA[
<p>关于Llama3所有的内容和分析参考DataLearnerAI的官方网站内容：</p><a class=" wrap external" href="https://www.datalearner.com/blog/1051713454866102" rel="nofollow noreferrer" target="_blank">开源王者！全球最强的开源大模型Llama3发布！15万亿数据集训练，最高4000亿参数，数学评测超过GPT-4，全球第二！</a><hr /><p>Llama3系列模型是MetaAI开源的第三代大语言模型，本次发布的包含2个不同参数规模的版本，一个是80亿参数的LLama3-8B，另一个是700亿参数规模的Llama3-70B。MetaAI为此创建了2个2.4万个GPU集群，让Llama3模型在其中的1.6万个GPU上同时训练！</p><p>不过，根据官方描述，目前这两个版本是早期预览版本，未来几个月，Llama3的能力将拓展到<b>多语言</b>支持和<b>更长的上下文</b>，并且会开源<b>更多不同参数规模</b>版本的模型。并且，Llama3将会有<b>多模态</b>版本的模型！</p><p>Llama3模型是在<b>15万亿tokens数据集</b>上训练，是Llama2的7倍！其中的代码数据集高4倍！当前支持的上下文长度是128K！</p><p>这里还有个好消息是，预训练数据集中有<b>5%的非英文数据集</b>，总共支持的<b>语言高达30种</b>，期待包含中文。不过官方也说了，其他语言可能不如英文。但是如果有这个基础，继续做对齐可能更为容易。</p><p>从这些信息看，Llama3整体获得的提升是全方位的，包括多模态、超长上下文、多语种等。而评测结果也很不错。虽然目前MetaAI仅仅开源了2个不同参数规模版本的模型，其中700亿参数规模的模型评测结果极其优秀。最大的亮点是数学评测GSM8K的结果上得分93分！根据DataLearnerAI目前收集的数据，<b>这个分数仅次于Claude3-Opus的95分，超过GPT-4，全球第二，是目前开源大模型中得分最高的一个</b>。</p><figure><img class="origin_image zh-lightbox-thumb lazy" src="https://pic4.zhimg.com/v2-f634c6e902ef7ccbc6adee489db57307_1440w.jpg" /></figure><p>数据来源：<a class=" external" href="https://www.datalearner.com/ai-models/llm-evaluation" rel="nofollow noreferrer" target="_blank"><span class="invisible">https://www.</span><span class="visible">datalearner.com/ai-mode</span><span class="invisible">ls/llm-evaluation</span><span class="ellipsis"></span></a></p><p>可以看到，按照GSM8K排序的靠前的模型中，<b>除了Llama3-70B外，全部是闭源的私有模型</b>。而数学能力与推理等强相关，在解决复杂任务方面有着非常好的优势。而<b>Llama3-70B的MMLU得分82.0分，排名在Claude3-Opus、GPT-4和Gemini Ultra之后，全球第四</b>，成绩也是非常好，也是开源模型中最强的一个。</p><p>而Llama3-8B版本的模型在MMLU（综合理解能力）得分68.4，接近GPT-3.5，GSM8K得分79.6，略超720亿参数规模的Qwen1.5-72B模型！</p><p>另外，作为一个综合的大模型，Llama3-70B模型的代码能力也非常出色，在HumanEval评测上得分81.7，低于GPT-4和Claude3-Opus，也比专门的编程大模型CodeQwen1.5-7B模型略低，但是全球排名第四！</p><p>注意，<b>上述模型的评测的是Llama3的指令优化版本</b>，基座预训练得分目前只公布了MMLU部分，稍低于这些分数。</p><p>MetaAI官方宣布，Llama3有很多版本，其中<b>最大的版本是4000亿参数规模</b>！但是这个版本的模型<b>还在训练中</b>！官方<b>没有明确说未来这个版本是否开源</b>，但是也没有否认。</p><p>目前开源的Llama3-8B和LLama3-70B模型没有在任何平台发布，<b>只在官网提供了申请链接</b>，需要审批通过之后才可以下载</p>
]]></content:encoded>
<pubDate>Thu, 18 Apr 2024 23:47:41 GMT</pubDate>
</item>
</channel>
</rss>