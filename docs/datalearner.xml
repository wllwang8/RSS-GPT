<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
<channel>
<title>数据学习的知乎动态</title>
<link>https://www.zhihu.com/people/datalearner/activities</link>

<item>
<title>数据学习赞同了回答: 为什么互联网公司很少用oppovivo的手机呢？</title>
<link>https://www.zhihu.com/question/475086611/answer/2033570791</link>
<guid>https://www.zhihu.com/question/475086611/answer/2033570791</guid>
<content:encoded><![CDATA[
<div> 用户画像、腾讯、oppo、vivo、互联网公司
<br><br>总结:腾讯的大数据显示，oppo和vivo用户主要分布在三四线城市和农村，学历较低，收入不高，女性用户占比较高。这四个特点与互联网公司从业者的情况存在冲突，因此互联网公司很少见oppo和vivo用户。根据酷安社区的例子，即使是绿厂员工也不一定会选择用自家手机，更不用说其他互联网企业了。 <div>
<p>这个原因很简单，腾讯曾经出过一期手机用户画像大数据。</p><figure><img class="origin_image zh-lightbox-thumb lazy" src="https://pic4.zhimg.com/v2-5ea4905c376de5775a8e35db906a3fd7_1440w.jpg" /></figure><p>其中，oppo用户的画像是这样的。</p><figure><img class="origin_image zh-lightbox-thumb lazy" src="https://pic1.zhimg.com/v2-7d5a6e61051e8652c8004648fb550708_1440w.jpg" /></figure><p>vivo用户画像是这样的。</p><figure><img class="origin_image zh-lightbox-thumb lazy" src="https://pic1.zhimg.com/v2-2c31c141769d9c51ea40d282e36302d8_1440w.jpg" /></figure><p class="ztext-empty-paragraph"><br /></p><p class="ztext-empty-paragraph"><br /></p><p>从上面可以看出，oppo vivo用户有以下四个特征:</p><p>1.主要分布在三四线城市和农村。</p><p>2，学历较低，大学本科以上学历只有5%左右，而友商苹果华为小米均在10%以上。</p><p>3，收入较低，月收入8000以上的仅有5%。</p><p>4.女性用户占比远大于男性用户占比。</p><p>很明显，互联网公司从业者和oppovivo用户的这四个特点完全冲突。三四线农村比较少互联网企业，本科以下学历从事互联网基本属于扯淡，互联网从业者也不至于月收入8000以下，互联网工作者较少女性。</p><p>所以说，互联网公司很少见oppovivo，是因为oppovivo用户普遍存在于三四线农村学历较低，能够胜任互联网工作的比较稀少。</p><p class="ztext-empty-paragraph"><br /></p><p class="ztext-empty-paragraph"><br /></p><p class="ztext-empty-paragraph"><br /></p><p class="ztext-empty-paragraph"><br /></p><p>ps:酷安社区也有几个绿厂员工，有一个用的三星note10+,有两个用的iphone12系列，还有一个用的小米11ultra，只有一个一加工程师坚持用自家手机一加8pro。连自家员工都做不到用自家手机，何况是别的互联网企业呢？</p>
]]></content:encoded>
<pubDate>Mon, 13 May 2024 01:19:21 GMT</pubDate>
<pubDate>Mon, 13 May 2024 01:19:21 GMT</pubDate>
</item>
<item>
<title>数据学习赞同了回答: 如何看待美国商务部将一批中国量子研究机构如中科大、物理所等加入「实体清单」？</title>
<link>https://www.zhihu.com/question/655547056/answer/3494661796</link>
<guid>https://www.zhihu.com/question/655547056/answer/3494661796</guid>
<content:encoded><![CDATA[
<div> onedrive, 学校, 数据备份, 科研, 云存储服务
<br><br>总结:
学校中的数据备份工作已经全面展开，各个学校在疯狂备份数据，其中提到了学校并不推荐使用onedrive，因为学校有自己的云存储服务并有每人几TB的空间。虽然onedrive是免费提供的，但科研数据一般不会存储在onedrive中。学校可能仍在维护onedrive的原因是因为它能提供大量的免费容量，对于一些学生来说，免费的云存储服务很有吸引力。 <div>
<p>制裁的第一波影响已来</p><p>全校都在疯狂备份数据</p><p>Σ(ŎдŎ|||)ﾉﾉ</p><figure><img class="origin_image zh-lightbox-thumb lazy" src="https://pic4.zhimg.com/v2-954f5c115cadf2d7bbd922845aa1ec9f_1440w.jpg" /></figure><p class="ztext-empty-paragraph"><br /></p><hr /><p>我来补充一下，onedrive不是学校官方提倡大家使用的，学校应该也没有为onedrive付过费。onedrive的教育版是<b>微软免费提供的</b>，各个学校只要愿意维护，应该都有。除此之外，<b>科大有自己的云存储服务</b>，每人也有几TB空间。</p><p>那为啥科大网络中心还在维护onedrive嘞，单纯是因为免费量大啊。各位有所不知，onedrive教育版之前每人有5TB容量，且完全免费，并且多端同步很爽啊，给穷学生们存存照片、存存视频啥的挺好的，毕竟国产的云存储服务很多学生用不起啊，免费的羊毛当然是能薅则薅了。(˃ ⌑ ˂ഃ )</p><p>不用担心科研数据啦，不会存onedrive的，科大再穷，各个课题组存储服务器还是买得起的。</p>
]]></content:encoded>
<pubDate>Mon, 13 May 2024 01:13:56 GMT</pubDate>
<pubDate>Mon, 13 May 2024 01:13:56 GMT</pubDate>
</item>
<item>
<title>数据学习发表了文章: 阿里开源截止目前为止参数规模最大的Qwen1.5-110B模型：MMLU评测接近Llama-3-70B，略超Mixtral-8×22B！</title>
<link>https://zhuanlan.zhihu.com/p/694895756</link>
<guid>https://zhuanlan.zhihu.com/p/694895756</guid>
<content:encoded><![CDATA[

<h2><b>本文原文来自DataLearnerAI官方网站：</b></h2><a class=" wrap external" href="https://www.datalearner.com/blog/1051714140775766" rel="nofollow noreferrer" target="_blank">阿里开源截止目前为止参数规模最大的Qwen1.5-110B模型：MMLU评测接近Llama-3-70B，略超Mixtral-8×22B！</a><p>Qwen1.5系列是阿里开源的一系列大语言模型，也是目前为止最强开源模型之一。Qwen1.5是Qwen2的beta版本，此前开源的模型最大参数规模都是720亿，和第一代模型一样。就在刚刚，阿里开源了1100亿参数规模的Qwen1.5-110B模型。评测结果显示MMLU略超Llama3-70B和Mixtral-8×22B。DataLearnerAI实测结果，<b>相比Qwen1.5-72B模型来说，Qwen1.5-110B模型复杂任务的逻辑提升比较明显！</b></p><h3><b>Qwen1.5-110B模型简介</b></h3><p>在开源大模型领域，最大的模型参数规模通常不会超过700亿参数规模。最近2个月，国外开源的DBRX、Mixtral-8×22B-MoE是最新的超过1000亿参数规模的模型。而国内此前开源领域最大的参数模型是720亿参数规模的Qwen1.5-72B规模和650亿参数的深圳元象科技开源的XVERSE-65B。</p><p>这次阿里开源的1100亿参数规模的<b>Qwen1.5-110B模型是截止目前为止国内开源模型中参数规模最大的模型</b>。Qwen1.5-110B模型与其它Qwen1.5系列模型架构一致。采用了分组查询注意力机制，因此推理效率很高。该模型最高支持32K上下文，并且支持多语言，包括英文、中文、法语、西班牙语、德语、俄语、韩语、日文等。</p><p>按照1100亿参数估计，<b>Qwen1.5-110B模型半精度的推理显存需要220GB</b>。</p><p>Qwen1.5-110B模型开源的版本包含基座模型和Chat优化版本，可以说诚意满满！</p><h3><b>Qwen1.5-110B模型的评测结果</b></h3><p>根据官方公布的评测结果，Qwen1.5-110B模型的评测结果略略超过Llama-3-70B和Mixtral-8×22B。也比Qwen1.5-72B模型本身更强，这几个模型的评测结果对比如下：</p><table><tbody><tr><th>模型列表</th><th>Qwen1.5-110B</th><th>Qwen1.5-72B</th><th>Llama-3-70B</th><th>Mixtral-8x22B</th></tr><tr><td>MMLU</td><td>80.4</td><td>77.5</td><td>79.5</td><td>77.8</td></tr><tr><td>TheoremQA</td><td>34.9</td><td>29.3</td><td>32.0</td><td>35.9</td></tr><tr><td>GPQA</td><td>35.9</td><td>36.3</td><td>36.4</td><td>34.3</td></tr><tr><td>Hellaswag</td><td>87.5</td><td>86.0</td><td>88.0</td><td>88.7</td></tr><tr><td>BBH</td><td>74.8</td><td>65.5</td><td>76.6</td><td>69.2</td></tr><tr><td>ARC-C</td><td>69.6</td><td>65.9</td><td>68.8</td><td>70.7</td></tr><tr><td>GSM8K</td><td>85.4</td><td>79.5</td><td>79.2</td><td>78.6</td></tr><tr><td>MATH</td><td>49.6</td><td>34.1</td><td>41.0</td><td>41.7</td></tr><tr><td>HumanEval</td><td>52.4</td><td>41.5</td><td>45.7</td><td>45.1</td></tr><tr><td>MBPP</td><td>58.1</td><td>53.4</td><td>55.1</td><td>71.2</td></tr></tbody></table><p>从上面的对比结果看，<b>Qwen1.5-110B模型在综合理解（MMLU）、数学推理（GSM8K和MATH）方面得分比Llama-3-70B略高一点点，是几个模型中最强的</b>。而在复杂推理任务ARC-C上则略低于Mixtral-8×22B模型。在编程测试HumanEval得分则是远超另几个模型，而MBPP编程测试上则低于Mixtral-8×22B模型。从这个评测结果看，Qwen1.5-110B模型应该是与全球最强的开源模型可以一拼。</p><p>在DataLearnerAI收集的全球大模型排行榜中，Qwen1.5-110B模型的评测结果非常靠前：</p><figure><img class="origin_image zh-lightbox-thumb" src="https://pic1.zhimg.com/v2-8bac07d31ac76fdee4627594e2b9e82c_1440w.jpg" /></figure><p class="ztext-empty-paragraph"><br /></p><p>数据来源：<a class=" external" href="https://www.datalearner.com/ai-models/leaderboard/datalearner-llm-leaderboard" rel="nofollow noreferrer" target="_blank"><span class="invisible">https://www.</span><span class="visible">datalearner.com/ai-mode</span><span class="invisible">ls/leaderboard/datalearner-llm-leaderboard</span><span class="ellipsis"></span></a></p><p>这是按照MMLU排序的结果，也是除了Llama3-70B-Instruct模型外最强的开源模型。</p><h3><b>Qwen1.5-110B模型实测结果</b></h3><p>官方在HF上放了演示链接，我们用一个实例测试了Qwen1.5-110B和Qwen1.5-72B，模型逻辑方面Qwen1.5-110B模型明显更好，答案非常准确：</p><figure><img class="origin_image zh-lightbox-thumb" src="https://pic4.zhimg.com/v2-2843bffcfec34202c125e4dcfdbe807b_1440w.jpg" /></figure><p class="ztext-empty-paragraph"><br /></p><figure><img class="origin_image zh-lightbox-thumb" src="https://pic2.zhimg.com/v2-7af782ea1a4ad4b0b966da1a32033b69_1440w.jpg" /></figure><p class="ztext-empty-paragraph"><br /></p><p>模型的开源地址和演示地址可以参考DataLearnerAI的模型信息卡：<br />Qwen1.5-110B：<a class=" external" href="https://www.datalearner.com/ai-models/pretrained-models/Qwen1_5-110B" rel="nofollow noreferrer" target="_blank"><span class="invisible">https://www.</span><span class="visible">datalearner.com/ai-mode</span><span class="invisible">ls/pretrained-models/Qwen1_5-110B</span><span class="ellipsis"></span></a><br />Qwen1.5-110B-Chat：<a class=" external" href="https://www.datalearner.com/ai-models/pretrained-models/Qwen1_5-110B-Chat" rel="nofollow noreferrer" target="_blank"><span class="invisible">https://www.</span><span class="visible">datalearner.com/ai-mode</span><span class="invisible">ls/pretrained-models/Qwen1_5-110B-Chat</span><span class="ellipsis"></span></a><br />Qwen1.5-72B：<a class=" external" href="https://www.datalearner.com/ai-models/pretrained-models/Qwen1_5-72B-Chat" rel="nofollow noreferrer" target="_blank"><span class="invisible">https://www.</span><span class="visible">datalearner.com/ai-mode</span><span class="invisible">ls/pretrained-models/Qwen1_5-72B-Chat</span><span class="ellipsis"></span></a></p>
]]></content:encoded>
<pubDate>Sat, 27 Apr 2024 11:34:27 GMT</pubDate>
<pubDate>Sat, 27 Apr 2024 11:34:27 GMT</pubDate>
</item>
<item>
<title>数据学习赞同了回答: 体制内越级汇报的危害有哪些？</title>
<link>https://www.zhihu.com/question/359501955/answer/2017343834</link>
<guid>https://www.zhihu.com/question/359501955/answer/2017343834</guid>
<content:encoded><![CDATA[

<p><b>就单单讲疫情武汉封城的事情吧，从中看看从市、省、中央的决策是怎样的。</b></p><p>我记得原来看过一篇文章讲的是武汉封城的前后决策过程，大概是这样的：</p><p>武汉医院接收了不明肺炎患者以后，院方汇报给市疾控中心，疾控中心上报市卫健委，市卫健委汇报给市委市政府，这个时候应该都是属于不了解肺炎的具体情况，然后市卫健委和市委市政府一起向湖北省卫健委和省委省政府汇报了，但是也都是都不了解具体的情况和该怎么处理。这时候省卫健委就向国家疾控中心汇报，说发现了不明肺炎。</p><p>这时候大概是1月19号20号那两天？国家疾控中心就派了专家组（钟南山李兰娟高福那些人）来武汉实地调查，发现人传人的现象以后回北京（注意是要回北京，不是在湖北，因为发现人传人的现象是十分严重的公共卫生事件，而且武汉这么大的一个城市，中部枢纽，副省级市，封城需要中央批准，省一级根本做不了主）马上报给国家卫健委，国家卫健委开会汇总以后，专家组和国家卫健委主任（正部级）一起向分管医疗卫生的国务院副总理汇报（好像是孙）。这时候武汉的肺炎已经算是中央介入的大事件了，这还不算，接下来是专家组、卫健委主任、分管副总理一起参会，向总理汇报了这个情况。我记得总理还说了一句“谢谢你们的工作，这对我们后续决策有非常大的帮助。”（开始暗示中央的直接介入）接下来应该是总理向zz局常委会汇报了这个情况（中国最高决策层），后来的事情大家都知道了，中央决定武汉封城，zz局常委会开会，在全国部署抗疫工作。</p><p>从发现病毒到武汉封城全国抗疫，汇报都是一层一层逐级上报的，不可能存在越级上报的情况，越到高层这个现象越显著。你想想，如果是武汉市委直接报送zz局常委会，市政府直接报送国务院，不是先追查病毒的事情，中央会先问责为什么你们绕过省委来汇报？省委也会问责为什么对不明肺炎这个情况完全不了解，反过来被中央骂了一通才知道？越级上报最糟的后果就是造成上级对下级或者下下级的情况不了解，无法有效做出决策。体制内分管部门这么多，每个部门都是对具体的事务负责的，有成熟的经验和应对措施去处理，越级上报对上对下都是不科学、不负责的行为。当然有一些非常紧急的事务除外。</p><p>这里不是洗白什么的……面对新冠这种全球性的卫生事件，刚开始谁都不了解情况，也只能一层一层报送上去，你总不能说某个市的医院收治了几个情况不明的患者就需要惊动国务院吧。后来也是事情越来越严重病例越来越多，中央才介入的。</p>
]]></content:encoded>
<pubDate>Wed, 24 Apr 2024 07:31:22 GMT</pubDate>
<pubDate>Wed, 24 Apr 2024 07:31:22 GMT</pubDate>
</item>
<item>
<title>数据学习发表了文章: 使用Gradio配合transformers的text streamer实现Llama3-8B-Instruct的网页聊天机器人，流式输出</title>
<link>https://zhuanlan.zhihu.com/p/693990819</link>
<guid>https://zhuanlan.zhihu.com/p/693990819</guid>
<content:encoded><![CDATA[

<p>DataLearnerAI官网新增了一个大模型教程模块，提供了当前主流大模型的部署教程，目的是在一个py文件中完成大模型的部署启动。目前已经上架了MiniCPM、多模态Yi-VL6-B和Llama3-8B三个模型的部署教程，三个教程均在一个py文件中实现，并可以通过gradio的chatbot完成web交互。所有的代码也都开源在github中。这些代码大多数内部测试使用，现在共享给大家，欢迎交流。同时我们也在仙宫云上发布了镜像（仙宫云提供按分钟租赁4090显卡，所以适合前期的学习测试，性价比很高，DataLearnerAI部分模型功能也是在此测试完成的），有模型部署学习和测试的童鞋可以关注交流。后续会陆续上架新的模型。</p><p>DataLearnerAI大模型部署教程地址：<a class=" wrap external" href="https://www.datalearner.com/llm-tutorials/pretrained-model-tutorials" rel="nofollow noreferrer" target="_blank">AI大模型手把手部署教程！ | 数据学习(DataLearner)</a></p><p>仙宫云的4090租赁可以参考：<a class=" wrap external" href="https://www.datalearner.com/blog/1051701266330292" rel="nofollow noreferrer" target="_blank">推荐一个国内可以按分钟计费的4090显卡租用公有云，一个小时24GB显存的4090只需要2.37元——仙宫云</a></p><h3>基本信息</h3><p>原教程地址： </p><a class=" wrap external" href="https://www.datalearner.com/llm-tutorials/pretrained-model-tutorials/llama3-8b-instruct-deployment-with-4090-in-one-py-file" rel="nofollow noreferrer" target="_blank">使用Gradio配合transformers的text streamer实现Llama3-8B-Instruct的网页聊天机器人，流式输出</a><p>模型发布时间： 2024-04-18</p><p> 模型发布机构： Facebook AI研究实验室</p><p> 模型信息详情：<a class=" wrap external" href="https://www.datalearner.com/ai-models/pretrained-models/Llama3-8B-Instruct" rel="nofollow noreferrer" target="_blank"> https://www.datalearner.com/ai-models/pretrained-models/Llama3-8B-Instruct</a></p><p> 代码GitHub地址：<a class=" wrap external" href="https://github.com/DataLearnerAI/LLMPractice/tree/main/llm_code/llama3" rel="nofollow noreferrer" target="_blank"> https://github.com/DataLearnerAI/LLMPractice/tree/main/llm_code/llama3</a></p><p> 仙宫云一键镜像：<a class=" wrap external" href="https://www.xiangongyun.com/image/detail/613fcb84-d94d-4b42-8584-e18aa7c92964" rel="nofollow noreferrer" target="_blank"> https://www.xiangongyun.com/image/detail/613fcb84-d94d-4b42-8584-e18aa7c92964</a></p><p>我们与GPU租赁服务商仙宫云合作，为您提供一键部署的便捷服务。上述镜像链接，您可以直接点击部署即可运行。 仙宫云提供按分钟租赁4090显卡（24GB）显存，因此可以运行120亿参数规模及以下的大语言模型（fp16精度），关于仙宫云的介绍参考：<a class=" wrap external" href="https://www.datalearner.com/blog/1051701266330292" rel="nofollow noreferrer" target="_blank">推荐一个国内可以按分钟计费的4090显卡租用公有云，一个小时24GB显存的4090只需要2.37元——仙宫云</a></p><p>注：通过DataLearnerAI专用邀请链接注册会有额外的3元即共8元的赠送额度（自己注册仅有5元额度）：<a class=" external" href="https://www.xiangongyun.com/register/6WTXZM" rel="nofollow noreferrer" target="_blank"><span class="invisible">https://www.</span><span class="visible">xiangongyun.com/registe</span><span class="invisible">r/6WTXZM</span><span class="ellipsis"></span></a></p><p><b>Llama3-8B-Instruct 部署教程简介</b><br /><br />Llama3系列是MetaAI最新开源的大语言模型，相比第二代模型，它的性能提升很高。其中700亿参数版本在上线2天后就在ChatBot Arena大模型匿名竞技场获得了2700多个匿名投票，得分超过此前最强的开源模型Command R+，登顶开源模型第一。相比较其它模型，Llama3在更多的数据训练，其模型架构也有了新的变化。关于Llama3系列模型的信息可以参考DataLearnerAI的介绍：<a class=" external" href="https://www.datalearner.com/blog/1051713454866102" rel="nofollow noreferrer" target="_blank"><span class="invisible">https://www.</span><span class="visible">datalearner.com/blog/10</span><span class="invisible">51713454866102</span><span class="ellipsis"></span></a><br /></p><figure><img class="origin_image zh-lightbox-thumb" src="https://pic3.zhimg.com/v2-8f28f743d7da20da60d3c20eefb0ad86_1440w.jpg" /></figure><p><br />本教程的主要目的是通过Gradio提供的ChatBot组件做web版的大模型聊天页面原型。然后通过transformers库提供TextStreamer工具实现流式输出。所有的代码均只在一个py文件中实现，搭建好环境之后，直接运行脚本即可绑定服务到80端口，实现web版本的Llama3的聊天应用。<br />注意，这里的Llama3-8B-Instruct模型本身需要约15GB显存才可以推理，如果你本身已经有资源可以自己尝试部署。本教程的模型文件和代码已经打包成镜像，上架仙宫云的镜像市场，想一键体验或者低成本学习可以参考我们的仙宫云合作信息。<br /></p><ul><li><a class=" wrap external" href="https://www.datalearner.com/llm-tutorials/pretrained-model-tutorials/llama3-8b-instruct-deployment-with-4090-in-one-py-file#Llama3%E6%A8%A1%E5%9E%8B%E4%B8%8B%E8%BD%BD" rel="nofollow noreferrer" target="_blank">Llama3模型下载</a></li><li><a class=" wrap external" href="https://www.datalearner.com/llm-tutorials/pretrained-model-tutorials/llama3-8b-instruct-deployment-with-4090-in-one-py-file#%E6%AD%A5%E9%AA%A40%EF%BC%9A%E5%AE%89%E8%A3%85Llama3-8B-Instruct%E7%9A%84%E4%BE%9D%E8%B5%96" rel="nofollow noreferrer" target="_blank">步骤0：安装Llama3-8B-Instruct的依赖</a></li><li><a class=" wrap external" href="https://www.datalearner.com/llm-tutorials/pretrained-model-tutorials/llama3-8b-instruct-deployment-with-4090-in-one-py-file#%E6%AD%A5%E9%AA%A41%EF%BC%9A%E5%AE%9A%E4%B9%89%E5%85%A8%E5%B1%80%E5%8F%98%E9%87%8F" rel="nofollow noreferrer" target="_blank">步骤1：定义全局变量</a></li><li><a class=" wrap external" href="https://www.datalearner.com/llm-tutorials/pretrained-model-tutorials/llama3-8b-instruct-deployment-with-4090-in-one-py-file#%E6%AD%A5%E9%AA%A42%EF%BC%9A%E5%88%9D%E5%A7%8B%E5%8C%96Llama3-8B-Instruct%E6%A8%A1%E5%9E%8B" rel="nofollow noreferrer" target="_blank">步骤2：初始化Llama3-8B-Instruct模型</a></li><li><a class=" wrap external" href="https://www.datalearner.com/llm-tutorials/pretrained-model-tutorials/llama3-8b-instruct-deployment-with-4090-in-one-py-file#%E6%AD%A5%E9%AA%A43%EF%BC%9A%E5%9F%BA%E4%BA%8EGradio%E5%88%9B%E5%BB%BA%E8%81%8A%E5%A4%A9%E6%9C%BA%E5%99%A8%E4%BA%BA" rel="nofollow noreferrer" target="_blank">步骤3：基于Gradio创建聊天机器人</a></li><li><a class=" wrap external" href="https://www.datalearner.com/llm-tutorials/pretrained-model-tutorials/llama3-8b-instruct-deployment-with-4090-in-one-py-file#%E6%AD%A5%E9%AA%A44%EF%BC%9A%E5%A2%9E%E5%8A%A0%E7%94%A8transformers%E5%BA%93%E6%8E%A8%E7%90%86%E5%9B%9E%E5%A4%8D%E7%94%A8%E6%88%B7%E6%B6%88%E6%81%AF%E7%9A%84%E5%86%85%E5%AE%B9" rel="nofollow noreferrer" target="_blank">步骤4：增加用transformers库推理回复用户消息的内容</a></li><li><a class=" wrap external" href="https://www.datalearner.com/llm-tutorials/pretrained-model-tutorials/llama3-8b-instruct-deployment-with-4090-in-one-py-file#%E6%AD%A5%E9%AA%A45%EF%BC%9A%E7%BB%91%E5%AE%9AGradio%E5%BA%94%E7%94%A8%E5%88%B080%E7%AB%AF%E5%8F%A3%EF%BC%8C%E8%BF%90%E8%A1%8Cweb%E8%AE%BF%E9%97%AE" rel="nofollow noreferrer" target="_blank">步骤5：绑定Gradio应用到80端口，运行web访问</a></li><li><a class=" wrap external" href="https://www.datalearner.com/llm-tutorials/pretrained-model-tutorials/llama3-8b-instruct-deployment-with-4090-in-one-py-file#%E5%9C%A8%E4%BB%99%E5%AE%AB%E4%BA%91%E9%95%9C%E5%83%8F%E4%B8%AD%E8%BF%90%E8%A1%8C" rel="nofollow noreferrer" target="_blank">在仙宫云镜像中运行</a></li></ul><p><br /><b>Llama3模型下载</b><br />首先需要注意的是，我们需要自己去申请拿到Llama3的预训练结果。目前看，尽量不要用个人邮箱申请，速度很快。DataLearnerAI的童鞋用企业邮箱注册申请，官网申请秒批，在HuggingFace申请也只需要几分钟即可。<br />另外，需要注意的是，GitHub下载脚本下载需要审批通过的链接。HuggingFace上审批通过后需要配合access token下载。这部分可以用git命令也可以用HuggingFace的transformers库在python中下载认证。我们推荐自己手动下载。<br /><b>步骤0：安装Llama3-8B-Instruct的依赖</b><br />Llama3模型本身比较新，采用了一些比较新的架构。我们这里考虑使用transformers库做推理，同时使用accelerate加速，并最终使用gradio创建聊天界面，因此，在安装了torch的cuda版本前提下还需要安装上述三个库（升级到新版本）：<br /></p><div class="highlight"><pre><code class="language-text">pip install -U transformers
pip install -U accelerate
pip install -U gradio</code></pre></div><p><br /><b>步骤1：定义全局变量</b><br />为了初始化模型，以及设置一些Gradio聊天的界面头像，我们先定义几个全局变量，便于后续使用。<br /></p><div class="highlight"><pre><code class="language-text">bot_avatar = "/home/datalearner/dl_logo_rect.png" # 聊天机器人头像位置
user_avatar = "/home/datalearner/user_avatar.png" # 用户头像位置
model_path = "/home/datalearner/Meta-Llama-3-8B-Instruct" # 已下载的模型位置

# 存储全局的历史对话记录，Llama3支持系统prompt，所以这里默认设置！
llama3_chat_history = [
 {"role": "system", "content": "You are a helpful assistant trained by MetaAI! But you are running with DataLearnerAI Code."}
]

# 初始化所有变量，用于载入模型
tokenizer = None
streamer = None
model = None
terminators = None</code></pre></div><p><br />这里需要注意2点：一个是llama3_chat_history，这个变量用来保存历史对话信息，尽管Gradio的Chatbot组件有内置的变量chat_history管理对话记录。但是最新的模型的历史消息一般都包含了系统指令、不同角色区分等。所以我们用一个新的变量保存这些历史记录。也因为引入了这个变量，在清理历史消息的时候也需要增加清理机制。另一个变量是terminators，是模型停止输出的标记。在Llama3中，结束标记是它们自己设置的，这个变量如果没有或者设置错误，即使是Insturct指令优化模型也会出现不断生成内容的现象，也就是说模型不知道什么时候该停止生成。<br /><b>步骤2：初始化Llama3-8B-Instruct模型</b><br />我们单独定义一个函数，用来初始化并加载Llama3-8B-Instruct模型，只需要启动时候初始化，后续对话直接使用即可：<br /></p><div class="highlight"><pre><code class="language-text">def init_model():
 """初始化模型，载入本地模型
    """
 global tokenizer, model, streamer, terminators
    tokenizer = AutoTokenizer.from_pretrained(
        model_path, local_files_only=True)

    model = AutoModelForCausalLM.from_pretrained(
        model_path,
        torch_dtype=torch.float16,
        device_map=device,
        trust_remote_code=True
 )

    terminators = [
        tokenizer.eos_token_id,
        tokenizer.convert_tokens_to_ids("&lt;|eot_id|&gt;")
 ]

    streamer = TextIteratorStreamer(
        tokenizer,
        skip_prompt=True,
        skip_special_tokens=True
 )

初始化方法很简单，与常规的transformers写法一致，只是注意Llama3独有的停止标记即可。
步骤3：基于Gradio创建聊天机器人
接下来我们就可以使用Gradio来创建聊天机器人了。大致的代码框架如下：


with gr.Blocks() as demo:
 # step1: 载入模型
    init_model()

 # step2: 初始化gradio的chatbot应用，并添加按钮等信息
    chatbot = gr.Chatbot(
        height=900,
        avatar_images=(user_avatar, bot_avatar)
 )
    msg = gr.Textbox()
    clear = gr.ClearButton([msg, chatbot])

 # 清楚历史记录
 def clear_history():
 global llama3_chat_history
        llama3_chat_history = []

 # 用于回复的方法
 def respond(message, chat_history):
 """用Llama3回复用户消息的处理，后面展开"""

 # 点击清楚按钮，触发历史记录清楚
    clear.click(clear_history)
    msg.submit(respond, [msg, chatbot], [msg, chatbot])</code></pre></div><p><br />这是非常常规的Gradio聊天机器人应用写法。除了清楚历史记录外，其它都与官方代码一致。这里加的的清楚历史记录是在最后<code>clear.click</code>时候触发的，clear是清楚历史对话的按钮，我们前面说了，我们引入了自定义变量，那就要这里记住清楚。<br />接下来我们来加入推理代码即可，即代码中<code>用于回复的方法</code>注释部分的具体内容。<br /><b>步骤4：增加用transformers库推理回复用户消息的内容</b><br />这部分就是拿到用户的输入之后，怎么用流式方法推理。在步骤2中，我们已经初始化了Llama3-8B-Instruct模型相关的变量。那么流式推理的代码主要如下：<br /></p><div class="highlight"><pre><code class="language-text">def respond(message, chat_history):

 # 引入全局变量
 global llama3_chat_history, tokenizer, model, streamer

 # 拼接对话历史
        llama3_chat_history.append({"role": "user", "content": message})

 # 使用Llama3自带的聊天模板，格式化对话记录
        history_str = tokenizer.apply_chat_template(
            llama3_chat_history,
            tokenize=False,
            add_generation_prompt=True
 )

 # 对历史记录进行tokenization
        inputs = tokenizer(history_str, return_tensors='pt').to(device)

 # 这个历史记录是Gradio的Chatbot自带的变量，用来控制页面显示逻辑的，我们必须也要对齐操作，保证页面展示正常
        chat_history.append([message, ""])

 # 拼接推理参数
        generation_kwargs = dict(
 **inputs,
            streamer=streamer,
            max_new_tokens=4096,
            num_beams=1,
            do_sample=True,
            top_p=0.8,
            temperature=0.3,
            eos_token_id=terminators
 )

 # 启动线程，用以监控流失输出结果
        thread = Thread(target=model.generate, kwargs=generation_kwargs)
        thread.start()

 # 循环推理streamer，每次得到新增的推理部分都附到历史记录末尾，Gradio会监控这个变量在页面展示
 for new_text in streamer:
            chat_history[-1][1] += new_text
 yield "", chat_history

 # 所有的输出完毕之后，我们自己的历史记录也要更新，把模型输出的完整结果加进来。
        llama3_chat_history.append(
 {"role": "assistant", "content": chat_history[-1][1]}
 )</code></pre></div><p><br />可以看到，这部分内容是最多的。但也不是很复杂。把前面所有内容拼接好就是完整的代码了。这个代码放到一个py脚本中，后面直接执行这个脚本即可。唯一需要注意的是安装的transformers等依赖的版本，以及要提前把模型文件下载好。<br /><b>步骤5：绑定Gradio应用到80端口，运行web访问</b><br />这一步很简单，常规操作，但是没有就无法启动了：<br /></p><div class="highlight"><pre><code class="language-text">if __name__ == "__main__":
    demo.launch(server_name="0.0.0.0", server_port=80)</code></pre></div><p><br /><b>在仙宫云镜像中运行</b><br />最后，我们简单总结一下如何运行。首先，这里的运行并不是一定要在仙宫云进行。只要你在超过15G显卡的机器上就可以跑了。<br />我们的代码文件和模型文件都放在了<code>/home/datalearner</code>目录下的<code>run_llama3_gradio.py</code>，所以，我们执行如下命令即可：<br /></p><div class="highlight"><pre><code class="language-text">cd /home/datalearner
python run_llama3_gradio.py</code></pre></div><p><br />接下来可以看到如下界面提示：<br /></p><figure><img class="origin_image zh-lightbox-thumb" src="https://pic4.zhimg.com/v2-b8f92ae826f0e04c0c3f1d5b19894d4b_1440w.jpg" /></figure><p>这个提示说明成功拉起模型，并启动了gradio的界面。通过运行<code>nvidia-smi</code>命令也可以看到4090的显存已经消耗了15GB左右。说明半精度的Llama3-8B-Instruct已经拉起来了。<br /></p><figure><img class="origin_image zh-lightbox-thumb" src="https://pic4.zhimg.com/v2-c963fb77f83cb853de203d5ef1cbaebf_1440w.jpg" /></figure><p><br />启动完成之后，本地可以通过http:/127.0.0.1 访问，如果是仙宫云，则直接可以通过外网访问。可以通过仙宫云控制台访问web地址：<br /></p><figure><img class="origin_image zh-lightbox-thumb" src="https://pic4.zhimg.com/v2-ebe1d3ead8468f4b8e4e0e1771957353_1440w.jpg" /></figure><p><br />Llama3-8B-Instruct简单测试：<br /></p><figure><img class="origin_image zh-lightbox-thumb" src="https://pic1.zhimg.com/v2-b4b01ad54c7242bce6304fe47ca2c5c8_1440w.jpg" /></figure><p class="ztext-empty-paragraph"><br /></p><p>访问教程原地址：<a class=" wrap external" href="https://www.datalearner.com/llm-tutorials/pretrained-model-tutorials/llama3-8b-instruct-deployment-with-4090-in-one-py-file" rel="nofollow noreferrer" target="_blank">使用Gradio配合transformers的text streamer实现Llama3-8B-Instruct的网页聊天机器人，流式输出 | 数据学习(DataLearner)</a></p>
]]></content:encoded>
<pubDate>Tue, 23 Apr 2024 00:58:14 GMT</pubDate>
<pubDate>Tue, 23 Apr 2024 00:58:14 GMT</pubDate>
</item>
<item>
<title>数据学习发表了文章: Llama3相比较前两代的模型（Llama1和Llama2）有哪些升级？几张图简单总结Llama3的训练成本、训练时间、模型架构升级等情况</title>
<link>https://zhuanlan.zhihu.com/p/693742469</link>
<guid>https://zhuanlan.zhihu.com/p/693742469</guid>
<content:encoded><![CDATA[

<p>本文原文来自DataLearnerAI官方网站：</p><a class=" wrap external" href="https://www.datalearner.com/blog/1051713702716647" rel="nofollow noreferrer" target="_blank">Llama3相比较前两代的模型（Llama1和Llama2）有哪些升级？几张图简单总结Llama3的训练成本、训练时间、模型架构升级等情况</a><hr /><p>Llama3是MetaAI开源的最新一代大语言模型。一发布就引起了全球AI大模型领域的广泛关注。这是MetaAI开源的第三代大语言模型，也是当前最强的开源模型。但相比较第一代和第二代的Llama模型，Llama3的升级之处有哪些？本文以图表的方式总结Llama3的升级之处。</p><figure><img class="origin_image zh-lightbox-thumb" src="https://pic3.zhimg.com/v2-9fd84180a853e0853b989432ee0248d6_1440w.jpg" /></figure><p class="ztext-empty-paragraph"><br /></p><p class="ztext-empty-paragraph"><br /></p><ul><li><a class=" wrap external" href="https://www.datalearner.com/blog/1051713702716647#Llama3%E7%B3%BB%E5%88%97%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%80%BB%E7%BB%93" rel="nofollow noreferrer" target="_blank">Llama3系列模型的总结</a></li><li><a class=" wrap external" href="https://www.datalearner.com/blog/1051713702716647#Llama3%E6%A8%A1%E5%9E%8B%E6%9E%B6%E6%9E%84%E7%9A%84%E5%8D%87%E7%BA%A7" rel="nofollow noreferrer" target="_blank">Llama3模型架构的升级</a></li><ul><li><a class=" wrap external" href="https://www.datalearner.com/blog/1051713702716647#Llama%E7%B3%BB%E5%88%97%E7%9A%84%E4%B8%8A%E4%B8%8B%E6%96%87%E9%95%BF%E5%BA%A6%E4%B8%80%E7%9B%B4%E5%9C%A8%E5%A2%9E%E9%95%BF" rel="nofollow noreferrer" target="_blank">Llama系列的上下文长度一直在增长</a></li><li><a class=" wrap external" href="https://www.datalearner.com/blog/1051713702716647#Llama3%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%AF%8D%E6%B1%87%E8%A1%A8%E5%A4%A7%E5%B9%85%E6%8F%90%E9%AB%98" rel="nofollow noreferrer" target="_blank">Llama3模型的词汇表大幅提高</a></li></ul><li><a class=" wrap external" href="https://www.datalearner.com/blog/1051713702716647#Llama3%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B%E7%9A%84%E5%8D%87%E7%BA%A7" rel="nofollow noreferrer" target="_blank">Llama3模型的训练过程的升级</a></li><ul><li><a class=" wrap external" href="https://www.datalearner.com/blog/1051713702716647#Llama3%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%AE%AD%E7%BB%83%E6%95%B0%E6%8D%AE%E5%A4%A7%E5%B9%85%E5%A2%9E%E9%95%BF" rel="nofollow noreferrer" target="_blank">Llama3模型的训练数据大幅增长</a></li><li><a class=" wrap external" href="https://www.datalearner.com/blog/1051713702716647#Llama3%E6%A8%A1%E5%9E%8B%E7%9A%84%E8%AE%AD%E7%BB%83%E6%97%B6%E9%95%BF%E5%A4%A7%E5%B9%85%E5%A2%9E%E5%8A%A0" rel="nofollow noreferrer" target="_blank">Llama3模型的训练时长大幅增加</a></li></ul><li><a class=" wrap external" href="https://www.datalearner.com/blog/1051713702716647#Llama3%E7%9A%84%E8%AF%84%E6%B5%8B%E5%9F%BA%E5%87%86%E5%A4%A7%E5%B9%85%E6%8F%90%E9%AB%98" rel="nofollow noreferrer" target="_blank">Llama3的评测基准大幅提高</a></li><li><a class=" wrap external" href="https://www.datalearner.com/blog/1051713702716647#Llama3%E6%80%BB%E7%BB%93" rel="nofollow noreferrer" target="_blank">Llama3总结</a></li></ul><h3><b>Llama3系列模型的总结</b></h3><p>第一代和第二代的Llama模型都包含了四个不同参数规模的版本，其中最小的模型参数规模在70亿，往上分别有130亿、340亿和700亿（第一代最高的是650亿）。而此次发布的第三代Llama3模型，目前公开的只有80亿参数规模版本和700亿版本。而根据透露，最高的参数版本是4000亿参数规模的模型。只是目前还在训练中。</p><p>下面我们就用图表的形式说明本次Llama3的升级细节。</p><h3><b>Llama3模型架构的升级</b></h3><p>首先是模型架构相关的升级。目前，官方没有公开Llama3的技术报告或者论文细节，在官方博客中只给出了一些简单的指标。</p><p>关于Llama3的模型架构，应该是没有本质变化，官方的说法是：</p><blockquote><i>根据我们的设计理念，我们在 Llama 3 中选择了一个相对标准的纯解码器（decoder-only）变压器架构。</i></blockquote><p>因此，模型架构基本没变，但是增加了Group Query Attention（分组查询注意力，GQA），这项技术最大的特点是可以<b>加速推理</b>，这也是我们实测中感受到的，Llama3-8B-Instruct在4090上的速度飞快！</p><p>剩余的模型相关的架构，这里我们对比的是上下文长度和词汇表。</p><h3><b>Llama系列的上下文长度一直在增长</b></h3><p>上下文长度每一代都翻倍了，在Llama3中，训练的时候用的就是8K上下文：</p><figure><img class="origin_image zh-lightbox-thumb" src="https://pic3.zhimg.com/v2-331f89b187e26b6abf4671589ef4561e_1440w.jpg" /></figure><p class="ztext-empty-paragraph"><br /></p><p>从上图可以看到，Llama三代模型的上下文长度分别是2K、4K和8K，虽然<b>Llama3训练是8K上下文</b>，但是按照目前业界的技术，应该是可以继续拓展到更长上下文的。而官方也说过，未来Llama3会有更长上下文的版本。</p><h3><b>Llama3模型的词汇表大幅提高</b></h3><p>在模型架构中另一个值得注意的是词汇表的大幅提高。在Llama1和Llama2中，MetaAI的词汇表都是32K大小，这可能与前两代模型的训练数据差距不大有关。而第三代的<b>Llama3模型的词汇表大小变为128K</b>，也就是说它的tokenizer有了较大的变化。</p><figure><img class="origin_image zh-lightbox-thumb" src="https://pic1.zhimg.com/v2-27bcfdfcd60136eb9c39c94e33174df4_1440w.jpg" /></figure><p class="ztext-empty-paragraph"><br /></p><p>更大的词汇表意味着更强的语义表达能力，也是支持更多语言的一个基础。</p><h3><b>Llama3模型的训练过程的升级</b></h3><p>训练过程的升级我们主要看训练时间和训练数据的变化。此前，业界一直说大模型的训练成本在下降。但是从Llama3的变化看，下降的是单位训练成本，但是大模型总的训练成本其实在大幅增长。</p><h3><b>Llama3模型的训练数据大幅增长</b></h3><p>Llama3的训练数据达到了15万亿，比第一代和第二代Llama模型加在一起还多好几倍。如下图所示，第一代的小一点的模型训练数据是1万亿tokens，而较大的650亿规模的模型训练数据是1.4万亿tokens。到了第二代Llama2系列，训练数据都增长到了2万亿tokens。</p><figure><img class="origin_image zh-lightbox-thumb" src="https://pic1.zhimg.com/v2-6aa9f7baad624cc343514e4238e94a84_1440w.jpg" /></figure><p class="ztext-empty-paragraph"><br /></p><p>可以看到，第三代Llama3训练数据大幅增加，几乎也是目前市场上训练数据最多的一个模型了。这里有一点也值得提一下，DeepMind发表过一个非常著名的论文，就是关于Chinchilla模型的论文，里面提到了训练数据对大模型性能的影响。根据论文发现的规律，<b>80亿参数规模的模型，用2000亿tokens数据集训练可以获得最佳性能，但是MetaAI发现，80亿参数规模的模型训练数据增长到15万亿tokens依然可以获得log线性增长！</b>因此，数据可以说依然是王道。</p><h3><b>Llama3模型的训练时长大幅增加</b></h3><p>Meta公司一直是全球拥有显卡最多的公司。在Llama1论文发布的时候，大家就发现，Meta训练Llama1模型可能花费了几百万上千万美金。原因是650亿参数的Llama1模型训练了102万个GPU小时，按照公有云A100租赁的价格打折计算，这个成本也是几百万美金。</p><p>到了Llama3模型这里，训练成本的增长更为恐怖，<b>Llama3-8B模型的训练时长比650亿参数规模的Llama1模型还长</b>。结果如下：</p><figure><img class="origin_image zh-lightbox-thumb" src="https://pic1.zhimg.com/v2-86bf0007b31daa3b780c0e1aec3c76ec_1440w.jpg" /></figure><p class="ztext-empty-paragraph"><br /></p><p>上图对比的是Llama系列70亿参数规模模型和700亿参数规模模型的训练时长，单位是百万个GPU小时。忽略Llama3可能是H100的显卡，这个训练时长增长太恐怖了。而这些也是Llama3训练成本非常高的原因之一。<b>以700亿参数规模的Llama3-70B为例，训练时长是640万个GPU小时。以AWS的p4d.24xlarge实例计算，包含8个A100，按需付费8卡是32.77美元一个小时，640万个GPU小时是80万台这样的机器，按需付费的价格就是80万×32.77美元=2621.6万美元</b>，假设MetaAI自己用自己的硬件，成本是一半的话，训练700亿参数规模的Llama3-70B就是1300多万美元，十分之一的话那就是262万美元！成本十分昂贵！</p><h3><b>Llama3的评测基准大幅提高</b></h3><p>最后，我们用图表展示一下三代不同Llama系列在综合理解评测基准MMLU、数学推理GSM8K以及代码能力HumanEval的评测结果。不用说，Llama3相比Llama2的提升应该是比Llama2相比Llama1的提升要高的多的。</p><figure><img class="origin_image zh-lightbox-thumb" src="https://pic1.zhimg.com/v2-3a8b1cb4f9699cac10899377721d8fcc_1440w.jpg" /></figure><p class="ztext-empty-paragraph"><br /></p><figure><img class="origin_image zh-lightbox-thumb" src="https://pic1.zhimg.com/v2-12697a2bb31a6bb7faa3519494d6a710_1440w.jpg" /></figure><p class="ztext-empty-paragraph"><br /></p><figure><img class="origin_image zh-lightbox-thumb" src="https://pic2.zhimg.com/v2-2130abb22291491feef26465fea31489_1440w.jpg" /></figure><p class="ztext-empty-paragraph"><br /></p><p>由于Llama1没有公布GSM8K和HumanEval的评测结果，所以没有展示了。</p><h3><b>Llama3总结</b></h3><p>Llama3-70B模型目前已经是Chatbot Arena大模型匿名评分最高的开源模型了。在英文的分项测试甚至超过了Claude-Opus模型，十分强悍。</p><figure><img class="origin_image zh-lightbox-thumb" src="https://pic1.zhimg.com/v2-f39733ba37ffb7c09f06dc23927058d0_1440w.jpg" /></figure><p class="ztext-empty-paragraph"><br /></p><p>数据来源：<a class=" external" href="https://www.datalearner.com/ai-models/leaderboard/lm-sys-chat-bot-arena-leaderboard" rel="nofollow noreferrer" target="_blank"><span class="invisible">https://www.</span><span class="visible">datalearner.com/ai-mode</span><span class="invisible">ls/leaderboard/lm-sys-chat-bot-arena-leaderboard</span><span class="ellipsis"></span></a></p><p>未来，如果4000亿参数规模的Llama3-400B也开源的话，那无疑是给闭源模型企业一颗巨大的炸弹。</p><p>Llama3的详细介绍参考：<a class=" external" href="https://www.datalearner.com/blog/1051713454866102" rel="nofollow noreferrer" target="_blank"><span class="invisible">https://www.</span><span class="visible">datalearner.com/blog/10</span><span class="invisible">51713454866102</span><span class="ellipsis"></span></a><br />Llama3在大模型匿名竞技场的得分排行榜：<a class=" external" href="https://www.datalearner.com/ai-models/leaderboard/lm-sys-chat-bot-arena-leaderboard" rel="nofollow noreferrer" target="_blank"><span class="invisible">https://www.</span><span class="visible">datalearner.com/ai-mode</span><span class="invisible">ls/leaderboard/lm-sys-chat-bot-arena-leaderboard</span><span class="ellipsis"></span></a><br />Llama3手动部署教程：<a class=" external" href="https://www.datalearner.com/llm-tutorials/pretrained-model-tutorials/llama3-8b-instruct-deployment-with-4090-in-one-py-file" rel="nofollow noreferrer" target="_blank"><span class="invisible">https://www.</span><span class="visible">datalearner.com/llm-tut</span><span class="invisible">orials/pretrained-model-tutorials/llama3-8b-instruct-deployment-with-4090-in-one-py-file</span><span class="ellipsis"></span></a></p>
]]></content:encoded>
<pubDate>Sun, 21 Apr 2024 12:54:33 GMT</pubDate>
<pubDate>Sun, 21 Apr 2024 12:54:33 GMT</pubDate>
</item>
<item>
<title>数据学习回答了问题: 如何看待 Meta 发布 Llama3，并将推出 400B+ 版本？</title>
<link>https://www.zhihu.com/question/653373334/answer/3470986557</link>
<guid>https://www.zhihu.com/question/653373334/answer/3470986557</guid>
<content:encoded><![CDATA[

<p>关于Llama3所有的内容和分析参考DataLearnerAI的官方网站内容：</p><a class=" wrap external" href="https://www.datalearner.com/blog/1051713454866102" rel="nofollow noreferrer" target="_blank">开源王者！全球最强的开源大模型Llama3发布！15万亿数据集训练，最高4000亿参数，数学评测超过GPT-4，全球第二！</a><hr /><p>Llama3系列模型是MetaAI开源的第三代大语言模型，本次发布的包含2个不同参数规模的版本，一个是80亿参数的LLama3-8B，另一个是700亿参数规模的Llama3-70B。MetaAI为此创建了2个2.4万个GPU集群，让Llama3模型在其中的1.6万个GPU上同时训练！</p><p>不过，根据官方描述，目前这两个版本是早期预览版本，未来几个月，Llama3的能力将拓展到<b>多语言</b>支持和<b>更长的上下文</b>，并且会开源<b>更多不同参数规模</b>版本的模型。并且，Llama3将会有<b>多模态</b>版本的模型！</p><p>Llama3模型是在<b>15万亿tokens数据集</b>上训练，是Llama2的7倍！其中的代码数据集高4倍！当前支持的上下文长度是128K！</p><p>这里还有个好消息是，预训练数据集中有<b>5%的非英文数据集</b>，总共支持的<b>语言高达30种</b>，期待包含中文。不过官方也说了，其他语言可能不如英文。但是如果有这个基础，继续做对齐可能更为容易。</p><p>从这些信息看，Llama3整体获得的提升是全方位的，包括多模态、超长上下文、多语种等。而评测结果也很不错。虽然目前MetaAI仅仅开源了2个不同参数规模版本的模型，其中700亿参数规模的模型评测结果极其优秀。最大的亮点是数学评测GSM8K的结果上得分93分！根据DataLearnerAI目前收集的数据，<b>这个分数仅次于Claude3-Opus的95分，超过GPT-4，全球第二，是目前开源大模型中得分最高的一个</b>。</p><figure><img class="origin_image zh-lightbox-thumb lazy" src="https://pic4.zhimg.com/v2-f634c6e902ef7ccbc6adee489db57307_1440w.jpg" /></figure><p>数据来源：<a class=" external" href="https://www.datalearner.com/ai-models/llm-evaluation" rel="nofollow noreferrer" target="_blank"><span class="invisible">https://www.</span><span class="visible">datalearner.com/ai-mode</span><span class="invisible">ls/llm-evaluation</span><span class="ellipsis"></span></a></p><p>可以看到，按照GSM8K排序的靠前的模型中，<b>除了Llama3-70B外，全部是闭源的私有模型</b>。而数学能力与推理等强相关，在解决复杂任务方面有着非常好的优势。而<b>Llama3-70B的MMLU得分82.0分，排名在Claude3-Opus、GPT-4和Gemini Ultra之后，全球第四</b>，成绩也是非常好，也是开源模型中最强的一个。</p><p>而Llama3-8B版本的模型在MMLU（综合理解能力）得分68.4，接近GPT-3.5，GSM8K得分79.6，略超720亿参数规模的Qwen1.5-72B模型！</p><p>另外，作为一个综合的大模型，Llama3-70B模型的代码能力也非常出色，在HumanEval评测上得分81.7，低于GPT-4和Claude3-Opus，也比专门的编程大模型CodeQwen1.5-7B模型略低，但是全球排名第四！</p><p>注意，<b>上述模型的评测的是Llama3的指令优化版本</b>，基座预训练得分目前只公布了MMLU部分，稍低于这些分数。</p><p>MetaAI官方宣布，Llama3有很多版本，其中<b>最大的版本是4000亿参数规模</b>！但是这个版本的模型<b>还在训练中</b>！官方<b>没有明确说未来这个版本是否开源</b>，但是也没有否认。</p><p>目前开源的Llama3-8B和LLama3-70B模型没有在任何平台发布，<b>只在官网提供了申请链接</b>，需要审批通过之后才可以下载</p>
]]></content:encoded>
<pubDate>Thu, 18 Apr 2024 23:47:41 GMT</pubDate>
<pubDate>Thu, 18 Apr 2024 23:47:41 GMT</pubDate>
</item>

</channel>
</rss>