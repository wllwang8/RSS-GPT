<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
<channel>
<title>强化学徒的知乎动态</title>
<link>https://www.zhihu.com/people/heda-he-28/activities</link>

<item>
<title>强化学徒发表了文章: 符尧博士-全栈Transformer推理优化第二季：部署长上下文模型-翻译</title>
<link>https://zhuanlan.zhihu.com/p/697244539</link>
<guid>https://zhuanlan.zhihu.com/p/697244539</guid>
<content:encoded><![CDATA[
<div> Transformer、长上下文、部署成本、优化、性能指标
<br>
<br>
总结:本文讨论了部署长上下文模型的优化挑战，重点关注了预填充、解码、并发性和上下文切换等性能指标。基于硬件架构和上下文长度等因素，提出了提高并发性、减少预填充和解码延迟、压缩隐藏信息等策略。针对层、头、标记和隐藏等维度，讨论了现有工作如何优化长上下文推理，并探讨了如何整合现有努力实现全栈优化。通过压缩KV缓存和维度减少等方法，可以降低长上下文模型的部署成本，促进其在各种应用中的广泛应用。 <div>
<h2>全栈Transformer推理优化第二季：部署长上下文模型</h2><p>符尧 | 网站 | 博客 | 推特 / X</p><p>爱丁堡大学 | <a href="mailto:yao.fu@ed.ac.uk">yao.fu@ed.ac.uk</a> </p><p>2024年5月17日 - Notionlytics | 尚未。</p><p>翻译校对：强化学徒</p><p>原帖：<a class=" wrap external" href="https://yaofu.notion.site/Full-Stack-Transformer-Inference-Optimization-Season-2-Deploying-Long-Context-Models-ee25d3a77ba14f73b8ae19147f77d5e2" rel="nofollow noreferrer" target="_blank">Notion – The all-in-one workspace for your notes, tasks, wikis, and databases.</a></p><p>为什么长上下文模型很重要？因为它们是先进AI应用的基础，如长达一小时的视频理解、存储库级别的编码智能体和终身AI伴侣。我们的研究目标是促进基于AI的应用生态系统。为了实现这一目标，我们必须降低长上下文Transformer的部署成本。</p><p>这是我们Transformer推理优化系列的第二季。在第一篇文章中，我们讨论了通用短上下文推理优化。本文侧重于长上下文优化。我们的目标是解决一个雄心勃勃的研究挑战：</p><ul><li>如何将1M上下文的生产级Transformer的部署成本降低到与4K一样便宜？</li></ul><p>我们首先描述了一个并发编程框架，用于定量分析在GPU高带宽内存（HBM）有限情况下为多个长上下文请求提供服务的效率挑战。我们详细分析了与4K上下文相比，所有额外的计算成本都可以追溯到一个单一来源：大规模的  缓存。我们以A100 NVLink上的50K上下文的34B GPT-3.5级模型为运行示例，并描述了其大型KV缓存导致四种部署挑战：</p><ol><li>预填充长输入比短输入需要更长的计算时间和GPU内存；</li><li>在预填充后，驻留在GPU HBM上的大型KV缓存大大限制了可服务的并发用户数量；</li><li>在解码过程中，从HBM到SM重复读取KV缓存大大增加了延迟；</li><li>当KV缓存内存溢出时，将其从HBM交换到DDR会导致显着的上下文切换延迟。</li></ol><p>我们进一步分析了现有工作如何从这四个角度解决部署挑战，并确定了结合它们构建端到端高效系统的可能性。我们希望我们的工作为分析长上下文Transformer的部署提供了一个基础框架，并确定了降低1M1M上下文推理成本至与4K4K一样便宜的重要方向。</p><p>1 - 有限GPU HBM尺寸下的并发编程框架</p><p>1.1 - 并发用户交互会话和偏好</p><p>1.2 - 作为端到端目标的基于会话的吞吐量</p><p>1.3 - 计算与内存受限性、算术强度和关键批处理大小</p><p>1.4 - 预填充</p><p>1.5 - 解码</p><p>1.6 - 并发控制和上下文切换</p><p>1.7 - 到目前为止的总结</p><p>2 - 强烈影响性能指标的因素</p><p>2.1 - 上下文长度</p><p>2.2 - 硬件架构</p><p>2.3 - 使用张量并行处理的多个GPU</p><p>3 - 可压缩性分析和现有工作</p><p>3.1 - 层</p><p>3.2 - 头</p><p>3.3 - 标记</p><p>3.4 - 隐藏</p><p>4 - 结论：朝着端到端系统的全栈优化迈进</p><h2>1 - 在有限GPU HBM尺寸下的并发编程框架</h2><p>考虑一个30+B 100K上下文GPT-3.5质量的开源模型，如QWen或Yi，4 K4 K与100 K100 K上下文的KV缓存之间的差异为：</p><p>100 K上下文：100000×60×8×128×2×2×2字节=22.8GB   </p><p>4 K上下文：4000seq1en ×60层 ×8头 ×128维度 ×2  ×2bf 16 字节=0.91 GB</p><p>这里我们使用Yi-34B 200K配置（60层，8个kv头和128隐藏维度）。假设我们使用2∗80G A100张量并行处理来为这个模型提供bf16服务，那么我们有2×2× 80−34×2=122 GB 的空余空间用于存储KV缓存。从这第一眼看，我们立即看到在这种设置下，我们可以实现约100+用户并发的4 K上下文，但只有5个100K上下文的用户。如果我们能够以与4 K相同的价格部署1M上下文模型，我们可以大幅推动长上下文和多模态生成模型的民主化，并促进新兴应用和智能体生态系统的发展。实际部署模型为多个用户提供服务时，工作流程通常如下：</p><p class="ztext-empty-paragraph"><br /></p><figure><img class="origin_image zh-lightbox-thumb" src="https://pic2.zhimg.com/v2-3b54cf00c4553e4ec70d0cf86c6fe16d_1440w.jpg" /></figure><p>图1. 在有限GPU HBM尺寸下为多个长上下文用户请求提供服务的并发编程框架。有四个关键因素共同决定用户交互会话的整体吞吐量：（1）并发受HBM尺寸限制：被服务的并发用户数量受GPU高带宽内存（HBM）尺寸的限制；（2）预填充受计算限制：到第一个生成的标记的延迟，即提示预填充时间，受GPU的浮点运算每秒（flops）的限制；（3）解码受内存限制（在关键批量大小下）：自回归解码的延迟（每秒生成的标记）受HBM的带宽限制；（4）上下文切换受PCIE限制：将用户1的KV缓存卸载到CPU DDR并将用户2的KV缓存加载到HBM受PCIE带宽的限制。所有这四个关键因素带来的效率挑战。</p><p>最终可以追溯到KV缓存的大小。</p><h2>1.1 - 并发用户交互会话和偏好</h2><p>在典型的交互会话中，用户从一个长文档的提示开始，然后是一个问题，并将其馈送给模型。模型接收初始提示，预先填充成KV缓存。用户等待预填充阶段，直到第一个标记开始生成，并希望等待时间不要太长。预填充后，模型开始自回归解码。用户与解码过程同时阅读输出，并希望解码速度快于人类阅读速度。模型完成解码后，用户继续阅读响应，思考，也许喝口咖啡，然后开始输入下一个问题。后续提示通常不像第一个提示那样长，因为第一个提示通常包含长上下文（书籍或视频），而后续提示通常只包含问题。当第一个用户正在阅读模型响应并考虑下一个问题时，模型基本上处于空闲状态，因此如果同时另一个用户提出另一个问题，模型可以通过将第一个用户的KV缓存卸载到CPU DDR内存来进行上下文切换，以便为第二个用户的KV缓存腾出HBM空间。两个用户互动地提出后续问题，直到会话结束。</p><h2>1.2 - 作为端到端目标的基于会话的吞吐量</h2><p>我们考虑多个用户同时与模型交互的情况。假设平均而言，一个用户会话包括50K个标记的文档/视频和5轮问题。在收到上一个问题的答案后，用户花费1分钟阅读答案并考虑下一个问题。我们的目标是最大化定义为会话基础吞吐量的指标：</p><figure><img class="origin_image zh-lightbox-thumb" src="https://pic1.zhimg.com/v2-03f2a58fb8e873d0787d20b39d1e7e10_1440w.jpg" /></figure><p>请注意，这个基于会话的吞吐量目标与现有的基于标记的吞吐量（即在给定时间内预填或解码的标记数量）不同。正如我们将很快讨论的那样，基于标记的吞吐量只是问题的一部分。我们的基于会话的吞吐量，即在给定时间内的并发用户交互数量，是一个端到端的目标，因为我们不仅考虑预填和解码，还考虑内存限制和上下文切换。</p><h2>1.3 - 计算与内存受限性，算术强度和关键批处理大小</h2><p>Transformer推理的一个重要观察是，预填充通常受GPU计算能力限制，即浮点运算，而解码受HBM内存带宽限制。我们说一个运算符是计算受限的，如果完成该运算符的大部分时间是在GPU的流多处理器上计算它（GPU执行块状并行计算）。我们说一个运算符是内存受限的，如果完成该运算符的大部分时间是将数据从内存移动到流多处理器（而不是实际在流多处理器上计算）。一个运算符是计算受限还是内存受限取决于其\textit{算术强度}，即每个内存访问操作（IO）执行多少浮点运算（FLOP）：</p><p><img alt="\begin{align}    \texttt{arithmetic intensity} = \frac{\texttt{FLOP}}{\texttt{IO}}\notag\end{align}" src="https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%7D++++%5Ctexttt%7Barithmetic+intensity%7D+%3D+%5Cfrac%7B%5Ctexttt%7BFLOP%7D%7D%7B%5Ctexttt%7BIO%7D%7D%5Cnotag%5Cend%7Balign%7D" /> </p><p>更高的并行性，更高的每个内存访问的FLOP，运算符更可能是计算受限的，我们更好地利用硬件。在给定的GPU上，运算符从内存受限变为计算受限的关键算术强度，即其FLOP / 内存带宽的比率。对于A100，它是：</p><p><img alt="\begin{align}    \texttt{A100 critical intensity} = \underset{\texttt{A100 bf16 FLOP}}{312\text{T flop per sec}}\;/ \underset{\texttt{A100 HBM Bandwidth}}{2\text{T byte per sec}} = 156\notag\end{align}" src="https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%7D++++%5Ctexttt%7BA100+critical+intensity%7D+%3D+%5Cunderset%7B%5Ctexttt%7BA100+bf16+FLOP%7D%7D%7B312%5Ctext%7BT+flop+per+sec%7D%7D%5C%3B%2F+%5Cunderset%7B%5Ctexttt%7BA100+HBM+Bandwidth%7D%7D%7B2%5Ctext%7BT+byte+per+sec%7D%7D+%3D+156%5Cnotag%5Cend%7Balign%7D" /> </p><p>对于transformer，并行性大约是我们向其中提供的标记数量，即批处理大小。这意味着，对于A100，当我们的批处理大小大于156个标记时，例如，在预填充阶段提示有50 K个标记时，我们是计算受限的，并充分利用A100的计算能力。当我们的批处理大小小于156个标记时，例如，在解码时我们只解码一个标记，我们是内存受限的，没有充分利用A100的计算能力。</p><h2>1.4 - 预填充</h2><p>现在我们分析在A100上预填充需要多长时间。由于预填充是计算受限的，即在A100上上下文长度超过156时，其理论峰值延迟为</p><p><img alt="\begin{align}    \texttt{theoretical peak latency} = \frac{\texttt{FLOP of prefilling}}{\texttt{FLOP per second of A100}}\notag\end{align}" src="https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%7D++++%5Ctexttt%7Btheoretical+peak+latency%7D+%3D+%5Cfrac%7B%5Ctexttt%7BFLOP+of+prefilling%7D%7D%7B%5Ctexttt%7BFLOP+per+second+of+A100%7D%7D%5Cnotag%5Cend%7Balign%7D" /> </p><p>For a prompt of 50K context length it is</p><p><img alt="\begin{align}\underset{\texttt{prefilling FLOP}}{4.33\text{P Flop}} &amp;= \underset{\texttt{seqlen}}{50 \text{K}} \times (2 \times \underset{\texttt{model param}}{34\text{B}} + 2 \times \underset{\texttt{layer}}{60} \times \underset{\texttt{seqlen}}{50\text{K}} \times\underset{\texttt{hidden}}{4096})\notag\\\underset{\texttt{prefilling latency}}{14.1 \;\text{seconds}} &amp;= \underset{\texttt{prefilling flop}}{4.33\text{P Flop}} /\; \underset{\texttt{A100 bf16}}{312\text{T Flop per sec}} \notag\end{align} " src="https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%7D%5Cunderset%7B%5Ctexttt%7Bprefilling+FLOP%7D%7D%7B4.33%5Ctext%7BP+Flop%7D%7D+%26%3D+%5Cunderset%7B%5Ctexttt%7Bseqlen%7D%7D%7B50+%5Ctext%7BK%7D%7D+%5Ctimes+%282+%5Ctimes+%5Cunderset%7B%5Ctexttt%7Bmodel+param%7D%7D%7B34%5Ctext%7BB%7D%7D+%2B+2+%5Ctimes+%5Cunderset%7B%5Ctexttt%7Blayer%7D%7D%7B60%7D+%5Ctimes+%5Cunderset%7B%5Ctexttt%7Bseqlen%7D%7D%7B50%5Ctext%7BK%7D%7D+%5Ctimes%5Cunderset%7B%5Ctexttt%7Bhidden%7D%7D%7B4096%7D%29%5Cnotag%5C%5C%5Cunderset%7B%5Ctexttt%7Bprefilling+latency%7D%7D%7B14.1+%5C%3B%5Ctext%7Bseconds%7D%7D+%26%3D+%5Cunderset%7B%5Ctexttt%7Bprefilling+flop%7D%7D%7B4.33%5Ctext%7BP+Flop%7D%7D+%2F%5C%3B+%5Cunderset%7B%5Ctexttt%7BA100+bf16%7D%7D%7B312%5Ctext%7BT+Flop+per+sec%7D%7D+%5Cnotag%5Cend%7Balign%7D+" /> </p><p>由于14.1秒是理论峰值，我们在图1中将其四舍五入为20秒以考虑实现开销。这意味着实际实现可能达到理论峰值性能的14.1/20≈ 70%，这对于在A100上进行cuda编程是一种常见经验。</p><p>如果上下文长度为4 而不是50 ，那么重复上述计算我们得到延迟为0.89秒。这里的差异是</p><p>4 的预填充延迟为0.89秒</p><p>50 的预填充延迟为14.1秒</p><p>13秒的差距，源自长上下文的额外FLOP，这是我们最终希望弥补的。</p><h2>1.5 - 解码</h2><p>现在我们分析解码需要多长时间。由于解码是内存受限的，即在A100上批处理大小小于156时，理论峰值延迟为</p><p>理论峰值延迟=内存访问字节数A100 HBM带宽理论峰值延迟=A100 HBM带宽内存访问字节数​</p><p><img alt="\begin{align}    \texttt{theoretical peak latency} = \frac{\texttt{bytes of memory access}}{\texttt{A100 HBM bandwidth}} \notag\end{align}" src="https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%7D++++%5Ctexttt%7Btheoretical+peak+latency%7D+%3D+%5Cfrac%7B%5Ctexttt%7Bbytes+of+memory+access%7D%7D%7B%5Ctexttt%7BA100+HBM+bandwidth%7D%7D+%5Cnotag%5Cend%7Balign%7D" /> </p><p>对于解码，一个前向传递意味着</p><p>内存访问字节数 == 模型权重 ++ KV缓存</p><p><img alt="\begin{align}    \texttt{bytes of memory access} = \texttt{model weights} + \texttt{KV Cache}\notag\end{align}" src="https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%7D++++%5Ctexttt%7Bbytes+of+memory+access%7D+%3D+%5Ctexttt%7Bmodel+weights%7D+%2B+%5Ctexttt%7BKV+Cache%7D%5Cnotag%5Cend%7Balign%7D" /> </p><p>我们假设平均模型生成一个屏幕标记（通常用户偏好生成长度约为一个屏幕），即约250个标记，那么峰值延迟为</p><p><img alt="\begin{align}    \underset{\texttt{one screen tokens}}{250} \times \underset{\texttt{model weight}}{(68\text{GB}} + \underset{\texttt{50K ctx KV cache}}{11\text{GB})}\;/\;\underset{\texttt{A100 HBM bandwidth}}{2\text{Tbytes per sec}} = \underset{\texttt{decoding latency}}{9.8\;\text{seconds}} \notag\end{align}" src="https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%7D++++%5Cunderset%7B%5Ctexttt%7Bone+screen+tokens%7D%7D%7B250%7D+%5Ctimes+%5Cunderset%7B%5Ctexttt%7Bmodel+weight%7D%7D%7B%2868%5Ctext%7BGB%7D%7D+%2B+%5Cunderset%7B%5Ctexttt%7B50K+ctx+KV+cache%7D%7D%7B11%5Ctext%7BGB%7D%29%7D%5C%3B%2F%5C%3B%5Cunderset%7B%5Ctexttt%7BA100+HBM+bandwidth%7D%7D%7B2%5Ctext%7BTbytes+per+sec%7D%7D+%3D+%5Cunderset%7B%5Ctexttt%7Bdecoding+latency%7D%7D%7B9.8%5C%3B%5Ctext%7Bseconds%7D%7D+%5Cnotag%5Cend%7Balign%7D" /> </p><p>由于9.8秒是理论峰值，在图1中我们将其四舍五入为12秒，以考虑实现开销。如果序列长度为4 K4 K，那么其对应的KV缓存仅为0.91GB，解码延迟降至8.5秒。然而，如果序列长度增加到200 K200 K，KV缓存变为44 GB44 GB，延迟增加到14秒。相对延迟增加与KV缓存和模型大小之间的相对大小有关，我们最终希望将其缩小。</p><h2>1.6 - 并发控制和上下文切换</h2><p>另一个重要考虑因素是，当KV缓存变大时，GPU HBM可以容纳的并发用户缓存数量为并发级别=HBM大小−模型权重KV缓存=KV缓存HBM大小−模型权重​</p><p>这意味着并发受HBM大小的限制。继续以我们的34B 50K模型示例为例，如果我们将其部署在一个80GB的A100上，我们只能为一个用户提供服务（图1）。但是，如果上下文为4 K，KV缓存约为1GB，我们可以同时为大约20个用户提供服务。</p><p>当第二个用户来询问关于长文档的问题时，为了为他们腾出KV缓存空间，我们需要进行上下文切换：将第一个用户的KV缓存卸载到CPU内存，并加载第二个用户的KV缓存（图1）。这会引起上下文切换开销：</p><p>上下文切换开销=用户1KV缓存+用户2KV缓存PCIE带宽=PCIE带宽用户1KV缓存+用户2KV缓存​</p><p>也就是说，上下文切换开销受PCIE带宽的限制，即GPU HBM与CPU DDR连接的速度有多快。假设我们使用每秒20G字节的PCIE gen 4，则50 K上下文的2个用户的上下文切换开销为：</p><p><img alt="\begin{align}    \underset{\texttt{context switching for 50K}}{1.1\;\text{seconds}} = \underset{\texttt{user 1 KV cache}}{(11\text{G bytes}} + \underset{\texttt{user 2 KV cache}}{11\text{G bytes})} / \;\underset{\texttt{PCIE bandwidth}}{{20\text{G bytes per sec}}}\notag\end{align}" src="https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%7D++++%5Cunderset%7B%5Ctexttt%7Bcontext+switching+for+50K%7D%7D%7B1.1%5C%3B%5Ctext%7Bseconds%7D%7D+%3D+%5Cunderset%7B%5Ctexttt%7Buser+1+KV+cache%7D%7D%7B%2811%5Ctext%7BG+bytes%7D%7D+%2B+%5Cunderset%7B%5Ctexttt%7Buser+2+KV+cache%7D%7D%7B11%5Ctext%7BG+bytes%7D%29%7D+%2F+%5C%3B%5Cunderset%7B%5Ctexttt%7BPCIE+bandwidth%7D%7D%7B%7B20%5Ctext%7BG+bytes+per+sec%7D%7D%7D%5Cnotag%5Cend%7Balign%7D" /> </p><p>在图1中，我们将1.1秒四舍五入为2秒，以考虑工程开销。如前所述，在我们的设置中，我们可以为4 K上下文长度的20个用户提供服务，而无需进行上下文切换，因为HBM足以容纳他们的KV缓存。如果我们想将我们的50 K并发性增加到20，那么随着并发性的增加，整体上下文切换开销也会增加：</p><p><img alt="\begin{align}    \underset{\texttt{Context switching for 20 users for 50K}}{22\;\text{seconds}} = \underset{\texttt{concurrency}}{20 \;\texttt{users}} \times \underset{\texttt{context switching between 2 users}}{1.1\;\text{seconds}} \notag\end{align}" src="https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%7D++++%5Cunderset%7B%5Ctexttt%7BContext+switching+for+20+users+for+50K%7D%7D%7B22%5C%3B%5Ctext%7Bseconds%7D%7D+%3D+%5Cunderset%7B%5Ctexttt%7Bconcurrency%7D%7D%7B20+%5C%3B%5Ctexttt%7Busers%7D%7D+%5Ctimes+%5Cunderset%7B%5Ctexttt%7Bcontext+switching+between+2+users%7D%7D%7B1.1%5C%3B%5Ctext%7Bseconds%7D%7D+%5Cnotag%5Cend%7Balign%7D" /> </p><p>20个用户的50K上下文的上下文切换在2个用户之间切换</p><p>这22秒的开销在4 K上下文范围内不存在，但在长上下文范围内会变得棘手。</p><h2>1.7 - 到目前为止的总结</h2><p>我们已经讨论了使用34 B模型50 K上下文作为运行示例部署长上下文transformers时的大部分细节。我们看到，整体性能，以单位时间内用户交互会话数量来衡量，可以分解为四个重要指标：</p><ul><li>由GPU flops限制的预填充延迟；</li><li>由HBM带宽限制的解码延迟；</li><li>由HBM大小限制的并发级别。</li><li>GPU-CPU连接带宽限制的上下文切换开销，即PCIe。</li></ul><p>在接下来的章节中，我们将讨论这些指标如何随着上下文长度和硬件架构的变化而变化，并最终确定瓶颈追溯到KV缓存的大小。</p><h2>2 - 强烈影响性能指标的因素</h2><p>我们从两个基本因素开始：上下文长度和硬件架构。当从4 K增加到50 K时，我们展示了四个指标（预填充、解码、并发和上下文切换）随不同速率（线性、反比和二次）变化。我们进一步表明张量并行性提高了并发性、预填充和解码，但并不改善上下文切换。总体性能如下所示：</p><figure><img class="origin_image zh-lightbox-thumb" src="https://pic2.zhimg.com/v2-6bb9c922728cb06567ff604537ecf44d_1440w.jpg" /></figure><p class="ztext-empty-paragraph"><br /></p><p>图2. 第一行：上下文长度如何改变四个关键性能指标。将上下文长度从4 K增加到50 K会反比减少并发性，二次增加预填充延迟，线性增加上下文切换开销，稍微（但也是线性地）增加解码延迟。第二行：不同硬件代的影响关键性能指标。并发性由并发用户数来衡量。预填充、解码和上下文切换延迟以秒为单位。</p><h2>2.1 - 上下文长度</h2><p>如图2的第一行所示，我们使用在前一节讨论的方程中计算了我们的Yi 34B示例运行的上下文长度从4 K到50 K的四个指标的理论峰值性能。我们观察到：</p><ul><li>随着更长的上下文长度，并发性反比减少；</li><li>随着更长的上下文长度，预填充延迟二次增加。</li><li>相比之下，解码延迟和上下文切换开销只是线性增加随着更长的上下文长度，解码延迟是最不受影响的因素，因为50 K上下文KV缓存仍然相对较小于模型参数（11GB对比68GB）。</li></ul><p>总体而言，并发性和预填充是两个受影响最严重的因素。</p><h2>2.2 - 硬件架构</h2><p>我们能否通过简单使用更先进的硬件来提高性能？在图2的第二行中，我们展示了随着硬件进步性能改善趋势。我们观察到：</p><ul><li>并发性随着HBM大小的增加而线性增加；</li><li>随着设备从4090升级到A100再到H100时，预填充延迟随着FLOPS的增加而反比减少；</li><li>随着内存带宽的增加，解码延迟随之反比减少。</li><li>上下文切换开销随着增加的PCIE带宽而逆向减少。请注意，我们使用的数字基于2024年5月的最新进展，即使我们使用最新的硬件，50K美元和4K美元之间的成本差距也没有缩小。换句话说，我们不能指望硬件进步来降低为长上下文模型提供服务的成本，我们必须进行算法创新。</li></ul><h2>2.3 - 使用张量并行性的多GPU</h2><p>正如Kipply的博客所讨论的，利用多个设备加速推理，通信开销可以忽略不计。一般来说：</p><ul><li>将设备数量线性增加到2、4和8会引入更多的HBM空间，从而线性增加并发性。</li><li>由于我们在多个设备上均匀分配计算，因此预填充和解码延迟也会随GPU数量的逆向减少。</li><li>然而，张量并行性无法减少上下文切换开销，因为DDR到HBM之间的PCIE带宽被所有设备共享。</li></ul><h2>3 - 可压缩性分析和现有工作</h2><p>到目前为止，我们在比较50K美元和4K美元时有以下重要观察：</p><ul><li>为了预填充长输入并生成KV缓存，预填充延迟从0.89增加到14.1秒；</li><li>由于大型KV缓存驻留在GPU内存中，并发性从约20减少到1；</li><li>在解码过程中，重复加载KV缓存会将延迟从8.5增加到9.8秒；</li><li>大型KV缓存引起昂贵的上下文切换开销，对于20个用户，额外需要约22秒（方程式17）。这四个因素共同导致了端到端基于会话的吞吐量方面的显着成本。</li></ul><p>如果我们能够无损减少预填充时间并压缩KV缓存，我们可能会显著降低为长上下文模型提供服务的成本。我们的最终目标是使部署1M上下文的成本像4K美元一样便宜，而4K美元的标记约为1GB的KV缓存。然后，我们的观察指向一个关键的研究问题：</p><ul><li>如何有效地将1M标记的KV缓存压缩到1G字节而不丢失信息？</li></ul><p>我们首先注意到，没有任何压缩，将1M标记存储为字节只需要约3-10MB的磁盘存储（取决于分词器词汇表的大小），因此1GB足以存储输入标记的全部信息。问题在于如何使它们的压缩表示可供大型transformer使用。</p><p>从业者通常会在各种长上下文任务上测试模型，以检查压缩是否有损失，其中“草堆中的针”测试要求模型精确地检索放置在长上下文任意位置的给定信息，作为一个入门障碍：如果一个模型无法通过这个测试，我们就不相信它能够完成更难的任务。不幸的是，似乎两个重要的模型系列，状态空间模型（例如Mamba）和线性注意力（例如LongT5），无法通过针测试，因此我们不将它们纳入我们的讨论范围。我们最近的工作表明，存在一组特殊的注意力头负责从上下文中检索重要信息。它们的发现表明，至少对于某些层和某些头部，应保留对大多数输入标记的全注意力 - 这些注意力头可能不太可压缩。下面我们讨论KV缓存的可压缩性，从其四个维度：层、头、标记和隐藏，以及现有工作如何改进长上下文推理。C：并发，P：预填充，D：解码，S：上下文切换：</p><table><tbody><tr><th></th><th>描述</th><th>改进</th><th>Needle(大海捞针)？</th></tr><tr><td>层</td></tr><tr><td>CALM</td><td>基于估计置信度的早期退出</td><td>C|P|D|S</td><td>？</td></tr><tr><td>Colt5</td><td>有条件地减少某些层上的计算</td><td>C|P|D|S</td><td>？</td></tr><tr><td>LayerSkip</td><td>跳过一些层然后验证</td><td>C|P|D|S</td><td>？</td></tr><tr><td>YOCO</td><td>仅使用一个全局KV缓存</td><td>C|P|D|S</td><td>✓</td></tr><tr><td>头</td></tr><tr><td>GQA</td><td>为一组头部重复使用KV缓存</td><td>C|D|S</td><td>✓</td></tr><tr><td>检索头</td><td>移除非检索头</td><td>C|D|S</td><td>✓</td></tr><tr><td>MLA</td><td>使用潜在头</td><td>C|P|D|S</td><td>✓</td></tr><tr><td>标记</td><td></td><td></td><td></td></tr><tr><td>H2O</td><td>在预填充后丢弃不重要的标记</td><td>C|D|S</td><td>？</td></tr><tr><td>FastGen</td><td>在预填充期间识别重要的标记</td><td>C|D|S</td><td>？</td></tr><tr><td>DMC</td><td>动态合并标记</td><td>C|P|D|S</td><td>？</td></tr><tr><td>SnapKV</td><td>根据用户问题识别重要的标记</td><td>D</td><td>✓</td></tr><tr><td>TriForce</td><td>长上下文的推测解码</td><td>D</td><td>✓</td></tr><tr><td>隐藏</td></tr><tr><td>KIVI</td><td>KV缓存量化</td><td>C|D|S</td><td>？</td></tr><tr><td>WKVQuant</td><td>权重和KV缓存量化</td><td>C|D|S</td><td>？</td></tr></tbody></table><p class="ztext-empty-paragraph"><br /></p><h2>2.1 - 层</h2><p>对于层维度，基本假设是一些任务可能不需要完整深度的计算。在预填充过程中跳过一些层可能对所有四个指标都有益，因为这同时减少了预填充的浮点运算次数和KV缓存的大小。实际上，层维度可能会从像Memorizing Transfomers和YOCO这样的现有作品的结果中被根本性地减少，对于长上下文任务可能只需要保留一个层KV缓存，这是一个1/60的压缩比率。</p><h2>2.2 - 头</h2><p>对于头维度，基本假设是一些头部专门用于检索和长上下文相关能力，因此可能保留检索头部并修剪其他头部。需要注意的是头部修剪通常发生在预填充之后，这意味着它们只会改善解码、并发性和上下文切换，但预填充仍然是昂贵的。总的来说，在头维度上存在非常高的稀疏性，头的数量可能会被根本性地减少到非常小的数字，例如，我们展示最强检索头的数量少于20个，这可能导致一个20/1024的压缩比率。</p><h2>2.3 - 标记</h2><p>对于标记维度，基本假设是如果可以从上下文中推断出标记的信息，我们可以通过删除或与邻近标记合并来压缩该标记。在标记维度上的大部分压缩并不会显著改善预填充，但它们通常会改善并发性、解码和上下文切换。目前看来，标记维度可能不像层和头维度那样稀疏，因为大多数标记必须保留以进行精确检索。我们尚未看到任何工作展示标记维度上超过50%的压缩比率的潜力。</p><h2>2.4 - 隐藏</h2><p>除了量化之外，对于进一步减少维度的工作并不多，这可能是因为隐藏大小已经是128，太小了无法进一步减少。然而，尝试在KV缓存上应用像LoRA这样的维度减少仍然值得，特别是考虑到最近来自DeepSeek V2的进展，该进展引入了类似LoRA的想法，有效地减少了KV头的大小。</p><p>这里的一个重要观察是许多现有作品可能只强调问题的一个方面。例如，TriForce [18]只考虑使用推测解码的解码延迟。它没有使KV缓存变小，甚至从草案模型中增加了GPU内存消耗的权衡。许多现有作品也是正交的，因此它们从不同方面的优势可能会汇聚力量。例如，如果我们将KV缓存减少到只有1层或10个头部，并且仅保留50%的标记，我们将获得约1000倍的性能提升。这自然引出了一个研究呼吁：</p><ul><li>我们是否可以将现有努力整合到一个端到端系统中，并推动全栈优化？</li></ul><h2>3 - 结论：朝着端到端系统的全栈优化迈进</h2><p>在本文中，我们对部署长上下文的挑战进行了详细分析。</p>
]]></content:encoded>
<pubDate>Sat, 11 May 2024 13:01:06 GMT</pubDate>
<pubDate>Sat, 11 May 2024 13:01:06 GMT</pubDate>
</item>
<item>
<title>强化学徒发布了想法: 其实反过来看，如果自媒体宣传的足够猛，可能会带来更好的生源(毕竟多数高中生和家长可能并不能看到负面影响)[飙泪笑] <br> <a href="https...</title>
<link>https://www.zhihu.com/pin/1772440774988619777</link>
<guid>https://www.zhihu.com/pin/1772440774988619777</guid>
<content:encoded><![CDATA[
<div> 自媒体、宣传、生源、高中生、家长
<br>
自媒体宣传足够猛可能带来更好的生源。多数高中生和家长可能无法看到负面影响，从而受到影响。因此，加强自媒体宣传，可能是吸引更多生源的有效策略。 <div>
<p>其实反过来看，如果自媒体宣传的足够猛，可能会带来更好的生源(毕竟多数高中生和家长可能并不能看到负面影响)[飙泪笑] <br /> <a class="internal" href="https://www.zhihu.com/question/655550363/answer/3494129786">中科大被美商务部列入实体清单，此前 18 所中国高校已在内，对留学影响有多大？会引发高考生报考热情吗？</a></p>
]]></content:encoded>
<pubDate>Fri, 10 May 2024 17:40:11 GMT</pubDate>
<pubDate>Fri, 10 May 2024 17:40:11 GMT</pubDate>
</item>
<item>
<title>强化学徒发布了想法: 鉴于最近关于自动驾驶事故频发，普通人有权知道危险源在哪儿。</title>
<link>https://www.zhihu.com/pin/1771853038028156928</link>
<guid>https://www.zhihu.com/pin/1771853038028156928</guid>
<content:encoded><![CDATA[

<p>鉴于最近关于自动驾驶事故频发，普通人有权知道危险源在哪儿。</p><p><img src="https://pic4.zhimg.com/100/v2-04073919aa9828094e2f7be1e3e70e5f_720w.jpg" /></p>
]]></content:encoded>
<pubDate>Thu, 09 May 2024 02:44:43 GMT</pubDate>
<pubDate>Thu, 09 May 2024 02:44:43 GMT</pubDate>
</item>
<item>
<title>强化学徒赞同了回答: 如何看待 DeepSeek 发布的 MoE 大模型 DeepSeek-V2？</title>
<link>https://www.zhihu.com/question/655172528/answer/3490374123</link>
<guid>https://www.zhihu.com/question/655172528/answer/3490374123</guid>
<content:encoded><![CDATA[

<p>很少见在框架上做改动的工作了，只能说不愧是幻方</p><p>说一下创新点，核心在低秩降维。传统的attention是直接将x映射到qkv的head dim，这里先将x映射到latent c，然后再通过c映射回到qkv的head dim</p><p>这么做有个什么好处呢？这种低秩结构很显然的好处就是计算量小了，原来是d×d的复杂度，现在变成了2×d×c，自然小了很多。</p><p>但这还只是一个附加的好处。这么做的真正好处在于kv cache，原先的kv cache需要存下完整的kv向量，这番操作之后，只需要保存低维的c，在用到的时候再通过两个权重矩阵映射回k，v就可以，这大大的减少了kv cache的空间占用</p><p>（而且这个思路似乎确实没有看到过类似的工作，直接cache x，而不是k和v，这样不就可以节省一半的空间？虽然牺牲一点计算量。有知道的也可以说一下）</p><p>另外一个也算比较创新的地方，把原先的head dim拆成两部分，只在其中一部分进行位置编码，另一部分不动。细节在后面补充</p><p>moe分了共享和专用，细粒度moe，也算比较新了</p><hr /><p>补充一个点，MLA是MHA的变种，不是现在比较广泛的xxQA，所以在理论上是强于MQA，GQA模型的</p><hr /><p>来解释一下为什么要在rope的时候对head dim拆分</p><figure><img class="origin_image zh-lightbox-thumb lazy" src="https://pic3.zhimg.com/v2-bae4e56d2d33b2beabcab99bf05324ee_1440w.jpg" /></figure><p>然后我们知道attention可以写成下面这种形式</p><p><img alt="attn=softmax(q k^{T})=softmax(xW^{q}(W^{k})^{T}x^{T})=softma(xW^{qk}x^T)" src="https://www.zhihu.com/equation?tex=attn%3Dsoftmax%28q+k%5E%7BT%7D%29%3Dsoftmax%28xW%5E%7Bq%7D%28W%5E%7Bk%7D%29%5E%7BT%7Dx%5E%7BT%7D%29%3Dsoftma%28xW%5E%7Bqk%7Dx%5ET%29" /> </p><p>其中W_qk是一个d×d的矩阵，因为在MLA中，cache的是c，通过c实时计算并利用k和v，所以能够进行这种操作。在inference阶段，这番操作下来可以提高的效率，还能减少参数占用</p><p>这时就面临一个问题，我对k加上的rope在这里就无了，因为k=xW_k，包含了W_k</p><p>于是这里就提出一个绝妙的想法，将k的dim拆分，取一大部分作为c，一小部分作为k_r，维度为r，由于rope的良好性质（赞美苏神），可以通过仅对k_r进行位置编码的方式将相对位置信息包含在内，这样在cache的时候额外将k_r存起来，然后在计算attn的时候分开计算，于是</p><p><img alt="attn=sm(q_ck_c^T+q_rk_r^T)=sm(c_qW_{qk}c^T+c_qW_{qr}k_r^T)" src="https://www.zhihu.com/equation?tex=attn%3Dsm%28q_ck_c%5ET%2Bq_rk_r%5ET%29%3Dsm%28c_qW_%7Bqk%7Dc%5ET%2Bc_qW_%7Bqr%7Dk_r%5ET%29" /> </p><p>其中W_qr是将c_q映射到到r的矩阵，c是cache存下的latent，k_r是额外存下的含有位置编码的key，W_qk是将W_q和W_k融合得到的。因为c_q是h通过降维得到的，c_q=hW_dq，W_dq是将h降维为c_q的参数矩阵，所以又可以写成</p><p><img alt="attn=sm(hWc^T+hW_rk_r^T)" src="https://www.zhihu.com/equation?tex=attn%3Dsm%28hWc%5ET%2BhW_rk_r%5ET%29" /> </p><p>其中，W是将W_dq, W_q, W_k三者融合在一起得到的矩阵，W_r是将W_dq, W_qr融合在一起得到的矩阵。这番操作之下，我们只需要cache一个c，一个k_r，就能同时保证：节省cache的空间，加速inference的速度，以及位置编码的质量</p><p>然后，这里就要提出一个问题：在MLA的attention中，传统意义上的W_q, W_k, W_v全部成了低秩矩阵的形式，那么低秩矩阵对满秩矩阵的替换，是否会影响到注意力部分的性能呢？在Deepseek的这份报告里虽然没有着重去提及，但效果上看起来是没有的，我相信Deepseek做出这种大胆的替换，一定是基于了一些前期实验结论的。换言之，这说明了一个可能的结论——attention模块内的权重矩阵具有较大的低秩特性，因而能够进行这种替换，attention中的满秩权重矩阵很可能就是冗余的</p><hr /><p>更新：感谢评论区指出已有对kv cache低秩压缩的工作，这里搜索了一下论文，找到一个相关工作GEAR</p><a class=" wrap external" href="https://arxiv.org/pdf/2403.05527" rel="nofollow noreferrer" target="_blank">2403.05527 (arxiv.org)</a><p>这个工作的做法是采用一个稀疏矩阵+量化+低秩分解对kv进行压缩，然后cache压缩后的kv。文中做到的压缩比率为3倍，并且在一众压缩方法中取得了sota</p><p>不过对比MLA中，减小了相比于传统MHA <img alt="\frac{9}{4n_{head}}" src="https://www.zhihu.com/equation?tex=%5Cfrac%7B9%7D%7B4n_%7Bhead%7D%7D" /> 倍的夸张的cache的空间复杂度面前，这种做法顿时就没有那么香了。只要Attention head大于7，MLA节省的cache空间就已经超过GEAR了，何况现在的模型动不动就几十上百个head。估计这也是Deepseek自信1元一百万tokens的来源吧</p><hr /><p>更新：这几天都在跟MLA打交道。研究了一下HF社区里的代码之后基本确定，这跟论文和跑在云服务上的代码不是一版。论文里面提到的merge weight，以及cache latent都是没在开源代码里面体现出来的，毕竟merge了之后要重写forward，cache也要重写，这么多优化做完都可以直接搬上服务器直接当deepseek v2用了，开源社区要紧的是能跑和兼容，肯定没精力把这些顾上。</p><p>其实这样想的话，其他的开源代码也未必就是他们自己工业上使用的代码，真的上服务器，肯定都会先merge weight，如果memory bound比较多的话估计也私底下把kv cache换成了h cache</p>
]]></content:encoded>
<pubDate>Wed, 08 May 2024 06:03:16 GMT</pubDate>
<pubDate>Wed, 08 May 2024 06:03:16 GMT</pubDate>
</item>
<item>
<title>强化学徒回答了问题: 如何看待 DeepSeek 发布的 MoE 大模型 DeepSeek-V2？</title>
<link>https://www.zhihu.com/question/655172528/answer/3490692813</link>
<guid>https://www.zhihu.com/question/655172528/answer/3490692813</guid>
<content:encoded><![CDATA[

<p>下次在没有实测之前，不能随便暴论了～</p><p>目前中文的指令跟随能力还有待提升～</p><figure><img class="origin_image zh-lightbox-thumb lazy" src="https://pic4.zhimg.com/v2-41b8ff5ec8628a3cdefae8046da9752b_1440w.jpg" /></figure><p></p>
]]></content:encoded>
<pubDate>Tue, 07 May 2024 12:52:45 GMT</pubDate>
<pubDate>Tue, 07 May 2024 12:52:45 GMT</pubDate>
</item>
<item>
<title>强化学徒发布了想法: 羡慕高飞老师的学生~</title>
<link>https://www.zhihu.com/pin/1771180099343409154</link>
<guid>https://www.zhihu.com/pin/1771180099343409154</guid>
<content:encoded><![CDATA[

<p>羡慕高飞老师的学生~</p><p><img src="https://pic2.zhimg.com/100/v2-65ed969e857723626e13603b1bf3b67d_720w.jpg" /></p>
]]></content:encoded>
<pubDate>Tue, 07 May 2024 06:10:42 GMT</pubDate>
<pubDate>Tue, 07 May 2024 06:10:42 GMT</pubDate>
</item>
<item>
<title>强化学徒发表了文章: 学术版gpt已经支持自带术语库翻译-教程</title>
<link>https://zhuanlan.zhihu.com/p/696279747</link>
<guid>https://zhuanlan.zhihu.com/p/696279747</guid>
<content:encoded><![CDATA[

<p>大家在翻译论文的时候，应该会经常遇到一些突兀的翻译，如果用端到端的直接翻译，手动修改替换就比较麻烦，所以我简单优化了一下术语库的支持。</p><p>效果非常的nice，arxiv原生翻译和PDF2PDF，以及markdown翻译都是可以的。 术语库的输入格式为json，你可以把你们领域一些比较重要的术语塞进去，比如我们机器人+RL+LLM领域，就有下面的一些常见术语：</p><blockquote>{  "agent": "智能体",  "environment": "环境",  "state": "状态",  "action": "动作",  "reward": "奖励",  "policy": "策略",  "value": "价值",  "model": "模型",  "exploration": "探索",  "exploitation": "利用",  "model-free": "无模型",  "model-based": "有模型",  "value-based": "基于价值",  "policy-based": "基于策略",  "transition": "转移元组",  "buffer": "经验池",  "replay": "回放",  "experience": "经验",  "episode": "回合",  "epoch": "轮次",  "zero-shot": "零样本",  "one-shot": "一次样本",  "few-shot": "少样本",  "long horizon": "长视角",  "demonstration": "示教",  "teleoperation": "遥操作",  "simulation": "仿真",      "token": "标记",      "transformer": "transformer",  "actor-critic": "actor-critic",  "off-policy": "off-policy",  "on-policy": "on-policy"  }</blockquote><p>值得注意的是，有一些术语不需要翻译，你映射为原单词即可，LLM也是能听懂的。</p><p>具体操作为：</p><figure><img class="origin_image zh-lightbox-thumb" src="https://pic3.zhimg.com/v2-a979c098eaee6ace5d1fb75b55e0944e_1440w.jpg" /></figure><p>比如：</p><figure><img class="origin_image zh-lightbox-thumb" src="https://pic4.zhimg.com/v2-95ab067325c3d759da9d1d1ad8d74f87_1440w.jpg" /><figcaption>如果是非arxiv的翻译，则不需要下面那段中文。</figcaption></figure><p>术语库到底怎么来？</p><p>这得看你们的专业积累了~</p><p>学术版gpt的免费网址：<a class=" wrap external" href="https://academic.chatwithpaper.org/" rel="nofollow noreferrer" target="_blank">GPT 学术优化</a></p><p>如果有反馈，欢迎加群：816116844</p>
]]></content:encoded>
<pubDate>Tue, 07 May 2024 00:55:22 GMT</pubDate>
<pubDate>Tue, 07 May 2024 00:55:22 GMT</pubDate>
</item>
<item>
<title>强化学徒发表了文章: 2024年AI领域最值得关注的博主和一手信息源盘点</title>
<link>https://zhuanlan.zhihu.com/p/682110383</link>
<guid>https://zhuanlan.zhihu.com/p/682110383</guid>
<content:encoded><![CDATA[

<p>2024.03.19：更新。新增7个B站up主，删掉一个微信公众号，</p><hr /><p>读博五年积累的信息获取渠道，分享给大家。</p><p>起因是昨天刷到一个【2024拒绝信息差！AI领域最值得关注的博主，优质信息良心推荐-哔哩哔哩】 <a class=" external" href="https://b23.tv/QhYLyDK" rel="nofollow noreferrer" target="_blank"><span class="invisible">https://</span><span class="visible">b23.tv/QhYLyDK</span><span class="invisible"></span></a></p><p>感觉，我也可以做一个类似的，分享给大家之后，可以起到一个同步信息源的作用，避免信息差和信息茧房。</p><p>由于我的时间有限，并且大家的注意力有限，我接下来，用最简单的关键词和一句话点评，供大家参考。</p><p>结构分为：知乎博主，B站up主，公众号，推特大V，一些垂直类AI网站。</p><hr /><p><b>点评纯主观片面，如果我的点评不对，欢迎指正，立刻修改！</b></p><hr /><h3>知乎博主：</h3><p>这里我直接根据我的关注列表，按照我的关注顺序介绍。大家找到感兴趣的博主之后，点开他们的动态，同样可以一挖挖一窝，哈哈。</p><h3>泛AI领域：</h3><ol><li><a class="internal" href="https://www.zhihu.com/people/bopengbopeng">PENG Bo</a>：rwkv作者，分享内容主要是rwkv的进展，暴论比较多，更新频率正常</li><li><a class="internal" href="https://www.zhihu.com/org/huggingface">Hugging Face</a>：Huggingface face官方号，会分享一些基础技术贴，工作日更新频率比较高。</li><li><a class="internal" href="https://www.zhihu.com/people/ding-xiao-yi-93">丁霄汉</a>：清华博士，现在腾讯AI lab。主要分享一些关于学术写作、审稿、cv、AI圈八卦和暴论。阅读起来比较开心。</li><li><a class="internal" href="https://www.zhihu.com/people/wan-shang-zhu-ce-de">毛航宇</a>：北大博士，前华为诺亚，现商汤。之前做MARL，现在弃坑，去做LLM Agent了（哈哈，本质都是多智能体），主要分享这两个方向的学术进展，个人见解，以及相关八卦。</li><li><a class="internal" href="https://www.zhihu.com/people/zhao-ytc">电光幻影炼金术</a>：上交博士，大佬的方向很杂，我刷了一圈都定位到具体专业。主要分享全领域学术进展、读研读博教程（包括写作、投稿、审稿、师生关系等），更新频率比较高。</li><li><a class="internal" href="https://www.zhihu.com/people/jackgethome">HeptaAI</a>：加州伯克利。表征学习 &amp; 强化学习。主要分享一些偏硬核的强化、LLM相关的帖子，一些个人点评，个人学习经历，喜欢硬核点的，比较值得看。</li><li><a class="internal" href="https://www.zhihu.com/people/who-u">何枝</a>：电子科大，现在字节。大佬是分享RLHF教程和代码讲解火出圈的，做相关工作的可以关注一下。不过在字节工作，应该是比较难输出了。</li><li><a class="internal" href="https://www.zhihu.com/people/xin-xi-men-xia-de-pao-gou">信息门下跑狗</a>：北大。跑姐不用多说，重拳出击学术造假，但最近更新频率也下降了。</li><li><a class="internal" href="https://www.zhihu.com/people/su-jian-lin-22">苏剑林</a>：苏佬去年从追一去月之暗面了。玩知乎的，苏佬应该不用多介绍了。虽然大本营在他自己的科学空间，后面也会介绍，但知乎刷起来会更方便一些。唯一难受的是，苏佬很多帖子都需要一定的数理基础才能看懂，我基本上都是收藏吃灰。</li><li><a class="internal" href="https://www.zhihu.com/people/wang-xiao-wei-64-66">王小惟 Weixun</a>：天大博士，现在网易伏羲。传统多智能体强化、LLM（RLHF）、游戏AI。做强化的关注就完事儿了。</li><li><a class="internal" href="https://www.zhihu.com/people/jiayi-weng">Trinkle</a>：清华，现在openai做RLHF。苹果哥也不用多介绍了，天授作者，chatgpt第一版RLHF他和同事炼的，偶尔会分享一些相关信息，关注就完事儿了。</li><li><a class="internal" href="https://www.zhihu.com/people/eyounx">俞扬</a>：南大教授。坚持offline RL落地，最近一年经常输出RL和RLHF的个人“暴论”，值得大家关注。</li><li><a class="internal" href="https://www.zhihu.com/people/tian-yuan-dong">田渊栋</a>：Meta FAIR研究院研究员，CMU机器人博士。之前做MARL，做长文本小说生成，以及现在做LLM，大佬非常强，在知乎的干货输出也很多，直接关注就行。</li><li><a class="internal" href="https://www.zhihu.com/people/rainstorm-53">曹越</a>：清华博士，<a class="internal" href="https://www.zhihu.com/question/492057377/answer/2169776709">swin transformer</a>作者。做cv，视频这块的内容可以关注一下大佬，主要分享一些学术进展和学术八卦。</li><li><a class="internal" href="https://www.zhihu.com/people/youngfish42">白小鱼</a>：上交。联邦学习相关干货知识分享，以及推荐各种LLM相关讯息。</li><li><a class="internal" href="https://www.zhihu.com/people/splitter">贱贱</a>：不用多介绍了，虽然我不是很认可他的观点，但学术八卦还是得看他，比如这次薛鹏教授的瓜。</li><li><a class="internal" href="https://www.zhihu.com/people/wang-feng-98-82">王峰</a>：TuSimple。看的不是很多，主要是自动驾驶相关的信息。</li><li><a class="internal" href="https://www.zhihu.com/people/dong-lin-zhong-sheng-76">东林钟声</a>：华科博士。博士方向是RL+灵巧手。现在主要研究LLM+灵巧手。大佬的干货比例和更新频率都比较好。</li><li><a class="internal" href="https://www.zhihu.com/people/sikila">王鹏程</a>：中科大。博主的想法会分享最新arxiv论文的图文介绍，刷起来很舒服。建议关注。</li><li><a class="internal" href="https://www.zhihu.com/people/li-bo-jie">李博杰</a>：科大博士，前华为天少。师兄的知乎分享频率非常高，质量同样高，长文干货贴+脑洞+个人见解。领域几乎包含全AI领域，值得大家关注。</li><li><a class="internal" href="https://www.zhihu.com/people/xie-ling-xi">谢凌曦</a>：清华，华为。盘古天气预报的作者，还有其他的就不介绍了，AI领域关注就行了。</li><li><a class="internal" href="https://www.zhihu.com/people/rumor-lee">李rumor</a>：北航，现在美团做RLHF。其实公众号是她的主战场，会分享一些有趣的AI知识。但感觉在美团做RLHF，已经占用了她太多的时间了，更新频率没那么高了。</li><li><a class="internal" href="https://www.zhihu.com/people/soulteary">苏洋</a>：大佬的经历一长串，没法介绍，哈哈。泛AI领域的资讯，关注就行了。</li><li><a class="internal" href="https://www.zhihu.com/people/ai--53-32">AI 小舟哥</a>：Huggingface 的大佬。知乎的分享好像不算多，但微信朋友圈的资讯都是一手最新的，关注我，我会分享最好的，哈哈。</li><li><a class="internal" href="https://www.zhihu.com/people/huangzhe">桔了个仔</a>：AI领域大佬答主了。泛AI领域的咨询。</li><li><a class="internal" href="https://www.zhihu.com/people/show-me-ai">ShowMeAI</a>：乔sir维护的AI日报。一个AI信息的整合平台。</li><li><a class="internal" href="https://www.zhihu.com/people/albert-chen-4">北方的郎</a>：LLM相关的知识分享和点评。</li><li><a class="internal" href="https://www.zhihu.com/people/mu-yao-12-34">穆尧</a>：港大博士生。大佬主要是做LLM，Robot（具身智能），diffusion Policy相关的内容。</li><li><a class="internal" href="https://www.zhihu.com/people/LiuCongNLP">刘聪NLP</a>：《ChatGPT原理与实战》作者。LLM的学术和行业信息。</li><li><a class="internal" href="https://www.zhihu.com/people/hai-tan-shang-chong-hua-47">OpenLLMAI</a>：浙大硕士。OpenRLHF作者，之前主要分享一些LLM相关的知识。现在输出较少，但转发评论比较多。</li><li><a class="internal" href="https://www.zhihu.com/people/zi-qi-dong-lai-1">紫气东来</a>: 上交硕士。干货很多，llm论文和项目原创分享。更新频率高。值得关注。</li><li><a class="internal" href="https://www.zhihu.com/people/zhang-jun-lin-76">张俊林</a>：新浪微博新技术研发负责人，知乎深度学习优秀答主。大佬从GPT2.0就开始在知乎分享LLM相关内容了，主要分享一些深度的思考（太硬核了，我之前一篇都没读进去，哈哈）。</li><li><a class="internal" href="https://www.zhihu.com/people/loveQt">段小草</a>：西电。主要分享一些最新的LLM行业产品分析和干货测评，大佬手速非常快。</li><li><a class="internal" href="https://www.zhihu.com/people/xiaohuzc">小小将</a>：泛AI领域答主，一时半会儿我总结不了。</li><li><a class="internal" href="https://www.zhihu.com/people/flood-sung">Flood Sung</a>：月之暗面。大佬之前做机器人、强化学习，现在在月之暗面做kemichat。现在大段干货分享频率没有之前那么高了，但点评比较多。</li></ol><h3>强化领域：</h3><ol><li><a class="internal" href="https://www.zhihu.com/people/hao-jian-ye-tian-jin-da-xue-51">强化学习实验室</a>：天大郝建业老师组的知识分享平台。强化学习领域必须关注！他们分享的论文都比较重要，他们写的帖子也非常深入浅出。输出频率不算高，但比较稳定。</li><li><a class="internal" href="https://www.zhihu.com/people/zhao-jian-2-54">赵鉴</a>：科大博士，现在南栖仙策带RL团队。主要分享游戏+RL的知识、RL的就业技巧，以及就业现状，观点往往一针见血。</li><li><a class="internal" href="https://www.zhihu.com/people/baichenjia">白辰甲</a>：哈工大博士，现在上海AIlab。之前主要做强化，不少教程贴，现在主要做四足和人形。博士毕业之后，知乎大段干货分享频率较低。</li><li><a class="internal" href="https://www.zhihu.com/people/zhang-chu-heng">张海抱</a>：清华博士。主要分享一些RL的最新论文解读。现在博士毕业之后，知乎号的分享频率要下降不少。</li><li><a class="internal" href="https://www.zhihu.com/people/ceng-yi-yan-8">曾伊言</a>：小雅的作者。RL领域的同学关注就行了。尽管更新频率已经下降很多了。</li></ol><h3>机器人领域：</h3><ol><li><a class="internal" href="https://www.zhihu.com/people/dong-lin-zhong-sheng-76">东林钟声</a>：华科博士。博士方向是RL+灵巧手。现在主要研究LLM+灵巧手。大佬的干货比例和更新频率都比较好。</li><li><a class="internal" href="https://www.zhihu.com/people/yyss2037">YY硕</a>：卡内基梅隆大学博士。机器人领域的优质答主，关注就完事儿了。输出频率不高，但每个帖子都值得认真阅读。</li><li><a class="internal" href="https://www.zhihu.com/people/li-miao-8-1">李淼robot</a>：EPFL( 瑞士洛桑联邦理工)·博士，现在武汉某高校（隐约记得是武大）。李老师是机器人领域的优质答主，之前有比较多的教程贴，现在更新频率较低。只能说工作之后，大家水知乎的时间越来越少了</li></ol><h3>B站up:</h3><ol><li><a class=" wrap external" href="https://space.bilibili.com/5760446/" rel="nofollow noreferrer" target="_blank">花儿不哭</a>：RVC变声器创始人，GPT-sovits作者，关注声音复刻的关注就完事儿了。</li><li><a class=" wrap external" href="https://space.bilibili.com/241286257" rel="nofollow noreferrer" target="_blank">风信子的猫Redamancy</a>：<span class="nolink">数字人对话系统 Linly-Talker</span>。</li><li><a class=" wrap external" href="https://space.bilibili.com/39089748" rel="nofollow noreferrer" target="_blank">李自然说</a>：AI连续创业者，对业界的思考很有价值。</li><li><a class=" wrap external" href="https://space.bilibili.com/19319172/?spm_id_from=333.999.0.0" rel="nofollow noreferrer" target="_blank">差评君</a>：一些AI领域评测和分享，范围较广。</li><li><a class=" wrap external" href="https://space.bilibili.com/1732848825" rel="nofollow noreferrer" target="_blank">耿同学讲故事</a>：北航老哥，战斗力非常猛，下饭利器！</li><li><a class=" wrap external" href="https://space.bilibili.com/49975325/?spm_id_from=333.999.0.0" rel="nofollow noreferrer" target="_blank">机器人科学与技术</a>：会分享最新的一些国际大组的机器人演示demo，但没有做更多点评。</li><li><a class=" wrap external" href="https://space.bilibili.com/257271972/?spm_id_from=333.999.0.0" rel="nofollow noreferrer" target="_blank">浙大高飞！</a>：古中国掌管无人机的爷！</li><li><a class=" wrap external" href="https://space.bilibili.com/371846699/?spm_id_from=333.999.0.0" rel="nofollow noreferrer" target="_blank">图灵的猫</a>：有朋友吐槽他的视频不太严谨，但我个人认为还是可以作为开拓视野来看的。</li><li><a class=" wrap external" href="https://space.bilibili.com/23947287/?spm_id_from=333.999.0.0" rel="nofollow noreferrer" target="_blank">小约翰可汗</a>：说到下饭视频，必须得有可汗，哈哈。</li><li><a class=" wrap external" href="https://space.bilibili.com/1010101551/?spm_id_from=333.999.0.0" rel="nofollow noreferrer" target="_blank">来自星星的何教授</a>：室温超导+学术八卦跑的最快的up~</li><li><a class=" wrap external" href="https://space.bilibili.com/393702473/?spm_id_from=333.999.0.0" rel="nofollow noreferrer" target="_blank">落英行者</a>：各种尖端行业深度解析，很好奇素材都是哪儿来的。</li><li><a class=" wrap external" href="https://space.bilibili.com/357669580?spm_id_from=333.337.0.0" rel="nofollow noreferrer" target="_blank">萌萌战队</a>：空气动力学，激波！最像营销号的干货号。</li><li><a class=" wrap external" href="https://space.bilibili.com/475312678/?spm_id_from=333.999.0.0" rel="nofollow noreferrer" target="_blank">二进制哈士奇</a>：学术版GPT的作者，分享学术版GPT最新的功能。</li><li><a class=" wrap external" href="https://space.bilibili.com/431556168/?spm_id_from=333.999.0.0" rel="nofollow noreferrer" target="_blank">浪子之心科技</a>：数字人，AIGC开源项目介绍。</li><li><a class=" wrap external" href="https://space.bilibili.com/1572312/?spm_id_from=333.999.0.0" rel="nofollow noreferrer" target="_blank">李鲁鲁</a>：AIGC、LLM角色扮演、论文分享，大佬的知乎我忘记贴了！</li><li><a class=" wrap external" href="https://space.bilibili.com/12566101/?spm_id_from=333.999.0.0" rel="nofollow noreferrer" target="_blank">秋葉aaaki</a>：AI绘图界的喂饭区UP主，狠狠关注！</li><li><a class=" wrap external" href="https://space.bilibili.com/615957867/?spm_id_from=333.999.0.0" rel="nofollow noreferrer" target="_blank">五里墩茶社</a>：最新的LLM相关工具分享，很多新工具都有新手入门，值得关注。</li><li><a class=" wrap external" href="https://space.bilibili.com/1369507485/?spm_id_from=333.999.0.0" rel="nofollow noreferrer" target="_blank">ShusenWang</a>：王老师的强化学习课和推荐系统课，都是免费的，讲的非常好！（虽然我都没看过，群友说的）</li><li><a class=" wrap external" href="https://space.bilibili.com/314022607/?spm_id_from=333.999.0.0" rel="nofollow noreferrer" target="_blank">王树义老师</a>：一些新AI工具的使用分享。比较适合小白。</li><li><a class=" wrap external" href="https://space.bilibili.com/295428344/?spm_id_from=333.999.0.0" rel="nofollow noreferrer" target="_blank">霍华德vlog</a>：华叔出走知乎，去了B站，现在主要分享rwkv的内容，以及一些泛AI的信息。</li><li><a class=" wrap external" href="https://space.bilibili.com/1567748478/?spm_id_from=333.999.0.0" rel="nofollow noreferrer" target="_blank">跟李沐学AI</a>：关注就行，最近老师创业去了，断更了。</li><li><a class=" wrap external" href="https://space.bilibili.com/604515161/?spm_id_from=333.999.0.0" rel="nofollow noreferrer" target="_blank">RLChina强化学习社区</a>：强化人必须关注！</li><li><a class=" wrap external" href="https://space.bilibili.com/344849038?spm_id_from=333.337.0.0" rel="nofollow noreferrer" target="_blank">YJango(于建国)</a>：于建国博士，现在在西电当老师，他对AI和认知的思考比较深刻，视频做的通俗易懂，推荐关注。</li></ol><h3>0319新增：</h3><ol><li><a class=" wrap external" href="https://space.bilibili.com/373596439/?spm_id_from=333.999.0.0" rel="nofollow noreferrer" target="_blank">deep_thoughts</a>：LLM和Diffusion经典论文解读和Pytorch源码解读；</li><li><a class=" wrap external" href="https://space.bilibili.com/59807853/?spm_id_from=333.999.0.0" rel="nofollow noreferrer" target="_blank">五道口纳什</a>：LLM+diffusion+RL+docker开发等教程；非常全~</li><li><a class=" wrap external" href="https://space.bilibili.com/823532/?spm_id_from=333.999.0.0" rel="nofollow noreferrer" target="_blank">秋刀鱼的炼丹工坊</a>：CV+LLM等最新论文速览；</li><li><a class=" wrap external" href="https://space.bilibili.com/13611123/?spm_id_from=333.999.0.0" rel="nofollow noreferrer" target="_blank">投研部杨摸鱼</a>：商科生，在投研工作，以最好看的方式、最通俗的语言，介绍，芯片、人形机器人相关的知识背景和底层逻辑。</li><li><a class=" wrap external" href="https://space.bilibili.com/2458477/?spm_id_from=333.999.0.0" rel="nofollow noreferrer" target="_blank">贯一智能科技</a>：泛AI领域的科普账号，内容做的比较好，具身智能，模型测评，论文导读各种栏目；</li><li><a class=" wrap external" href="https://space.bilibili.com/3031494/?spm_id_from=333.999.0.0" rel="nofollow noreferrer" target="_blank">刘悦的技术博客</a>：语音合成领域非常活跃的大佬，视频更新频率离谱的高。</li><li><a class=" wrap external" href="https://space.bilibili.com/81249447/?spm_id_from=333.999.0.0" rel="nofollow noreferrer" target="_blank">老司机耿进财</a>：科研和学术领域的基础规则，导师和师兄师姐都不会跟你说这么明白的事情，现在你可以免费听得到。</li></ol><h3>微信公众号：</h3><blockquote>标注别人公众号有软广，有点不合适，我先删了，大家关注的时候自己注意下。</blockquote><h3>泛AI领域：</h3><p>首先是中文三顶会！</p><p><b>量子位、机器之心、新智元</b>。</p><p>别管什么标题党，什么小编不懂AI，谁都不能否认他们是泛AI的顶流自媒体。他们确实会分享一些最新的比较火的资讯，关注，每天刷一遍标题，就完事儿了。</p><p>然后介绍一些其他的泛AI媒体：</p><ol><li>36氪：和LLM、AI关系没那么大，但也是传统科技媒体了。</li><li>Z Potentials：LLM，AIGC创业投资相关资讯。</li><li>爱可可爱生活：其实微博才是大佬的主战场，大佬会分享一些最新的论文，如果有点评就更好了。</li><li>数字生命卡兹克：最新AI产品分析、AI技术干货整理、原创AI应用分享。泛AI领域，啥都玩的大佬。</li><li>李rumor：强化，大模型相关资讯分享，招聘信息发布。</li><li>AI科技评论：CSDN旗下的公众号，相当于是弱版的三顶会。（哈哈，这么评价会不会得罪人？）</li><li>将门创投：干货较多，但更新频率不高，能有办法直接邀请论文作者做免费分享。</li></ol><p>NLP领域：</p><ol><li>老刘说NLP：NLP相关的干货内容。</li><li>JioNLP：作者喜欢开源，喜欢分享各种AI知识，干货比例会比较高。</li><li>NewBeeNLP：nlp相关内容，最新学术、技术贴。</li><li>GithubDaily：会介绍一些Github热门的项目，现在主要是LLM相关的内容。</li><li>这个信息质量太差了，我删掉了。</li></ol><h3>机器人领域的：</h3><ol><li>机器人大讲堂：清华孙富春老师团队维护的平台，机器人必关注，干货多。</li><li>九章智驾：自动驾驶领域的“顶会”。</li></ol><h3>计算机视觉领域：</h3><ol><li>我爱计算机视觉：主要会介绍一些最新论文解读。</li><li>AIwalker：cv相关的论文分享，干货比例高。</li><li>cver：相当于cv界的顶会，哈哈。</li><li>计算机视觉life：主要是自动驾驶、slam相关的视觉。</li></ol><h3>游戏领域：</h3><ol><li>游戏葡萄：干货比较多，平台能接触到很多一线大厂的资源。</li></ol><p><b>公众号太难推荐了，大家帮忙分享一些~</b></p><h3>优质博客、网页推荐：</h3><ol><li><a class=" wrap external" href="https://lilianweng.github.io/" rel="nofollow noreferrer" target="_blank">Lil'Log</a>：lilian 小姐姐应该放第一个！看她的博客，基本上可以把一个领域，系统的梳理清楚。通俗易懂，深入浅出！前几天刚出一个博客，摁头安利！</li><li><a class=" wrap external" href="https://arxiv.org/search/?query=large+language+model&amp;searchtype=all&amp;abstracts=show&amp;order=-submitted_date&amp;size=50" rel="nofollow noreferrer" target="_blank">LLM-Arxiv</a>：关于LLM的最新Arxiv论文列表，有空刷一下。这是原始信息源。下面的cool papers是进阶版。</li><li><a class=" wrap external" href="https://papers.cool/" rel="nofollow noreferrer" target="_blank">Cool Papers - Immersive Paper Discover</a>：苏剑林大佬开发的一个刷论文的网站，虽然我感觉直接看英文还是不够舒服，但群友们的反馈还挺好的。 </li><li><a class=" wrap external" href="https://spaces.ac.cn/" rel="nofollow noreferrer" target="_blank">科学空间|Scientific Spaces</a>：苏佬的科学空间不多介绍了。</li><li><a class=" wrap external" href="https://huggingface.co/papers" rel="nofollow noreferrer" target="_blank">Daily Papers - Hugging Face</a>：由Huggingface的AK大佬亲自维护的一个论文日榜，但对中文用户不太友好。</li><li><a class=" wrap external" href="https://github.com/trending?spoken_language_code=" rel="nofollow noreferrer" target="_blank">Github Trending</a>：Github热榜，程序员必刷，祝大家早日登榜！</li><li><a class=" wrap external" href="https://news.mit.edu/" rel="nofollow noreferrer" target="_blank">MITNews</a>：应该是国内科技自媒体的上游信息源了。</li><li><a class=" external" href="https://paperswithcode.com/sota" rel="nofollow noreferrer" target="_blank"><span class="invisible">https://</span><span class="visible">paperswithcode.com/sota</span><span class="invisible"></span></a>：一些领域的sota方法排行榜。</li><li><a class=" wrap external" href="https://www.runoob.com/" rel="nofollow noreferrer" target="_blank">菜鸟教程 - 学的不仅是技术，更是梦想！</a>：拓展技术栈比较好的网站，无广告，非常舒服。</li></ol><h3>播客：</h3><p>现在播客好像也成了新的战场，推荐几个干货非常多的博主，都在“小宇宙app”（@小宇宙，快把广子给我，哈哈）：</p><ol><li>42章经：关注就完事儿了。都是直接访谈AI领域最一线的大佬，纯干货。</li><li>AI局内人：同上。</li><li>屠龙之术：同上；</li><li>张小珺：商业访谈录，最近深度采访月之暗面的杨植麟，王小川等大佬；</li></ol><p>好吧，我暂时就关注了这两个，欢迎大家补充。</p><p class="ztext-empty-paragraph"><br /></p><h3>了解一个领域的常见技巧：</h3><ol><li>谷歌学术搜关键词：找到survey，或者引用数比较高的论文，然后用ChatPaper总结，或者学术版GPT免费翻译，快速阅读。</li><li>如果是最新的论文：Arxiv搜关键词。</li><li>看paperwithcode的排行榜，比如：<a class=" external" href="https://paperswithcode.com/sota" rel="nofollow noreferrer" target="_blank"><span class="invisible">https://</span><span class="visible">paperswithcode.com/sota</span><span class="invisible"></span></a></li><li>如果是代码复现，可以Github搜：<b>awesome+xxx</b>,一般会有大佬给你整理好相关的资料。</li></ol><p class="ztext-empty-paragraph"><br /></p><h3>推：</h3><p>直接参考这个就好了：</p><p><a class=" wrap external" href="https://hub.baai.ac.cn/view/24040" rel="nofollow noreferrer" target="_blank">107 个值得关注的AI Twitter，61 个工具和 28 个通讯 - 智源社区</a></p><p class="ztext-empty-paragraph"><br /></p><p>欢迎大家评论区分享自己关注的其他优秀答主！</p><p>希望大家在收藏的同时，记得点赞和评论！</p>
]]></content:encoded>
<pubDate>Sun, 25 Feb 2024 17:55:19 GMT</pubDate>
<pubDate>Sun, 25 Feb 2024 17:55:19 GMT</pubDate>
</item>

</channel>
</rss>