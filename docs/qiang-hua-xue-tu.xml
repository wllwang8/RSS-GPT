<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
<channel>
<title>强化学徒的知乎动态</title>
<link>https://www.zhihu.com/people/heda-he-28/activities</link>


<item>
<title>强化学徒回答了问题: OpenAI发布了最新的旗舰模型GPT-4o,如何评价GPT-4o?</title>
<link>https://www.zhihu.com/question/655916007/answer/3497499593</link>
<guid>https://www.zhihu.com/question/655916007/answer/3497499593</guid>
<content:encoded><![CDATA[
<div> 机器人、发布会、伦理风险、4o、5o
<br />
要点1: 发布会未提及机器人，但对机器人界影响最大，可能存在伦理问题，4o具备多个器官，带来更大的伦理风险。
要点2: 调用机器人控制使其能在物理世界与人类实时交互，风险比简单交互大。
要点3: 任何做机器人的人都不会拒绝将4o与机器人结合，担心被他人先实现某功能。
要点4: 未来发展走向不确定，希望5o时不会惩罚如此言论。
<br /><br />
总结: 本文探讨了发布会未提及机器人但对机器人界影响最大的情况，讨论了伦理问题以及4o与机器人结合的可能性。同时指出机器人控制带来的风险比简单交互大，未来发展不确定，希望5o对发表此言论的人不加惩罚。 <div>
<p>这次发布会全程没提机器人，但其实对机器人界的影响是最大的。</p><p>甚至于可能存在一些伦理问题，因为现在4o已经开眼+有耳朵+有嗓子了，比moss还多两个器官。</p><p>稍微调用一下机器人的控制，就能在物理世界和人类实时交互+物理交互，这个的伦理风险，比简单的聊天和讲笑话，要大的多。</p><p>但几乎任何做机器人的朋友，都不会拒绝将4o和机器人缝合，甚至于担心手慢了，被别人先实现某个功能。</p><p>未来啊，真的不知道是好还是坏，只希望5o来了的时候，看到我的这些帖子，不会把我带走狠狠拷打~轻点也行~</p>
]]></content:encoded>
<pubDate>Mon, 13 May 2024 19:03:20 GMT</pubDate>
</item>
<item>
<title>强化学徒回答了问题: OpenAI 发完 GPT-4o，国内大模型行业还有哪些机会？</title>
<link>https://www.zhihu.com/question/655916551/answer/3497498418</link>
<guid>https://www.zhihu.com/question/655916551/answer/3497498418</guid>
<content:encoded><![CDATA[
<div> openai，大模型，三模态，文本性能，具身智能
<br />
总结: 4o的发布对国内大模型行业带来了危机和机遇，openai为实时端到端输入输出打开了新的路线。然而，关于具体实现细节尚未公开，需要等待后续数据的蒸馏。此外，文本性能和具身智能方面也遇到了挑战，但也引发了友商的竞争潜力。4o的出现为机器人项目提供了发展机会，让项目可以快速起飞，为具身智能的实现提供更多可能性。 <div>
<p>4o的发布，对国内大模型行业同样也是危机并存。</p><p>openai至少帮大家跑通了这条路线：三模态端到端实时输入输出。</p><p>至于如何实现，目前我看完所有的公开资料，也没有看到具体的内容，openai的blog现在是一点技术都没有~</p><p>只能期待后面不断的蒸馏4o的数据，哈哈。</p><p>另外，关于文本的性能，4o的提升也遇到了瓶颈，这个结果对大家来说应该还是好消息，不能每年被遥遥领先一次吧，好不容易已经有友商快接近4的能力了。</p><figure><img class="origin_image zh-lightbox-thumb lazy" src="https://pic4.zhimg.com/v2-398355bf98b18a081c87b3fdd6c0a867_1440w.jpg" /></figure><figure><img class="origin_image zh-lightbox-thumb lazy" src="https://pic4.zhimg.com/v2-311f96ca5134b526266c42c348aa13c7_1440w.jpg" /></figure><figure><img class="origin_image zh-lightbox-thumb lazy" src="https://pic2.zhimg.com/v2-0757e6ed61c31991fde0efeca939b35d_1440w.jpg" /></figure><p>最后，对于做具身智能，机器人的朋友，4o的出现，让大家的机器人项目可以迅速起飞，我现在恨不得已经答辩通过，去某个厂梭哈具身，把女朋友做出来~</p><p></p>
]]></content:encoded>
<pubDate>Mon, 13 May 2024 18:55:19 GMT</pubDate>
</item>
<item>
<title>强化学徒回答了问题: GPT-4o 的实时对话效果，是否可以取代同声传译了？</title>
<link>https://www.zhihu.com/question/655916532/answer/3497497291</link>
<guid>https://www.zhihu.com/question/655916532/answer/3497497291</guid>
<content:encoded><![CDATA[
<p>这个延迟和效果，应该是可以的，如果他们的中文没有故意调差的话（苹果哥说支持中文，所以问题应该不大~）</p><p>另外，可以替代的除了同传，还有家庭教师（拍照解题），还有各种奇奇怪怪的工种，人类，危~</p><p></p>
]]></content:encoded>
<pubDate>Mon, 13 May 2024 18:49:14 GMT</pubDate>
</item>
<item>
<title>强化学徒回答了问题: GPT-4o 实时语音交流表现惊艳，它在语音识别和生成方面有哪些创新？</title>
<link>https://www.zhihu.com/question/655916910/answer/3497496580</link>
<guid>https://www.zhihu.com/question/655916910/answer/3497496580</guid>
<content:encoded><![CDATA[
<p>极低的延迟（0.3秒的平均延迟）+音色识别+灵活的音色/情绪生成。</p><p>整个模型端到端构建了音频+图/视频+音频的输入输出，而且速度非常快，难以想象。</p>
]]></content:encoded>
<pubDate>Mon, 13 May 2024 18:45:23 GMT</pubDate>
</item>
<item>
<title>强化学徒回答了问题: ChatGPT 更新实时语音能力效果惊艳，在哪些应用场景可以迅速改善用户体验？</title>
<link>https://www.zhihu.com/question/655916952/answer/3497495766</link>
<guid>https://www.zhihu.com/question/655916952/answer/3497495766</guid>
<content:encoded><![CDATA[
<p>好，我先答这题。前段时间和朋友刚做了一个英文视频自动中文配音的项目：<a class=" external" href="https://github.com/CuSO4Gem/pytvzhen" rel="nofollow noreferrer" target="_blank"><span class="invisible">https://</span><span class="visible">github.com/CuSO4Gem/pyt</span><span class="invisible">vzhen</span><span class="ellipsis"></span></a></p><p>这段时间我刷油管英文教程的速度大幅度提升，还开心了好久。</p><p>4o出来之后，工作流可以大幅度降低，直接调API就完事儿了，最多再保留一下音频拆解和视频合成。</p><p>输了，彻底输了。</p>
]]></content:encoded>
<pubDate>Mon, 13 May 2024 18:41:00 GMT</pubDate>
</item>
<item>
<title>强化学徒赞同了回答: 南京林业大学 38 岁副教授宋某自杀离世，网传因科研压力，高校回应称与事实严重不符，哪些问题值得深思？</title>
<link>https://www.zhihu.com/question/655814051/answer/3497372787</link>
<guid>https://www.zhihu.com/question/655814051/answer/3497372787</guid>
<content:encoded><![CDATA[
<div> 高校 教职 青椒 转行 退路
总结：<br /><br />文章讨论了学术圈中教职固化的问题，提出了要有退出机制的思路。然而当真实实施时，会给一些青椒带来困扰。作者认为需要放宽视野，往外看，找到其他机会。他分享了自己转行后的经历，认为在理工科转行去企业并不难，可以找到不错的工作。建议如果对学术圈不满意或想转行的人可以考虑到体制外或企业里寻找新机会。<br /> <div>
<p>感觉挺无解的。</p><p>之前讨论学术圈阶级固化，不学无术的老头子们躺在高位尸位素餐，不肯下来的时候，很多人在说要有退出机制，能上也要能下。</p><p>结果真的实施起来了，板子打在自己身上了，又成了青椒们不能承受之痛了。</p><p class="ztext-empty-paragraph"><br /></p><p>我很能理解宋老师的心情，四年考核期不合格，降级还得退安家费，对一个刚回国的青年教师来说确实是天塌了一样的存在，更不用说可能还在高位接盘了南京的房子。</p><p>但另一方面，副教授变成讲师编制还在也不是完全不能过，学术圈这种充斥着社会达尔文主义成王败寇思想的地方，拿不到青基也确实有些难混。</p><p>所以到底咋办？</p><p class="ztext-empty-paragraph"><br /></p><p>我现在能想到的就是，还是得视野放宽点，往外看，到学术圈外面找机会。</p><p>只有清晰地看到退路，不管用不用得上吧，才会有勇气去面对，去接受，或是去拒绝遭受的挫折。</p><p>我博士毕不了业的时候也很痛苦，网络上也有很多毕不了业的博士生们很痛苦，甚至想不开走上绝路的。</p><p>大家也都知道，对于这种钻牛角尖的心态，最好的解其实就是想开点，毕不了业又怎么样，实在不行凭借硕士乃至本科文凭出去找工作，又不会死。</p><p class="ztext-empty-paragraph"><br /></p><p>而对青椒来说，客观上不搞学术的沉没成本要更大一些，对于出去能不能找到合适工作的担忧也会更大一些。</p><p>主观上，学术圈浸淫久了，也会产生万般皆下品惟有教职高的心态，特别是都已经拿到教职了，感觉已经扬眉吐气打败99%的学术同龄人了，突然发现这条路走不下去，心理上也是很难接受的。</p><p>我最近在跟一些学术圈的朋友聊天，他们直到两年多之后的今天，也依旧在劝我：要不还是投个简历回高校吧，你这个背景不继续做学术真的可惜了。</p><p class="ztext-empty-paragraph"><br /></p><p>虽然这可能和我日常哭穷哭惨有关（笑），但这种对比的倾向性还是很有代表性的。</p><p>然而从我的角度，我是真的不觉得找一所不错的高校当个副教授养老就一定比在企业里面当个打工狗优越，也真的觉得现在相当一部分的学术圈，真的，很无趣。</p><p>你可以不赞同我的后半句，但我希望你能认可前半句。</p><p class="ztext-empty-paragraph"><br /></p><p>接受自己转行的可能性后，就是可行性的问题了。文科我不了解，理工科高校老师出来去企业真的没那么难找工作。</p><p>大家好歹都是博士，本硕博学校又不会差，体制外对博士一般也不会卡35岁，高校工作经历虽然有点专，但再也么也不至于是减分项。</p><p>像我一样转金融或许有点49年入国军的意思（但说实话也没那么惨，好歹不用过着天天吸有机溶剂的日子了），去个行业内的大公司做研发简历关还是很好过的。</p><p>有这方面担心的，学术圈混不下去想找退路的，或是科研做的不开心想往外跳的，也欢迎来找我聊一聊机会。不说一定能提供多有价值的建议，但至少看看我从高校辞职后还活蹦乱跳的样子，或许也能得到一些宽慰吧。</p>
]]></content:encoded>
<pubDate>Mon, 13 May 2024 15:11:32 GMT</pubDate>
</item>
<item>
<title>强化学徒赞同了文章: 强化学习reward shaping推导和理解</title>
<link>https://zhuanlan.zhihu.com/p/42518767</link>
<guid>https://zhuanlan.zhihu.com/p/42518767</guid>
<content:encoded><![CDATA[
<div> 强化学习、reward shaping、value function、Q-function、Bellman Equation
<br />
在这篇文章中，通过对强化学习中的reward shaping理论进行分析，深入理解了RL中的基本概念和思想。首先，强化学习问题可以看作是MDP过程，其中reward function包含当前状态、动作和下一个状态。策略Policy、value function和Q-function是重要的定义，用于衡量状态和动作的“优秀”程度。最优策略需要满足Bellman最优原理，而reward shaping则可以通过将potential-based函数F添加到奖励函数中，不改变最优策略的情况下加速强化学习。尽管reward shaping会让最优value function变为0，使学习变得简单，但需要注意先验估计的影响。因此，reward shaping在实际应用中需谨慎使用。 
<br /><br />总结: 
1. 强化学习是MDP过程，reward shaping包含当前状态、动作和下一个状态。
2. Policy、value function和Q-function衡量“优秀”程度。
3. 最优策略需满足Bellman最优原理。
4. Reward shaping通过添加potential-based函数加速学习，但需注意先验估计的影响。
5. 虽然reward shaping使最优value function变为0简化学习，但需谨慎使用。 <div>
<p>最近看了相关的一些工作和文章，强化学习最近确实是引起了广泛的关注，但是觉得对其原理还有理论的一些理解和工作还是相当有必要的。这里分析下吴恩达很早之前做的reward shaping（ICML 1999）。其实看很多文章都是引用过这篇文章的，而且现在的引用也有快800，但是在推导完成后我返回去看，确实觉得这里的约束太强，而且novelty感觉很简单，并且有点隔靴搔痒的感觉，个人见解，不过要是完整的推完你会跟我有一样的感觉。不过其实写这篇文章的目的并不是来说reward shaping，而是通过RS (reward shaping)来理解RL中的基本概念以及一些基本思想（value function, Q-function, policy, MDP, Bellman Equation）。所有的推导其实来源于<a class=" external" href="https://www.teach.cs.toronto.edu//~csc2542h/fall/material/csc2542f16_reward_shaping.pdf" rel="nofollow noreferrer" target="_blank"><span class="invisible">https://www.</span><span class="visible">teach.cs.toronto.edu//~</span><span class="invisible">csc2542h/fall/material/csc2542f16_reward_shaping.pdf</span><span class="ellipsis"></span></a>。</p><p><b>1、考虑强化学习问题为MDP过程</b></p><figure><img class="origin_image zh-lightbox-thumb" src="https://pic1.zhimg.com/v2-3c82ba071ee4e1bf1b641517df4083b0_1440w.jpg" /></figure><p>这里公式太多，就直接截图，但是还是比较简单的模型，比较要注意或者说仔细看的位置是reward function <img alt="R :S \times A \times S \to \mathbb{R} " src="https://www.zhihu.com/equation?tex=R+%3AS+%5Ctimes+A+%5Ctimes+S+%5Cto+%5Cmathbb%7BR%7D+" /> , 意思就是这个奖励函数要同时获得三个元素：当前状态、动作、以及相应的下一个状态。是不是感觉有点问题？这里为什么要获取下一个时刻的状态呢？你本来是个不停滚动向前的过程，只用包含(s, a)就行，下个时刻的reward自然会b包含s'。先这里存疑，后面就会发现这样的用法，就是为了“凑”reward shaping的一些属性。</p><p>策略Policy 的定义就很简单</p><figure><img class="origin_image zh-lightbox-thumb" src="https://pic3.zhimg.com/v2-181e05d60132cc9d737b8d66c2e0f186_1440w.jpg" /></figure><p><b>这里几个重要的定义：</b></p><figure><img class="origin_image zh-lightbox-thumb" src="https://pic2.zhimg.com/v2-04bd8a2fb66469ddd12606b741ae1e0d_1440w.jpg" /></figure><figure><img class="origin_image zh-lightbox-thumb" src="https://pic4.zhimg.com/v2-c67a8dee8c0976163ccec11ae26ab4cb_1440w.jpg" /></figure><p>value function 和 Q-function。主要注意这里的下标以及上标 <img alt="V^{\pi}_M(s)" src="https://www.zhihu.com/equation?tex=V%5E%7B%5Cpi%7D_M%28s%29" /> 表示在马尔科夫过程M下面，按照策略Pi的value function。然后看数学定义也很容易知道value function表示当前状态按照如此梯度得到的期望（平均）累积返回奖励函数，有点蒙特卡洛的意思。简单的来说，就是衡量<b>当前状态采用策略Pi</b>的“优秀”程度。Q-function其实跟value function类似，就是多了一个action在里面，就是<b>当前状态采用动作a后再按照策略Pi的“优秀”程度。</b></p><p><b>2、最优（Optimal）</b></p><p>既然是策略和奖励函数，肯定有最优的。最优的衡量指标是discounted sum of reward，其实就是value function。</p><p><img alt="V^*_M = \sup_{\pi} V^{\pi}_M" src="https://www.zhihu.com/equation?tex=V%5E%2A_M+%3D+%5Csup_%7B%5Cpi%7D+V%5E%7B%5Cpi%7D_M" /> </p><p>那么同样有</p><p><img alt="Q^*_M = \sup_{\pi} Q^{\pi}_M" src="https://www.zhihu.com/equation?tex=Q%5E%2A_M+%3D+%5Csup_%7B%5Cpi%7D+Q%5E%7B%5Cpi%7D_M" /></p><p>那么最优的策略，就是给定状态s后产生让上面的Q最大的动作a的一个映射了</p><p><img alt="\pi^*(s) = \underset{a \in A}\arg \max Q^*_M(s, a)" src="https://www.zhihu.com/equation?tex=%5Cpi%5E%2A%28s%29+%3D+%5Cunderset%7Ba+%5Cin+A%7D%5Carg+%5Cmax+Q%5E%2A_M%28s%2C+a%29" /> </p><p>这些都很好理解只要认真的看了数学的表达式。</p><p><b>3、最优原理</b></p><p>根据Bellman最优原理</p><figure><img class="origin_image zh-lightbox-thumb" src="https://pic4.zhimg.com/v2-09f25e7cfd001a091bdeaffd5ca618ef_1440w.jpg" /></figure><p>这个式子是核心，一定要理解。其实这里可以用极限的思想来理解，我这里举个不恰当的例子，但是理解的思想是一样的。</p><p><img alt="\lim_{t\to \infty} \|M(t+1) - M(t)\| = 0" src="https://www.zhihu.com/equation?tex=%5Clim_%7Bt%5Cto+%5Cinfty%7D+%5C%7CM%28t%2B1%29+-+M%28t%29%5C%7C+%3D+0" /> 如果M极限存在。</p><p><b>4、reward shaping</b></p><p>这里先放结论</p><figure><img class="origin_image zh-lightbox-thumb" src="https://pic3.zhimg.com/v2-7530f1e4bc3f4f5864ff852073fcbeda_1440w.jpg" /></figure><p>就是如果F是potential-based，那么改变之后的reward function= R + F重新构成的马尔科夫过程的最优控制还是不变，跟原来一样。</p><figure><img class="origin_image zh-lightbox-thumb" src="https://pic1.zhimg.com/v2-541cd293f3ed20629558124df23116dc_1440w.jpg" /></figure><p>这个定义就很巧妙了，记住之前为什么要用s'的问题。开始推导。</p><p>推导也蛮简单，就是在Bellman最优方程两边同时减去phi(s)来把F凑出来。</p><figure><img class="origin_image zh-lightbox-thumb" src="https://pic4.zhimg.com/v2-6d5b6bd29491d6f0fa37d07ebb00881f_1440w.jpg" /></figure><figure><img class="origin_image zh-lightbox-thumb" src="https://pic1.zhimg.com/v2-044e95bd2bffcd9cc7926fc972bc5884_1440w.jpg" /></figure><p>还记的F的定义吗？</p><figure><img class="content_image" src="https://pic2.zhimg.com/v2-25a263d55d42b2e448705d15200b09bd_1440w.jpg" /></figure><p>同时定义</p><figure><img class="content_image" src="https://pic2.zhimg.com/v2-cebd77c09a72983f8317988c1748fd81_1440w.jpg" /></figure><p>替换之后</p><p>得到</p><figure><img class="origin_image zh-lightbox-thumb" src="https://pic4.zhimg.com/v2-c98431e3dc3fc8166d2d2bae39164467_1440w.jpg" /></figure><p>然后得到 <img alt="\hat{Q}_{M'} " src="https://www.zhihu.com/equation?tex=%5Chat%7BQ%7D_%7BM%27%7D+" /> 也满足Bellman最优方程。所以 <img alt="\hat{Q}_{M'} = Q^*_{M'} " src="https://www.zhihu.com/equation?tex=%5Chat%7BQ%7D_%7BM%27%7D+%3D+Q%5E%2A_%7BM%27%7D+" /> 。所以这里说明M中的最优策略也是M'中的最优策略，因为我们是从Q在M中的Bellman方程推过来的。所以这里成立。reward shaping 不改变最优策略。但是我们推完，是不是感觉就是加减了一项，然后把多出来的给了R？所以这里为什么R是要是R(s, a, s')的原因了。感觉被欺骗了。</p><p class="ztext-empty-paragraph"><br /></p><p>好，然后这个的好处，可以加速强化学习。</p><figure><img class="origin_image zh-lightbox-thumb" src="https://pic1.zhimg.com/v2-b87388ccaca48e19b0200bdc58e11e70_1440w.jpg" /></figure><p>这个让Gamma等于1其实无可厚非。但是，下面</p><figure><img class="origin_image zh-lightbox-thumb" src="https://pic3.zhimg.com/v2-6c2a0819921ae8995e7ef08b68c729e2_1440w.jpg" /></figure><p>如果让 <img alt="\phi{(s)} = V^*_M(s) " src="https://www.zhihu.com/equation?tex=%5Cphi%7B%28s%29%7D+%3D+V%5E%2A_M%28s%29+" /> ，这样要找的最优value function就是0了，这样就可以使学习更简单（这里确实没毛病，<b>学习0和学习identity（参考ResNet）确实简单，说明ResNet的思路Ng早就提出来了</b>）。但是！！！！我要是知道了 <img alt=" V^*_M(s) " src="https://www.zhihu.com/equation?tex=+V%5E%2A_M%28s%29+" /> ……我还用干嘛？……所以我觉得reward shaping就是在眼子……。好吧，其实也不是眼子，就是你有个先验的估计确实可以加速。</p><p class="ztext-empty-paragraph"><br /></p><p>完。</p><p></p>
]]></content:encoded>
<pubDate>Mon, 13 May 2024 06:30:28 GMT</pubDate>
</item>
<item>
<title>强化学徒发表了文章: 符尧博士-全栈Transformer推理优化第二季：部署长上下文模型-翻译</title>
<link>https://zhuanlan.zhihu.com/p/697244539</link>
<guid>https://zhuanlan.zhihu.com/p/697244539</guid>
<content:encoded><![CDATA[
<div> Transformer、长上下文、部署成本、优化、性能指标
<br />
<br />
总结:本文讨论了部署长上下文模型的优化挑战，重点关注了预填充、解码、并发性和上下文切换等性能指标。基于硬件架构和上下文长度等因素，提出了提高并发性、减少预填充和解码延迟、压缩隐藏信息等策略。针对层、头、标记和隐藏等维度，讨论了现有工作如何优化长上下文推理，并探讨了如何整合现有努力实现全栈优化。通过压缩KV缓存和维度减少等方法，可以降低长上下文模型的部署成本，促进其在各种应用中的广泛应用。 <div>
<h2>全栈Transformer推理优化第二季：部署长上下文模型</h2><p>符尧 | 网站 | 博客 | 推特 / X</p><p>爱丁堡大学 | <a href="mailto:yao.fu@ed.ac.uk">yao.fu@ed.ac.uk</a> </p><p>2024年5月17日 - Notionlytics | 尚未。</p><p>翻译校对：强化学徒</p><p>原帖：<a class=" wrap external" href="https://yaofu.notion.site/Full-Stack-Transformer-Inference-Optimization-Season-2-Deploying-Long-Context-Models-ee25d3a77ba14f73b8ae19147f77d5e2" rel="nofollow noreferrer" target="_blank">Notion – The all-in-one workspace for your notes, tasks, wikis, and databases.</a></p><p>为什么长上下文模型很重要？因为它们是先进AI应用的基础，如长达一小时的视频理解、存储库级别的编码智能体和终身AI伴侣。我们的研究目标是促进基于AI的应用生态系统。为了实现这一目标，我们必须降低长上下文Transformer的部署成本。</p><p>这是我们Transformer推理优化系列的第二季。在第一篇文章中，我们讨论了通用短上下文推理优化。本文侧重于长上下文优化。我们的目标是解决一个雄心勃勃的研究挑战：</p><ul><li>如何将1M上下文的生产级Transformer的部署成本降低到与4K一样便宜？</li></ul><p>我们首先描述了一个并发编程框架，用于定量分析在GPU高带宽内存（HBM）有限情况下为多个长上下文请求提供服务的效率挑战。我们详细分析了与4K上下文相比，所有额外的计算成本都可以追溯到一个单一来源：大规模的  缓存。我们以A100 NVLink上的50K上下文的34B GPT-3.5级模型为运行示例，并描述了其大型KV缓存导致四种部署挑战：</p><ol><li>预填充长输入比短输入需要更长的计算时间和GPU内存；</li><li>在预填充后，驻留在GPU HBM上的大型KV缓存大大限制了可服务的并发用户数量；</li><li>在解码过程中，从HBM到SM重复读取KV缓存大大增加了延迟；</li><li>当KV缓存内存溢出时，将其从HBM交换到DDR会导致显着的上下文切换延迟。</li></ol><p>我们进一步分析了现有工作如何从这四个角度解决部署挑战，并确定了结合它们构建端到端高效系统的可能性。我们希望我们的工作为分析长上下文Transformer的部署提供了一个基础框架，并确定了降低1M1M上下文推理成本至与4K4K一样便宜的重要方向。</p><p>1 - 有限GPU HBM尺寸下的并发编程框架</p><p>1.1 - 并发用户交互会话和偏好</p><p>1.2 - 作为端到端目标的基于会话的吞吐量</p><p>1.3 - 计算与内存受限性、算术强度和关键批处理大小</p><p>1.4 - 预填充</p><p>1.5 - 解码</p><p>1.6 - 并发控制和上下文切换</p><p>1.7 - 到目前为止的总结</p><p>2 - 强烈影响性能指标的因素</p><p>2.1 - 上下文长度</p><p>2.2 - 硬件架构</p><p>2.3 - 使用张量并行处理的多个GPU</p><p>3 - 可压缩性分析和现有工作</p><p>3.1 - 层</p><p>3.2 - 头</p><p>3.3 - 标记</p><p>3.4 - 隐藏</p><p>4 - 结论：朝着端到端系统的全栈优化迈进</p><h2>1 - 在有限GPU HBM尺寸下的并发编程框架</h2><p>考虑一个30+B 100K上下文GPT-3.5质量的开源模型，如QWen或Yi，4 K4 K与100 K100 K上下文的KV缓存之间的差异为：</p><p>100 K上下文：100000×60×8×128×2×2×2字节=22.8GB   </p><p>4 K上下文：4000seq1en ×60层 ×8头 ×128维度 ×2  ×2bf 16 字节=0.91 GB</p><p>这里我们使用Yi-34B 200K配置（60层，8个kv头和128隐藏维度）。假设我们使用2∗80G A100张量并行处理来为这个模型提供bf16服务，那么我们有2×2× 80−34×2=122 GB 的空余空间用于存储KV缓存。从这第一眼看，我们立即看到在这种设置下，我们可以实现约100+用户并发的4 K上下文，但只有5个100K上下文的用户。如果我们能够以与4 K相同的价格部署1M上下文模型，我们可以大幅推动长上下文和多模态生成模型的民主化，并促进新兴应用和智能体生态系统的发展。实际部署模型为多个用户提供服务时，工作流程通常如下：</p><p class="ztext-empty-paragraph"><br /></p><figure><img class="origin_image zh-lightbox-thumb" src="https://pic2.zhimg.com/v2-3b54cf00c4553e4ec70d0cf86c6fe16d_1440w.jpg" /></figure><p>图1. 在有限GPU HBM尺寸下为多个长上下文用户请求提供服务的并发编程框架。有四个关键因素共同决定用户交互会话的整体吞吐量：（1）并发受HBM尺寸限制：被服务的并发用户数量受GPU高带宽内存（HBM）尺寸的限制；（2）预填充受计算限制：到第一个生成的标记的延迟，即提示预填充时间，受GPU的浮点运算每秒（flops）的限制；（3）解码受内存限制（在关键批量大小下）：自回归解码的延迟（每秒生成的标记）受HBM的带宽限制；（4）上下文切换受PCIE限制：将用户1的KV缓存卸载到CPU DDR并将用户2的KV缓存加载到HBM受PCIE带宽的限制。所有这四个关键因素带来的效率挑战。</p><p>最终可以追溯到KV缓存的大小。</p><h2>1.1 - 并发用户交互会话和偏好</h2><p>在典型的交互会话中，用户从一个长文档的提示开始，然后是一个问题，并将其馈送给模型。模型接收初始提示，预先填充成KV缓存。用户等待预填充阶段，直到第一个标记开始生成，并希望等待时间不要太长。预填充后，模型开始自回归解码。用户与解码过程同时阅读输出，并希望解码速度快于人类阅读速度。模型完成解码后，用户继续阅读响应，思考，也许喝口咖啡，然后开始输入下一个问题。后续提示通常不像第一个提示那样长，因为第一个提示通常包含长上下文（书籍或视频），而后续提示通常只包含问题。当第一个用户正在阅读模型响应并考虑下一个问题时，模型基本上处于空闲状态，因此如果同时另一个用户提出另一个问题，模型可以通过将第一个用户的KV缓存卸载到CPU DDR内存来进行上下文切换，以便为第二个用户的KV缓存腾出HBM空间。两个用户互动地提出后续问题，直到会话结束。</p><h2>1.2 - 作为端到端目标的基于会话的吞吐量</h2><p>我们考虑多个用户同时与模型交互的情况。假设平均而言，一个用户会话包括50K个标记的文档/视频和5轮问题。在收到上一个问题的答案后，用户花费1分钟阅读答案并考虑下一个问题。我们的目标是最大化定义为会话基础吞吐量的指标：</p><figure><img class="origin_image zh-lightbox-thumb" src="https://pic1.zhimg.com/v2-03f2a58fb8e873d0787d20b39d1e7e10_1440w.jpg" /></figure><p>请注意，这个基于会话的吞吐量目标与现有的基于标记的吞吐量（即在给定时间内预填或解码的标记数量）不同。正如我们将很快讨论的那样，基于标记的吞吐量只是问题的一部分。我们的基于会话的吞吐量，即在给定时间内的并发用户交互数量，是一个端到端的目标，因为我们不仅考虑预填和解码，还考虑内存限制和上下文切换。</p><h2>1.3 - 计算与内存受限性，算术强度和关键批处理大小</h2><p>Transformer推理的一个重要观察是，预填充通常受GPU计算能力限制，即浮点运算，而解码受HBM内存带宽限制。我们说一个运算符是计算受限的，如果完成该运算符的大部分时间是在GPU的流多处理器上计算它（GPU执行块状并行计算）。我们说一个运算符是内存受限的，如果完成该运算符的大部分时间是将数据从内存移动到流多处理器（而不是实际在流多处理器上计算）。一个运算符是计算受限还是内存受限取决于其\textit{算术强度}，即每个内存访问操作（IO）执行多少浮点运算（FLOP）：</p><p><img alt="\begin{align}    \texttt{arithmetic intensity} = \frac{\texttt{FLOP}}{\texttt{IO}}\notag\end{align}" src="https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%7D++++%5Ctexttt%7Barithmetic+intensity%7D+%3D+%5Cfrac%7B%5Ctexttt%7BFLOP%7D%7D%7B%5Ctexttt%7BIO%7D%7D%5Cnotag%5Cend%7Balign%7D" /> </p><p>更高的并行性，更高的每个内存访问的FLOP，运算符更可能是计算受限的，我们更好地利用硬件。在给定的GPU上，运算符从内存受限变为计算受限的关键算术强度，即其FLOP / 内存带宽的比率。对于A100，它是：</p><p><img alt="\begin{align}    \texttt{A100 critical intensity} = \underset{\texttt{A100 bf16 FLOP}}{312\text{T flop per sec}}\;/ \underset{\texttt{A100 HBM Bandwidth}}{2\text{T byte per sec}} = 156\notag\end{align}" src="https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%7D++++%5Ctexttt%7BA100+critical+intensity%7D+%3D+%5Cunderset%7B%5Ctexttt%7BA100+bf16+FLOP%7D%7D%7B312%5Ctext%7BT+flop+per+sec%7D%7D%5C%3B%2F+%5Cunderset%7B%5Ctexttt%7BA100+HBM+Bandwidth%7D%7D%7B2%5Ctext%7BT+byte+per+sec%7D%7D+%3D+156%5Cnotag%5Cend%7Balign%7D" /> </p><p>对于transformer，并行性大约是我们向其中提供的标记数量，即批处理大小。这意味着，对于A100，当我们的批处理大小大于156个标记时，例如，在预填充阶段提示有50 K个标记时，我们是计算受限的，并充分利用A100的计算能力。当我们的批处理大小小于156个标记时，例如，在解码时我们只解码一个标记，我们是内存受限的，没有充分利用A100的计算能力。</p><h2>1.4 - 预填充</h2><p>现在我们分析在A100上预填充需要多长时间。由于预填充是计算受限的，即在A100上上下文长度超过156时，其理论峰值延迟为</p><p><img alt="\begin{align}    \texttt{theoretical peak latency} = \frac{\texttt{FLOP of prefilling}}{\texttt{FLOP per second of A100}}\notag\end{align}" src="https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%7D++++%5Ctexttt%7Btheoretical+peak+latency%7D+%3D+%5Cfrac%7B%5Ctexttt%7BFLOP+of+prefilling%7D%7D%7B%5Ctexttt%7BFLOP+per+second+of+A100%7D%7D%5Cnotag%5Cend%7Balign%7D" /> </p><p>For a prompt of 50K context length it is</p><p><img alt="\begin{align}\underset{\texttt{prefilling FLOP}}{4.33\text{P Flop}} &amp;= \underset{\texttt{seqlen}}{50 \text{K}} \times (2 \times \underset{\texttt{model param}}{34\text{B}} + 2 \times \underset{\texttt{layer}}{60} \times \underset{\texttt{seqlen}}{50\text{K}} \times\underset{\texttt{hidden}}{4096})\notag\\\underset{\texttt{prefilling latency}}{14.1 \;\text{seconds}} &amp;= \underset{\texttt{prefilling flop}}{4.33\text{P Flop}} /\; \underset{\texttt{A100 bf16}}{312\text{T Flop per sec}} \notag\end{align} " src="https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%7D%5Cunderset%7B%5Ctexttt%7Bprefilling+FLOP%7D%7D%7B4.33%5Ctext%7BP+Flop%7D%7D+%26%3D+%5Cunderset%7B%5Ctexttt%7Bseqlen%7D%7D%7B50+%5Ctext%7BK%7D%7D+%5Ctimes+%282+%5Ctimes+%5Cunderset%7B%5Ctexttt%7Bmodel+param%7D%7D%7B34%5Ctext%7BB%7D%7D+%2B+2+%5Ctimes+%5Cunderset%7B%5Ctexttt%7Blayer%7D%7D%7B60%7D+%5Ctimes+%5Cunderset%7B%5Ctexttt%7Bseqlen%7D%7D%7B50%5Ctext%7BK%7D%7D+%5Ctimes%5Cunderset%7B%5Ctexttt%7Bhidden%7D%7D%7B4096%7D%29%5Cnotag%5C%5C%5Cunderset%7B%5Ctexttt%7Bprefilling+latency%7D%7D%7B14.1+%5C%3B%5Ctext%7Bseconds%7D%7D+%26%3D+%5Cunderset%7B%5Ctexttt%7Bprefilling+flop%7D%7D%7B4.33%5Ctext%7BP+Flop%7D%7D+%2F%5C%3B+%5Cunderset%7B%5Ctexttt%7BA100+bf16%7D%7D%7B312%5Ctext%7BT+Flop+per+sec%7D%7D+%5Cnotag%5Cend%7Balign%7D+" /> </p><p>由于14.1秒是理论峰值，我们在图1中将其四舍五入为20秒以考虑实现开销。这意味着实际实现可能达到理论峰值性能的14.1/20≈ 70%，这对于在A100上进行cuda编程是一种常见经验。</p><p>如果上下文长度为4 而不是50 ，那么重复上述计算我们得到延迟为0.89秒。这里的差异是</p><p>4 的预填充延迟为0.89秒</p><p>50 的预填充延迟为14.1秒</p><p>13秒的差距，源自长上下文的额外FLOP，这是我们最终希望弥补的。</p><h2>1.5 - 解码</h2><p>现在我们分析解码需要多长时间。由于解码是内存受限的，即在A100上批处理大小小于156时，理论峰值延迟为</p><p>理论峰值延迟=内存访问字节数A100 HBM带宽理论峰值延迟=A100 HBM带宽内存访问字节数​</p><p><img alt="\begin{align}    \texttt{theoretical peak latency} = \frac{\texttt{bytes of memory access}}{\texttt{A100 HBM bandwidth}} \notag\end{align}" src="https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%7D++++%5Ctexttt%7Btheoretical+peak+latency%7D+%3D+%5Cfrac%7B%5Ctexttt%7Bbytes+of+memory+access%7D%7D%7B%5Ctexttt%7BA100+HBM+bandwidth%7D%7D+%5Cnotag%5Cend%7Balign%7D" /> </p><p>对于解码，一个前向传递意味着</p><p>内存访问字节数 == 模型权重 ++ KV缓存</p><p><img alt="\begin{align}    \texttt{bytes of memory access} = \texttt{model weights} + \texttt{KV Cache}\notag\end{align}" src="https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%7D++++%5Ctexttt%7Bbytes+of+memory+access%7D+%3D+%5Ctexttt%7Bmodel+weights%7D+%2B+%5Ctexttt%7BKV+Cache%7D%5Cnotag%5Cend%7Balign%7D" /> </p><p>我们假设平均模型生成一个屏幕标记（通常用户偏好生成长度约为一个屏幕），即约250个标记，那么峰值延迟为</p><p><img alt="\begin{align}    \underset{\texttt{one screen tokens}}{250} \times \underset{\texttt{model weight}}{(68\text{GB}} + \underset{\texttt{50K ctx KV cache}}{11\text{GB})}\;/\;\underset{\texttt{A100 HBM bandwidth}}{2\text{Tbytes per sec}} = \underset{\texttt{decoding latency}}{9.8\;\text{seconds}} \notag\end{align}" src="https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%7D++++%5Cunderset%7B%5Ctexttt%7Bone+screen+tokens%7D%7D%7B250%7D+%5Ctimes+%5Cunderset%7B%5Ctexttt%7Bmodel+weight%7D%7D%7B%2868%5Ctext%7BGB%7D%7D+%2B+%5Cunderset%7B%5Ctexttt%7B50K+ctx+KV+cache%7D%7D%7B11%5Ctext%7BGB%7D%29%7D%5C%3B%2F%5C%3B%5Cunderset%7B%5Ctexttt%7BA100+HBM+bandwidth%7D%7D%7B2%5Ctext%7BTbytes+per+sec%7D%7D+%3D+%5Cunderset%7B%5Ctexttt%7Bdecoding+latency%7D%7D%7B9.8%5C%3B%5Ctext%7Bseconds%7D%7D+%5Cnotag%5Cend%7Balign%7D" /> </p><p>由于9.8秒是理论峰值，在图1中我们将其四舍五入为12秒，以考虑实现开销。如果序列长度为4 K4 K，那么其对应的KV缓存仅为0.91GB，解码延迟降至8.5秒。然而，如果序列长度增加到200 K200 K，KV缓存变为44 GB44 GB，延迟增加到14秒。相对延迟增加与KV缓存和模型大小之间的相对大小有关，我们最终希望将其缩小。</p><h2>1.6 - 并发控制和上下文切换</h2><p>另一个重要考虑因素是，当KV缓存变大时，GPU HBM可以容纳的并发用户缓存数量为并发级别=HBM大小−模型权重KV缓存=KV缓存HBM大小−模型权重​</p><p>这意味着并发受HBM大小的限制。继续以我们的34B 50K模型示例为例，如果我们将其部署在一个80GB的A100上，我们只能为一个用户提供服务（图1）。但是，如果上下文为4 K，KV缓存约为1GB，我们可以同时为大约20个用户提供服务。</p><p>当第二个用户来询问关于长文档的问题时，为了为他们腾出KV缓存空间，我们需要进行上下文切换：将第一个用户的KV缓存卸载到CPU内存，并加载第二个用户的KV缓存（图1）。这会引起上下文切换开销：</p><p>上下文切换开销=用户1KV缓存+用户2KV缓存PCIE带宽=PCIE带宽用户1KV缓存+用户2KV缓存​</p><p>也就是说，上下文切换开销受PCIE带宽的限制，即GPU HBM与CPU DDR连接的速度有多快。假设我们使用每秒20G字节的PCIE gen 4，则50 K上下文的2个用户的上下文切换开销为：</p><p><img alt="\begin{align}    \underset{\texttt{context switching for 50K}}{1.1\;\text{seconds}} = \underset{\texttt{user 1 KV cache}}{(11\text{G bytes}} + \underset{\texttt{user 2 KV cache}}{11\text{G bytes})} / \;\underset{\texttt{PCIE bandwidth}}{{20\text{G bytes per sec}}}\notag\end{align}" src="https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%7D++++%5Cunderset%7B%5Ctexttt%7Bcontext+switching+for+50K%7D%7D%7B1.1%5C%3B%5Ctext%7Bseconds%7D%7D+%3D+%5Cunderset%7B%5Ctexttt%7Buser+1+KV+cache%7D%7D%7B%2811%5Ctext%7BG+bytes%7D%7D+%2B+%5Cunderset%7B%5Ctexttt%7Buser+2+KV+cache%7D%7D%7B11%5Ctext%7BG+bytes%7D%29%7D+%2F+%5C%3B%5Cunderset%7B%5Ctexttt%7BPCIE+bandwidth%7D%7D%7B%7B20%5Ctext%7BG+bytes+per+sec%7D%7D%7D%5Cnotag%5Cend%7Balign%7D" /> </p><p>在图1中，我们将1.1秒四舍五入为2秒，以考虑工程开销。如前所述，在我们的设置中，我们可以为4 K上下文长度的20个用户提供服务，而无需进行上下文切换，因为HBM足以容纳他们的KV缓存。如果我们想将我们的50 K并发性增加到20，那么随着并发性的增加，整体上下文切换开销也会增加：</p><p><img alt="\begin{align}    \underset{\texttt{Context switching for 20 users for 50K}}{22\;\text{seconds}} = \underset{\texttt{concurrency}}{20 \;\texttt{users}} \times \underset{\texttt{context switching between 2 users}}{1.1\;\text{seconds}} \notag\end{align}" src="https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%7D++++%5Cunderset%7B%5Ctexttt%7BContext+switching+for+20+users+for+50K%7D%7D%7B22%5C%3B%5Ctext%7Bseconds%7D%7D+%3D+%5Cunderset%7B%5Ctexttt%7Bconcurrency%7D%7D%7B20+%5C%3B%5Ctexttt%7Busers%7D%7D+%5Ctimes+%5Cunderset%7B%5Ctexttt%7Bcontext+switching+between+2+users%7D%7D%7B1.1%5C%3B%5Ctext%7Bseconds%7D%7D+%5Cnotag%5Cend%7Balign%7D" /> </p><p>20个用户的50K上下文的上下文切换在2个用户之间切换</p><p>这22秒的开销在4 K上下文范围内不存在，但在长上下文范围内会变得棘手。</p><h2>1.7 - 到目前为止的总结</h2><p>我们已经讨论了使用34 B模型50 K上下文作为运行示例部署长上下文transformers时的大部分细节。我们看到，整体性能，以单位时间内用户交互会话数量来衡量，可以分解为四个重要指标：</p><ul><li>由GPU flops限制的预填充延迟；</li><li>由HBM带宽限制的解码延迟；</li><li>由HBM大小限制的并发级别。</li><li>GPU-CPU连接带宽限制的上下文切换开销，即PCIe。</li></ul><p>在接下来的章节中，我们将讨论这些指标如何随着上下文长度和硬件架构的变化而变化，并最终确定瓶颈追溯到KV缓存的大小。</p><h2>2 - 强烈影响性能指标的因素</h2><p>我们从两个基本因素开始：上下文长度和硬件架构。当从4 K增加到50 K时，我们展示了四个指标（预填充、解码、并发和上下文切换）随不同速率（线性、反比和二次）变化。我们进一步表明张量并行性提高了并发性、预填充和解码，但并不改善上下文切换。总体性能如下所示：</p><figure><img class="origin_image zh-lightbox-thumb" src="https://pic2.zhimg.com/v2-6bb9c922728cb06567ff604537ecf44d_1440w.jpg" /></figure><p class="ztext-empty-paragraph"><br /></p><p>图2. 第一行：上下文长度如何改变四个关键性能指标。将上下文长度从4 K增加到50 K会反比减少并发性，二次增加预填充延迟，线性增加上下文切换开销，稍微（但也是线性地）增加解码延迟。第二行：不同硬件代的影响关键性能指标。并发性由并发用户数来衡量。预填充、解码和上下文切换延迟以秒为单位。</p><h2>2.1 - 上下文长度</h2><p>如图2的第一行所示，我们使用在前一节讨论的方程中计算了我们的Yi 34B示例运行的上下文长度从4 K到50 K的四个指标的理论峰值性能。我们观察到：</p><ul><li>随着更长的上下文长度，并发性反比减少；</li><li>随着更长的上下文长度，预填充延迟二次增加。</li><li>相比之下，解码延迟和上下文切换开销只是线性增加随着更长的上下文长度，解码延迟是最不受影响的因素，因为50 K上下文KV缓存仍然相对较小于模型参数（11GB对比68GB）。</li></ul><p>总体而言，并发性和预填充是两个受影响最严重的因素。</p><h2>2.2 - 硬件架构</h2><p>我们能否通过简单使用更先进的硬件来提高性能？在图2的第二行中，我们展示了随着硬件进步性能改善趋势。我们观察到：</p><ul><li>并发性随着HBM大小的增加而线性增加；</li><li>随着设备从4090升级到A100再到H100时，预填充延迟随着FLOPS的增加而反比减少；</li><li>随着内存带宽的增加，解码延迟随之反比减少。</li><li>上下文切换开销随着增加的PCIE带宽而逆向减少。请注意，我们使用的数字基于2024年5月的最新进展，即使我们使用最新的硬件，50K美元和4K美元之间的成本差距也没有缩小。换句话说，我们不能指望硬件进步来降低为长上下文模型提供服务的成本，我们必须进行算法创新。</li></ul><h2>2.3 - 使用张量并行性的多GPU</h2><p>正如Kipply的博客所讨论的，利用多个设备加速推理，通信开销可以忽略不计。一般来说：</p><ul><li>将设备数量线性增加到2、4和8会引入更多的HBM空间，从而线性增加并发性。</li><li>由于我们在多个设备上均匀分配计算，因此预填充和解码延迟也会随GPU数量的逆向减少。</li><li>然而，张量并行性无法减少上下文切换开销，因为DDR到HBM之间的PCIE带宽被所有设备共享。</li></ul><h2>3 - 可压缩性分析和现有工作</h2><p>到目前为止，我们在比较50K美元和4K美元时有以下重要观察：</p><ul><li>为了预填充长输入并生成KV缓存，预填充延迟从0.89增加到14.1秒；</li><li>由于大型KV缓存驻留在GPU内存中，并发性从约20减少到1；</li><li>在解码过程中，重复加载KV缓存会将延迟从8.5增加到9.8秒；</li><li>大型KV缓存引起昂贵的上下文切换开销，对于20个用户，额外需要约22秒（方程式17）。这四个因素共同导致了端到端基于会话的吞吐量方面的显着成本。</li></ul><p>如果我们能够无损减少预填充时间并压缩KV缓存，我们可能会显著降低为长上下文模型提供服务的成本。我们的最终目标是使部署1M上下文的成本像4K美元一样便宜，而4K美元的标记约为1GB的KV缓存。然后，我们的观察指向一个关键的研究问题：</p><ul><li>如何有效地将1M标记的KV缓存压缩到1G字节而不丢失信息？</li></ul><p>我们首先注意到，没有任何压缩，将1M标记存储为字节只需要约3-10MB的磁盘存储（取决于分词器词汇表的大小），因此1GB足以存储输入标记的全部信息。问题在于如何使它们的压缩表示可供大型transformer使用。</p><p>从业者通常会在各种长上下文任务上测试模型，以检查压缩是否有损失，其中“草堆中的针”测试要求模型精确地检索放置在长上下文任意位置的给定信息，作为一个入门障碍：如果一个模型无法通过这个测试，我们就不相信它能够完成更难的任务。不幸的是，似乎两个重要的模型系列，状态空间模型（例如Mamba）和线性注意力（例如LongT5），无法通过针测试，因此我们不将它们纳入我们的讨论范围。我们最近的工作表明，存在一组特殊的注意力头负责从上下文中检索重要信息。它们的发现表明，至少对于某些层和某些头部，应保留对大多数输入标记的全注意力 - 这些注意力头可能不太可压缩。下面我们讨论KV缓存的可压缩性，从其四个维度：层、头、标记和隐藏，以及现有工作如何改进长上下文推理。C：并发，P：预填充，D：解码，S：上下文切换：</p><table><tbody><tr><th></th><th>描述</th><th>改进</th><th>Needle(大海捞针)？</th></tr><tr><td>层</td></tr><tr><td>CALM</td><td>基于估计置信度的早期退出</td><td>C|P|D|S</td><td>？</td></tr><tr><td>Colt5</td><td>有条件地减少某些层上的计算</td><td>C|P|D|S</td><td>？</td></tr><tr><td>LayerSkip</td><td>跳过一些层然后验证</td><td>C|P|D|S</td><td>？</td></tr><tr><td>YOCO</td><td>仅使用一个全局KV缓存</td><td>C|P|D|S</td><td>✓</td></tr><tr><td>头</td></tr><tr><td>GQA</td><td>为一组头部重复使用KV缓存</td><td>C|D|S</td><td>✓</td></tr><tr><td>检索头</td><td>移除非检索头</td><td>C|D|S</td><td>✓</td></tr><tr><td>MLA</td><td>使用潜在头</td><td>C|P|D|S</td><td>✓</td></tr><tr><td>标记</td><td></td><td></td><td></td></tr><tr><td>H2O</td><td>在预填充后丢弃不重要的标记</td><td>C|D|S</td><td>？</td></tr><tr><td>FastGen</td><td>在预填充期间识别重要的标记</td><td>C|D|S</td><td>？</td></tr><tr><td>DMC</td><td>动态合并标记</td><td>C|P|D|S</td><td>？</td></tr><tr><td>SnapKV</td><td>根据用户问题识别重要的标记</td><td>D</td><td>✓</td></tr><tr><td>TriForce</td><td>长上下文的推测解码</td><td>D</td><td>✓</td></tr><tr><td>隐藏</td></tr><tr><td>KIVI</td><td>KV缓存量化</td><td>C|D|S</td><td>？</td></tr><tr><td>WKVQuant</td><td>权重和KV缓存量化</td><td>C|D|S</td><td>？</td></tr></tbody></table><p class="ztext-empty-paragraph"><br /></p><h2>2.1 - 层</h2><p>对于层维度，基本假设是一些任务可能不需要完整深度的计算。在预填充过程中跳过一些层可能对所有四个指标都有益，因为这同时减少了预填充的浮点运算次数和KV缓存的大小。实际上，层维度可能会从像Memorizing Transfomers和YOCO这样的现有作品的结果中被根本性地减少，对于长上下文任务可能只需要保留一个层KV缓存，这是一个1/60的压缩比率。</p><h2>2.2 - 头</h2><p>对于头维度，基本假设是一些头部专门用于检索和长上下文相关能力，因此可能保留检索头部并修剪其他头部。需要注意的是头部修剪通常发生在预填充之后，这意味着它们只会改善解码、并发性和上下文切换，但预填充仍然是昂贵的。总的来说，在头维度上存在非常高的稀疏性，头的数量可能会被根本性地减少到非常小的数字，例如，我们展示最强检索头的数量少于20个，这可能导致一个20/1024的压缩比率。</p><h2>2.3 - 标记</h2><p>对于标记维度，基本假设是如果可以从上下文中推断出标记的信息，我们可以通过删除或与邻近标记合并来压缩该标记。在标记维度上的大部分压缩并不会显著改善预填充，但它们通常会改善并发性、解码和上下文切换。目前看来，标记维度可能不像层和头维度那样稀疏，因为大多数标记必须保留以进行精确检索。我们尚未看到任何工作展示标记维度上超过50%的压缩比率的潜力。</p><h2>2.4 - 隐藏</h2><p>除了量化之外，对于进一步减少维度的工作并不多，这可能是因为隐藏大小已经是128，太小了无法进一步减少。然而，尝试在KV缓存上应用像LoRA这样的维度减少仍然值得，特别是考虑到最近来自DeepSeek V2的进展，该进展引入了类似LoRA的想法，有效地减少了KV头的大小。</p><p>这里的一个重要观察是许多现有作品可能只强调问题的一个方面。例如，TriForce [18]只考虑使用推测解码的解码延迟。它没有使KV缓存变小，甚至从草案模型中增加了GPU内存消耗的权衡。许多现有作品也是正交的，因此它们从不同方面的优势可能会汇聚力量。例如，如果我们将KV缓存减少到只有1层或10个头部，并且仅保留50%的标记，我们将获得约1000倍的性能提升。这自然引出了一个研究呼吁：</p><ul><li>我们是否可以将现有努力整合到一个端到端系统中，并推动全栈优化？</li></ul><h2>3 - 结论：朝着端到端系统的全栈优化迈进</h2><p>在本文中，我们对部署长上下文的挑战进行了详细分析。</p>
]]></content:encoded>
<pubDate>Sat, 11 May 2024 13:01:06 GMT</pubDate>
</item>
<item>
<title>强化学徒发布了想法: 其实反过来看，如果自媒体宣传的足够猛，可能会带来更好的生源(毕竟多数高中生和家长可能并不能看到负面影响)[飙泪笑] <br /> <a href="https...&amp;lt;/title&amp;gt;
&amp;lt;link&amp;gt;https://www.zhihu.com/pin/1772440774988619777&amp;lt;/link&amp;gt;
&amp;lt;guid&amp;gt;https://www.zhihu.com/pin/1772440774988619777&amp;lt;/guid&amp;gt;
&amp;lt;content:encoded&amp;gt;&amp;lt;![CDATA[
&amp;lt;div&amp;gt; 自媒体、宣传、生源、高中生、家长
&amp;lt;br&amp;gt;
自媒体宣传足够猛可能带来更好的生源。多数高中生和家长可能无法看到负面影响，从而受到影响。因此，加强自媒体宣传，可能是吸引更多生源的有效策略。 &amp;lt;div&amp;gt;
&amp;lt;p&amp;gt;其实反过来看，如果自媒体宣传的足够猛，可能会带来更好的生源(毕竟多数高中生和家长可能并不能看到负面影响)[飙泪笑] &amp;lt;br /&amp;gt; &amp;lt;a class="></title>
<link>https://www.zhihu.com/pin/1772440774988619777</link>
<guid>https://www.zhihu.com/pin/1772440774988619777</guid>
<content:encoded><![CDATA[
<div> 自媒体、宣传、生源、高中生、家长
<br />
自媒体宣传足够猛可能带来更好的生源。多数高中生和家长可能无法看到负面影响，从而受到影响。因此，加强自媒体宣传，可能是吸引更多生源的有效策略。 <div>
<p>其实反过来看，如果自媒体宣传的足够猛，可能会带来更好的生源(毕竟多数高中生和家长可能并不能看到负面影响)[飙泪笑] <br /> <a class="internal" href="https://www.zhihu.com/question/655550363/answer/3494129786">中科大被美商务部列入实体清单，此前 18 所中国高校已在内，对留学影响有多大？会引发高考生报考热情吗？</a></p>
]]></content:encoded>
<pubDate>Fri, 10 May 2024 17:40:11 GMT</pubDate>
</item>
<item>
<title>强化学徒发布了想法: 鉴于最近关于自动驾驶事故频发，普通人有权知道危险源在哪儿。</title>
<link>https://www.zhihu.com/pin/1771853038028156928</link>
<guid>https://www.zhihu.com/pin/1771853038028156928</guid>
<content:encoded><![CDATA[
<p>鉴于最近关于自动驾驶事故频发，普通人有权知道危险源在哪儿。</p><p><img src="https://pic4.zhimg.com/100/v2-04073919aa9828094e2f7be1e3e70e5f_720w.jpg" /></p>
]]></content:encoded>
<pubDate>Thu, 09 May 2024 02:44:43 GMT</pubDate>
</item>
<item>
<title>强化学徒赞同了回答: 如何看待 DeepSeek 发布的 MoE 大模型 DeepSeek-V2？</title>
<link>https://www.zhihu.com/question/655172528/answer/3490374123</link>
<guid>https://www.zhihu.com/question/655172528/answer/3490374123</guid>
<content:encoded><![CDATA[
<p>很少见在框架上做改动的工作了，只能说不愧是幻方</p><p>说一下创新点，核心在低秩降维。传统的attention是直接将x映射到qkv的head dim，这里先将x映射到latent c，然后再通过c映射回到qkv的head dim</p><p>这么做有个什么好处呢？这种低秩结构很显然的好处就是计算量小了，原来是d×d的复杂度，现在变成了2×d×c，自然小了很多。</p><p>但这还只是一个附加的好处。这么做的真正好处在于kv cache，原先的kv cache需要存下完整的kv向量，这番操作之后，只需要保存低维的c，在用到的时候再通过两个权重矩阵映射回k，v就可以，这大大的减少了kv cache的空间占用</p><p>（而且这个思路似乎确实没有看到过类似的工作，直接cache x，而不是k和v，这样不就可以节省一半的空间？虽然牺牲一点计算量。有知道的也可以说一下）</p><p>另外一个也算比较创新的地方，把原先的head dim拆成两部分，只在其中一部分进行位置编码，另一部分不动。细节在后面补充</p><p>moe分了共享和专用，细粒度moe，也算比较新了</p><hr /><p>补充一个点，MLA是MHA的变种，不是现在比较广泛的xxQA，所以在理论上是强于MQA，GQA模型的</p><hr /><p>来解释一下为什么要在rope的时候对head dim拆分</p><figure><img class="origin_image zh-lightbox-thumb lazy" src="https://pic3.zhimg.com/v2-bae4e56d2d33b2beabcab99bf05324ee_1440w.jpg" /></figure><p>然后我们知道attention可以写成下面这种形式</p><p><img alt="attn=softmax(q k^{T})=softmax(xW^{q}(W^{k})^{T}x^{T})=softma(xW^{qk}x^T)" src="https://www.zhihu.com/equation?tex=attn%3Dsoftmax%28q+k%5E%7BT%7D%29%3Dsoftmax%28xW%5E%7Bq%7D%28W%5E%7Bk%7D%29%5E%7BT%7Dx%5E%7BT%7D%29%3Dsoftma%28xW%5E%7Bqk%7Dx%5ET%29" /> </p><p>其中W_qk是一个d×d的矩阵，因为在MLA中，cache的是c，通过c实时计算并利用k和v，所以能够进行这种操作。在inference阶段，这番操作下来可以提高的效率，还能减少参数占用</p><p>这时就面临一个问题，我对k加上的rope在这里就无了，因为k=xW_k，包含了W_k</p><p>于是这里就提出一个绝妙的想法，将k的dim拆分，取一大部分作为c，一小部分作为k_r，维度为r，由于rope的良好性质（赞美苏神），可以通过仅对k_r进行位置编码的方式将相对位置信息包含在内，这样在cache的时候额外将k_r存起来，然后在计算attn的时候分开计算，于是</p><p><img alt="attn=sm(q_ck_c^T+q_rk_r^T)=sm(c_qW_{qk}c^T+c_qW_{qr}k_r^T)" src="https://www.zhihu.com/equation?tex=attn%3Dsm%28q_ck_c%5ET%2Bq_rk_r%5ET%29%3Dsm%28c_qW_%7Bqk%7Dc%5ET%2Bc_qW_%7Bqr%7Dk_r%5ET%29" /> </p><p>其中W_qr是将c_q映射到到r的矩阵，c是cache存下的latent，k_r是额外存下的含有位置编码的key，W_qk是将W_q和W_k融合得到的。因为c_q是h通过降维得到的，c_q=hW_dq，W_dq是将h降维为c_q的参数矩阵，所以又可以写成</p><p><img alt="attn=sm(hWc^T+hW_rk_r^T)" src="https://www.zhihu.com/equation?tex=attn%3Dsm%28hWc%5ET%2BhW_rk_r%5ET%29" /> </p><p>其中，W是将W_dq, W_q, W_k三者融合在一起得到的矩阵，W_r是将W_dq, W_qr融合在一起得到的矩阵。这番操作之下，我们只需要cache一个c，一个k_r，就能同时保证：节省cache的空间，加速inference的速度，以及位置编码的质量</p><p>然后，这里就要提出一个问题：在MLA的attention中，传统意义上的W_q, W_k, W_v全部成了低秩矩阵的形式，那么低秩矩阵对满秩矩阵的替换，是否会影响到注意力部分的性能呢？在Deepseek的这份报告里虽然没有着重去提及，但效果上看起来是没有的，我相信Deepseek做出这种大胆的替换，一定是基于了一些前期实验结论的。换言之，这说明了一个可能的结论——attention模块内的权重矩阵具有较大的低秩特性，因而能够进行这种替换，attention中的满秩权重矩阵很可能就是冗余的</p><hr /><p>更新：感谢评论区指出已有对kv cache低秩压缩的工作，这里搜索了一下论文，找到一个相关工作GEAR</p><a class=" wrap external" href="https://arxiv.org/pdf/2403.05527" rel="nofollow noreferrer" target="_blank">2403.05527 (arxiv.org)</a><p>这个工作的做法是采用一个稀疏矩阵+量化+低秩分解对kv进行压缩，然后cache压缩后的kv。文中做到的压缩比率为3倍，并且在一众压缩方法中取得了sota</p><p>不过对比MLA中，减小了相比于传统MHA <img alt="\frac{9}{4n_{head}}" src="https://www.zhihu.com/equation?tex=%5Cfrac%7B9%7D%7B4n_%7Bhead%7D%7D" /> 倍的夸张的cache的空间复杂度面前，这种做法顿时就没有那么香了。只要Attention head大于7，MLA节省的cache空间就已经超过GEAR了，何况现在的模型动不动就几十上百个head。估计这也是Deepseek自信1元一百万tokens的来源吧</p><hr /><p>更新：这几天都在跟MLA打交道。研究了一下HF社区里的代码之后基本确定，这跟论文和跑在云服务上的代码不是一版。论文里面提到的merge weight，以及cache latent都是没在开源代码里面体现出来的，毕竟merge了之后要重写forward，cache也要重写，这么多优化做完都可以直接搬上服务器直接当deepseek v2用了，开源社区要紧的是能跑和兼容，肯定没精力把这些顾上。</p><p>其实这样想的话，其他的开源代码也未必就是他们自己工业上使用的代码，真的上服务器，肯定都会先merge weight，如果memory bound比较多的话估计也私底下把kv cache换成了h cache</p>
]]></content:encoded>
<pubDate>Wed, 08 May 2024 06:03:16 GMT</pubDate>
</item>
<item>
<title>强化学徒回答了问题: 如何看待 DeepSeek 发布的 MoE 大模型 DeepSeek-V2？</title>
<link>https://www.zhihu.com/question/655172528/answer/3490692813</link>
<guid>https://www.zhihu.com/question/655172528/answer/3490692813</guid>
<content:encoded><![CDATA[
<p>下次在没有实测之前，不能随便暴论了～</p><p>目前中文的指令跟随能力还有待提升～</p><figure><img class="origin_image zh-lightbox-thumb lazy" src="https://pic4.zhimg.com/v2-41b8ff5ec8628a3cdefae8046da9752b_1440w.jpg" /></figure><p></p>
]]></content:encoded>
<pubDate>Tue, 07 May 2024 12:52:45 GMT</pubDate>
</item>
<item>
<title>强化学徒发布了想法: 羡慕高飞老师的学生~</title>
<link>https://www.zhihu.com/pin/1771180099343409154</link>
<guid>https://www.zhihu.com/pin/1771180099343409154</guid>
<content:encoded><![CDATA[
<p>羡慕高飞老师的学生~</p><p><img src="https://pic2.zhimg.com/100/v2-65ed969e857723626e13603b1bf3b67d_720w.jpg" /></p>
]]></content:encoded>
<pubDate>Tue, 07 May 2024 06:10:42 GMT</pubDate>
</item>
<item>
<title>强化学徒发表了文章: 学术版gpt已经支持自带术语库翻译-教程</title>
<link>https://zhuanlan.zhihu.com/p/696279747</link>
<guid>https://zhuanlan.zhihu.com/p/696279747</guid>
<content:encoded><![CDATA[
<p>大家在翻译论文的时候，应该会经常遇到一些突兀的翻译，如果用端到端的直接翻译，手动修改替换就比较麻烦，所以我简单优化了一下术语库的支持。</p><p>效果非常的nice，arxiv原生翻译和PDF2PDF，以及markdown翻译都是可以的。 术语库的输入格式为json，你可以把你们领域一些比较重要的术语塞进去，比如我们机器人+RL+LLM领域，就有下面的一些常见术语：</p><blockquote>{  "agent": "智能体",  "environment": "环境",  "state": "状态",  "action": "动作",  "reward": "奖励",  "policy": "策略",  "value": "价值",  "model": "模型",  "exploration": "探索",  "exploitation": "利用",  "model-free": "无模型",  "model-based": "有模型",  "value-based": "基于价值",  "policy-based": "基于策略",  "transition": "转移元组",  "buffer": "经验池",  "replay": "回放",  "experience": "经验",  "episode": "回合",  "epoch": "轮次",  "zero-shot": "零样本",  "one-shot": "一次样本",  "few-shot": "少样本",  "long horizon": "长视角",  "demonstration": "示教",  "teleoperation": "遥操作",  "simulation": "仿真",      "token": "标记",      "transformer": "transformer",  "actor-critic": "actor-critic",  "off-policy": "off-policy",  "on-policy": "on-policy"  }</blockquote><p>值得注意的是，有一些术语不需要翻译，你映射为原单词即可，LLM也是能听懂的。</p><p>具体操作为：</p><figure><img class="origin_image zh-lightbox-thumb" src="https://pic3.zhimg.com/v2-a979c098eaee6ace5d1fb75b55e0944e_1440w.jpg" /></figure><p>比如：</p><figure><img class="origin_image zh-lightbox-thumb" src="https://pic4.zhimg.com/v2-95ab067325c3d759da9d1d1ad8d74f87_1440w.jpg" /><figcaption>如果是非arxiv的翻译，则不需要下面那段中文。</figcaption></figure><p>术语库到底怎么来？</p><p>这得看你们的专业积累了~</p><p>学术版gpt的免费网址：<a class=" wrap external" href="https://academic.chatwithpaper.org/" rel="nofollow noreferrer" target="_blank">GPT 学术优化</a></p><p>如果有反馈，欢迎加群：816116844</p>
]]></content:encoded>
<pubDate>Tue, 07 May 2024 00:55:22 GMT</pubDate>
</item>
<item>
<title>强化学徒发表了文章: 2024年AI领域最值得关注的博主和一手信息源盘点</title>
<link>https://zhuanlan.zhihu.com/p/682110383</link>
<guid>https://zhuanlan.zhihu.com/p/682110383</guid>
<content:encoded><![CDATA[
<p>2024.03.19：更新。新增7个B站up主，删掉一个微信公众号，</p><hr /><p>读博五年积累的信息获取渠道，分享给大家。</p><p>起因是昨天刷到一个【2024拒绝信息差！AI领域最值得关注的博主，优质信息良心推荐-哔哩哔哩】 <a class=" external" href="https://b23.tv/QhYLyDK" rel="nofollow noreferrer" target="_blank"><span class="invisible">https://</span><span class="visible">b23.tv/QhYLyDK</span><span class="invisible"></span></a></p><p>感觉，我也可以做一个类似的，分享给大家之后，可以起到一个同步信息源的作用，避免信息差和信息茧房。</p><p>由于我的时间有限，并且大家的注意力有限，我接下来，用最简单的关键词和一句话点评，供大家参考。</p><p>结构分为：知乎博主，B站up主，公众号，推特大V，一些垂直类AI网站。</p><hr /><p><b>点评纯主观片面，如果我的点评不对，欢迎指正，立刻修改！</b></p><hr /><h3>知乎博主：</h3><p>这里我直接根据我的关注列表，按照我的关注顺序介绍。大家找到感兴趣的博主之后，点开他们的动态，同样可以一挖挖一窝，哈哈。</p><h3>泛AI领域：</h3><ol><li><a class="internal" href="https://www.zhihu.com/people/bopengbopeng">PENG Bo</a>：rwkv作者，分享内容主要是rwkv的进展，暴论比较多，更新频率正常</li><li><a class="internal" href="https://www.zhihu.com/org/huggingface">Hugging Face</a>：Huggingface face官方号，会分享一些基础技术贴，工作日更新频率比较高。</li><li><a class="internal" href="https://www.zhihu.com/people/ding-xiao-yi-93">丁霄汉</a>：清华博士，现在腾讯AI lab。主要分享一些关于学术写作、审稿、cv、AI圈八卦和暴论。阅读起来比较开心。</li><li><a class="internal" href="https://www.zhihu.com/people/wan-shang-zhu-ce-de">毛航宇</a>：北大博士，前华为诺亚，现商汤。之前做MARL，现在弃坑，去做LLM Agent了（哈哈，本质都是多智能体），主要分享这两个方向的学术进展，个人见解，以及相关八卦。</li><li><a class="internal" href="https://www.zhihu.com/people/zhao-ytc">电光幻影炼金术</a>：上交博士，大佬的方向很杂，我刷了一圈都定位到具体专业。主要分享全领域学术进展、读研读博教程（包括写作、投稿、审稿、师生关系等），更新频率比较高。</li><li><a class="internal" href="https://www.zhihu.com/people/jackgethome">HeptaAI</a>：加州伯克利。表征学习 &amp; 强化学习。主要分享一些偏硬核的强化、LLM相关的帖子，一些个人点评，个人学习经历，喜欢硬核点的，比较值得看。</li><li><a class="internal" href="https://www.zhihu.com/people/who-u">何枝</a>：电子科大，现在字节。大佬是分享RLHF教程和代码讲解火出圈的，做相关工作的可以关注一下。不过在字节工作，应该是比较难输出了。</li><li><a class="internal" href="https://www.zhihu.com/people/xin-xi-men-xia-de-pao-gou">信息门下跑狗</a>：北大。跑姐不用多说，重拳出击学术造假，但最近更新频率也下降了。</li><li><a class="internal" href="https://www.zhihu.com/people/su-jian-lin-22">苏剑林</a>：苏佬去年从追一去月之暗面了。玩知乎的，苏佬应该不用多介绍了。虽然大本营在他自己的科学空间，后面也会介绍，但知乎刷起来会更方便一些。唯一难受的是，苏佬很多帖子都需要一定的数理基础才能看懂，我基本上都是收藏吃灰。</li><li><a class="internal" href="https://www.zhihu.com/people/wang-xiao-wei-64-66">王小惟 Weixun</a>：天大博士，现在网易伏羲。传统多智能体强化、LLM（RLHF）、游戏AI。做强化的关注就完事儿了。</li><li><a class="internal" href="https://www.zhihu.com/people/jiayi-weng">Trinkle</a>：清华，现在openai做RLHF。苹果哥也不用多介绍了，天授作者，chatgpt第一版RLHF他和同事炼的，偶尔会分享一些相关信息，关注就完事儿了。</li><li><a class="internal" href="https://www.zhihu.com/people/eyounx">俞扬</a>：南大教授。坚持offline RL落地，最近一年经常输出RL和RLHF的个人“暴论”，值得大家关注。</li><li><a class="internal" href="https://www.zhihu.com/people/tian-yuan-dong">田渊栋</a>：Meta FAIR研究院研究员，CMU机器人博士。之前做MARL，做长文本小说生成，以及现在做LLM，大佬非常强，在知乎的干货输出也很多，直接关注就行。</li><li><a class="internal" href="https://www.zhihu.com/people/rainstorm-53">曹越</a>：清华博士，<a class="internal" href="https://www.zhihu.com/question/492057377/answer/2169776709">swin transformer</a>作者。做cv，视频这块的内容可以关注一下大佬，主要分享一些学术进展和学术八卦。</li><li><a class="internal" href="https://www.zhihu.com/people/youngfish42">白小鱼</a>：上交。联邦学习相关干货知识分享，以及推荐各种LLM相关讯息。</li><li><a class="internal" href="https://www.zhihu.com/people/splitter">贱贱</a>：不用多介绍了，虽然我不是很认可他的观点，但学术八卦还是得看他，比如这次薛鹏教授的瓜。</li><li><a class="internal" href="https://www.zhihu.com/people/wang-feng-98-82">王峰</a>：TuSimple。看的不是很多，主要是自动驾驶相关的信息。</li><li><a class="internal" href="https://www.zhihu.com/people/dong-lin-zhong-sheng-76">东林钟声</a>：华科博士。博士方向是RL+灵巧手。现在主要研究LLM+灵巧手。大佬的干货比例和更新频率都比较好。</li><li><a class="internal" href="https://www.zhihu.com/people/sikila">王鹏程</a>：中科大。博主的想法会分享最新arxiv论文的图文介绍，刷起来很舒服。建议关注。</li><li><a class="internal" href="https://www.zhihu.com/people/li-bo-jie">李博杰</a>：科大博士，前华为天少。师兄的知乎分享频率非常高，质量同样高，长文干货贴+脑洞+个人见解。领域几乎包含全AI领域，值得大家关注。</li><li><a class="internal" href="https://www.zhihu.com/people/xie-ling-xi">谢凌曦</a>：清华，华为。盘古天气预报的作者，还有其他的就不介绍了，AI领域关注就行了。</li><li><a class="internal" href="https://www.zhihu.com/people/rumor-lee">李rumor</a>：北航，现在美团做RLHF。其实公众号是她的主战场，会分享一些有趣的AI知识。但感觉在美团做RLHF，已经占用了她太多的时间了，更新频率没那么高了。</li><li><a class="internal" href="https://www.zhihu.com/people/soulteary">苏洋</a>：大佬的经历一长串，没法介绍，哈哈。泛AI领域的资讯，关注就行了。</li><li><a class="internal" href="https://www.zhihu.com/people/ai--53-32">AI 小舟哥</a>：Huggingface 的大佬。知乎的分享好像不算多，但微信朋友圈的资讯都是一手最新的，关注我，我会分享最好的，哈哈。</li><li><a class="internal" href="https://www.zhihu.com/people/huangzhe">桔了个仔</a>：AI领域大佬答主了。泛AI领域的咨询。</li><li><a class="internal" href="https://www.zhihu.com/people/show-me-ai">ShowMeAI</a>：乔sir维护的AI日报。一个AI信息的整合平台。</li><li><a class="internal" href="https://www.zhihu.com/people/albert-chen-4">北方的郎</a>：LLM相关的知识分享和点评。</li><li><a class="internal" href="https://www.zhihu.com/people/mu-yao-12-34">穆尧</a>：港大博士生。大佬主要是做LLM，Robot（具身智能），diffusion Policy相关的内容。</li><li><a class="internal" href="https://www.zhihu.com/people/LiuCongNLP">刘聪NLP</a>：《ChatGPT原理与实战》作者。LLM的学术和行业信息。</li><li><a class="internal" href="https://www.zhihu.com/people/hai-tan-shang-chong-hua-47">OpenLLMAI</a>：浙大硕士。OpenRLHF作者，之前主要分享一些LLM相关的知识。现在输出较少，但转发评论比较多。</li><li><a class="internal" href="https://www.zhihu.com/people/zi-qi-dong-lai-1">紫气东来</a>: 上交硕士。干货很多，llm论文和项目原创分享。更新频率高。值得关注。</li><li><a class="internal" href="https://www.zhihu.com/people/zhang-jun-lin-76">张俊林</a>：新浪微博新技术研发负责人，知乎深度学习优秀答主。大佬从GPT2.0就开始在知乎分享LLM相关内容了，主要分享一些深度的思考（太硬核了，我之前一篇都没读进去，哈哈）。</li><li><a class="internal" href="https://www.zhihu.com/people/loveQt">段小草</a>：西电。主要分享一些最新的LLM行业产品分析和干货测评，大佬手速非常快。</li><li><a class="internal" href="https://www.zhihu.com/people/xiaohuzc">小小将</a>：泛AI领域答主，一时半会儿我总结不了。</li><li><a class="internal" href="https://www.zhihu.com/people/flood-sung">Flood Sung</a>：月之暗面。大佬之前做机器人、强化学习，现在在月之暗面做kemichat。现在大段干货分享频率没有之前那么高了，但点评比较多。</li></ol><h3>强化领域：</h3><ol><li><a class="internal" href="https://www.zhihu.com/people/hao-jian-ye-tian-jin-da-xue-51">强化学习实验室</a>：天大郝建业老师组的知识分享平台。强化学习领域必须关注！他们分享的论文都比较重要，他们写的帖子也非常深入浅出。输出频率不算高，但比较稳定。</li><li><a class="internal" href="https://www.zhihu.com/people/zhao-jian-2-54">赵鉴</a>：科大博士，现在南栖仙策带RL团队。主要分享游戏+RL的知识、RL的就业技巧，以及就业现状，观点往往一针见血。</li><li><a class="internal" href="https://www.zhihu.com/people/baichenjia">白辰甲</a>：哈工大博士，现在上海AIlab。之前主要做强化，不少教程贴，现在主要做四足和人形。博士毕业之后，知乎大段干货分享频率较低。</li><li><a class="internal" href="https://www.zhihu.com/people/zhang-chu-heng">张海抱</a>：清华博士。主要分享一些RL的最新论文解读。现在博士毕业之后，知乎号的分享频率要下降不少。</li><li><a class="internal" href="https://www.zhihu.com/people/ceng-yi-yan-8">曾伊言</a>：小雅的作者。RL领域的同学关注就行了。尽管更新频率已经下降很多了。</li></ol><h3>机器人领域：</h3><ol><li><a class="internal" href="https://www.zhihu.com/people/dong-lin-zhong-sheng-76">东林钟声</a>：华科博士。博士方向是RL+灵巧手。现在主要研究LLM+灵巧手。大佬的干货比例和更新频率都比较好。</li><li><a class="internal" href="https://www.zhihu.com/people/yyss2037">YY硕</a>：卡内基梅隆大学博士。机器人领域的优质答主，关注就完事儿了。输出频率不高，但每个帖子都值得认真阅读。</li><li><a class="internal" href="https://www.zhihu.com/people/li-miao-8-1">李淼robot</a>：EPFL( 瑞士洛桑联邦理工)·博士，现在武汉某高校（隐约记得是武大）。李老师是机器人领域的优质答主，之前有比较多的教程贴，现在更新频率较低。只能说工作之后，大家水知乎的时间越来越少了</li></ol><h3>B站up:</h3><ol><li><a class=" wrap external" href="https://space.bilibili.com/5760446/" rel="nofollow noreferrer" target="_blank">花儿不哭</a>：RVC变声器创始人，GPT-sovits作者，关注声音复刻的关注就完事儿了。</li><li><a class=" wrap external" href="https://space.bilibili.com/241286257" rel="nofollow noreferrer" target="_blank">风信子的猫Redamancy</a>：<span class="nolink">数字人对话系统 Linly-Talker</span>。</li><li><a class=" wrap external" href="https://space.bilibili.com/39089748" rel="nofollow noreferrer" target="_blank">李自然说</a>：AI连续创业者，对业界的思考很有价值。</li><li><a class=" wrap external" href="https://space.bilibili.com/19319172/?spm_id_from=333.999.0.0" rel="nofollow noreferrer" target="_blank">差评君</a>：一些AI领域评测和分享，范围较广。</li><li><a class=" wrap external" href="https://space.bilibili.com/1732848825" rel="nofollow noreferrer" target="_blank">耿同学讲故事</a>：北航老哥，战斗力非常猛，下饭利器！</li><li><a class=" wrap external" href="https://space.bilibili.com/49975325/?spm_id_from=333.999.0.0" rel="nofollow noreferrer" target="_blank">机器人科学与技术</a>：会分享最新的一些国际大组的机器人演示demo，但没有做更多点评。</li><li><a class=" wrap external" href="https://space.bilibili.com/257271972/?spm_id_from=333.999.0.0" rel="nofollow noreferrer" target="_blank">浙大高飞！</a>：古中国掌管无人机的爷！</li><li><a class=" wrap external" href="https://space.bilibili.com/371846699/?spm_id_from=333.999.0.0" rel="nofollow noreferrer" target="_blank">图灵的猫</a>：有朋友吐槽他的视频不太严谨，但我个人认为还是可以作为开拓视野来看的。</li><li><a class=" wrap external" href="https://space.bilibili.com/23947287/?spm_id_from=333.999.0.0" rel="nofollow noreferrer" target="_blank">小约翰可汗</a>：说到下饭视频，必须得有可汗，哈哈。</li><li><a class=" wrap external" href="https://space.bilibili.com/1010101551/?spm_id_from=333.999.0.0" rel="nofollow noreferrer" target="_blank">来自星星的何教授</a>：室温超导+学术八卦跑的最快的up~</li><li><a class=" wrap external" href="https://space.bilibili.com/393702473/?spm_id_from=333.999.0.0" rel="nofollow noreferrer" target="_blank">落英行者</a>：各种尖端行业深度解析，很好奇素材都是哪儿来的。</li><li><a class=" wrap external" href="https://space.bilibili.com/357669580?spm_id_from=333.337.0.0" rel="nofollow noreferrer" target="_blank">萌萌战队</a>：空气动力学，激波！最像营销号的干货号。</li><li><a class=" wrap external" href="https://space.bilibili.com/475312678/?spm_id_from=333.999.0.0" rel="nofollow noreferrer" target="_blank">二进制哈士奇</a>：学术版GPT的作者，分享学术版GPT最新的功能。</li><li><a class=" wrap external" href="https://space.bilibili.com/431556168/?spm_id_from=333.999.0.0" rel="nofollow noreferrer" target="_blank">浪子之心科技</a>：数字人，AIGC开源项目介绍。</li><li><a class=" wrap external" href="https://space.bilibili.com/1572312/?spm_id_from=333.999.0.0" rel="nofollow noreferrer" target="_blank">李鲁鲁</a>：AIGC、LLM角色扮演、论文分享，大佬的知乎我忘记贴了！</li><li><a class=" wrap external" href="https://space.bilibili.com/12566101/?spm_id_from=333.999.0.0" rel="nofollow noreferrer" target="_blank">秋葉aaaki</a>：AI绘图界的喂饭区UP主，狠狠关注！</li><li><a class=" wrap external" href="https://space.bilibili.com/615957867/?spm_id_from=333.999.0.0" rel="nofollow noreferrer" target="_blank">五里墩茶社</a>：最新的LLM相关工具分享，很多新工具都有新手入门，值得关注。</li><li><a class=" wrap external" href="https://space.bilibili.com/1369507485/?spm_id_from=333.999.0.0" rel="nofollow noreferrer" target="_blank">ShusenWang</a>：王老师的强化学习课和推荐系统课，都是免费的，讲的非常好！（虽然我都没看过，群友说的）</li><li><a class=" wrap external" href="https://space.bilibili.com/314022607/?spm_id_from=333.999.0.0" rel="nofollow noreferrer" target="_blank">王树义老师</a>：一些新AI工具的使用分享。比较适合小白。</li><li><a class=" wrap external" href="https://space.bilibili.com/295428344/?spm_id_from=333.999.0.0" rel="nofollow noreferrer" target="_blank">霍华德vlog</a>：华叔出走知乎，去了B站，现在主要分享rwkv的内容，以及一些泛AI的信息。</li><li><a class=" wrap external" href="https://space.bilibili.com/1567748478/?spm_id_from=333.999.0.0" rel="nofollow noreferrer" target="_blank">跟李沐学AI</a>：关注就行，最近老师创业去了，断更了。</li><li><a class=" wrap external" href="https://space.bilibili.com/604515161/?spm_id_from=333.999.0.0" rel="nofollow noreferrer" target="_blank">RLChina强化学习社区</a>：强化人必须关注！</li><li><a class=" wrap external" href="https://space.bilibili.com/344849038?spm_id_from=333.337.0.0" rel="nofollow noreferrer" target="_blank">YJango(于建国)</a>：于建国博士，现在在西电当老师，他对AI和认知的思考比较深刻，视频做的通俗易懂，推荐关注。</li></ol><h3>0319新增：</h3><ol><li><a class=" wrap external" href="https://space.bilibili.com/373596439/?spm_id_from=333.999.0.0" rel="nofollow noreferrer" target="_blank">deep_thoughts</a>：LLM和Diffusion经典论文解读和Pytorch源码解读；</li><li><a class=" wrap external" href="https://space.bilibili.com/59807853/?spm_id_from=333.999.0.0" rel="nofollow noreferrer" target="_blank">五道口纳什</a>：LLM+diffusion+RL+docker开发等教程；非常全~</li><li><a class=" wrap external" href="https://space.bilibili.com/823532/?spm_id_from=333.999.0.0" rel="nofollow noreferrer" target="_blank">秋刀鱼的炼丹工坊</a>：CV+LLM等最新论文速览；</li><li><a class=" wrap external" href="https://space.bilibili.com/13611123/?spm_id_from=333.999.0.0" rel="nofollow noreferrer" target="_blank">投研部杨摸鱼</a>：商科生，在投研工作，以最好看的方式、最通俗的语言，介绍，芯片、人形机器人相关的知识背景和底层逻辑。</li><li><a class=" wrap external" href="https://space.bilibili.com/2458477/?spm_id_from=333.999.0.0" rel="nofollow noreferrer" target="_blank">贯一智能科技</a>：泛AI领域的科普账号，内容做的比较好，具身智能，模型测评，论文导读各种栏目；</li><li><a class=" wrap external" href="https://space.bilibili.com/3031494/?spm_id_from=333.999.0.0" rel="nofollow noreferrer" target="_blank">刘悦的技术博客</a>：语音合成领域非常活跃的大佬，视频更新频率离谱的高。</li><li><a class=" wrap external" href="https://space.bilibili.com/81249447/?spm_id_from=333.999.0.0" rel="nofollow noreferrer" target="_blank">老司机耿进财</a>：科研和学术领域的基础规则，导师和师兄师姐都不会跟你说这么明白的事情，现在你可以免费听得到。</li></ol><h3>微信公众号：</h3><blockquote>标注别人公众号有软广，有点不合适，我先删了，大家关注的时候自己注意下。</blockquote><h3>泛AI领域：</h3><p>首先是中文三顶会！</p><p><b>量子位、机器之心、新智元</b>。</p><p>别管什么标题党，什么小编不懂AI，谁都不能否认他们是泛AI的顶流自媒体。他们确实会分享一些最新的比较火的资讯，关注，每天刷一遍标题，就完事儿了。</p><p>然后介绍一些其他的泛AI媒体：</p><ol><li>36氪：和LLM、AI关系没那么大，但也是传统科技媒体了。</li><li>Z Potentials：LLM，AIGC创业投资相关资讯。</li><li>爱可可爱生活：其实微博才是大佬的主战场，大佬会分享一些最新的论文，如果有点评就更好了。</li><li>数字生命卡兹克：最新AI产品分析、AI技术干货整理、原创AI应用分享。泛AI领域，啥都玩的大佬。</li><li>李rumor：强化，大模型相关资讯分享，招聘信息发布。</li><li>AI科技评论：CSDN旗下的公众号，相当于是弱版的三顶会。（哈哈，这么评价会不会得罪人？）</li><li>将门创投：干货较多，但更新频率不高，能有办法直接邀请论文作者做免费分享。</li></ol><p>NLP领域：</p><ol><li>老刘说NLP：NLP相关的干货内容。</li><li>JioNLP：作者喜欢开源，喜欢分享各种AI知识，干货比例会比较高。</li><li>NewBeeNLP：nlp相关内容，最新学术、技术贴。</li><li>GithubDaily：会介绍一些Github热门的项目，现在主要是LLM相关的内容。</li><li>这个信息质量太差了，我删掉了。</li></ol><h3>机器人领域的：</h3><ol><li>机器人大讲堂：清华孙富春老师团队维护的平台，机器人必关注，干货多。</li><li>九章智驾：自动驾驶领域的“顶会”。</li></ol><h3>计算机视觉领域：</h3><ol><li>我爱计算机视觉：主要会介绍一些最新论文解读。</li><li>AIwalker：cv相关的论文分享，干货比例高。</li><li>cver：相当于cv界的顶会，哈哈。</li><li>计算机视觉life：主要是自动驾驶、slam相关的视觉。</li></ol><h3>游戏领域：</h3><ol><li>游戏葡萄：干货比较多，平台能接触到很多一线大厂的资源。</li></ol><p><b>公众号太难推荐了，大家帮忙分享一些~</b></p><h3>优质博客、网页推荐：</h3><ol><li><a class=" wrap external" href="https://lilianweng.github.io/" rel="nofollow noreferrer" target="_blank">Lil'Log</a>：lilian 小姐姐应该放第一个！看她的博客，基本上可以把一个领域，系统的梳理清楚。通俗易懂，深入浅出！前几天刚出一个博客，摁头安利！</li><li><a class=" wrap external" href="https://arxiv.org/search/?query=large+language+model&amp;searchtype=all&amp;abstracts=show&amp;order=-submitted_date&amp;size=50" rel="nofollow noreferrer" target="_blank">LLM-Arxiv</a>：关于LLM的最新Arxiv论文列表，有空刷一下。这是原始信息源。下面的cool papers是进阶版。</li><li><a class=" wrap external" href="https://papers.cool/" rel="nofollow noreferrer" target="_blank">Cool Papers - Immersive Paper Discover</a>：苏剑林大佬开发的一个刷论文的网站，虽然我感觉直接看英文还是不够舒服，但群友们的反馈还挺好的。 </li><li><a class=" wrap external" href="https://spaces.ac.cn/" rel="nofollow noreferrer" target="_blank">科学空间|Scientific Spaces</a>：苏佬的科学空间不多介绍了。</li><li><a class=" wrap external" href="https://huggingface.co/papers" rel="nofollow noreferrer" target="_blank">Daily Papers - Hugging Face</a>：由Huggingface的AK大佬亲自维护的一个论文日榜，但对中文用户不太友好。</li><li><a class=" wrap external" href="https://github.com/trending?spoken_language_code=" rel="nofollow noreferrer" target="_blank">Github Trending</a>：Github热榜，程序员必刷，祝大家早日登榜！</li><li><a class=" wrap external" href="https://news.mit.edu/" rel="nofollow noreferrer" target="_blank">MITNews</a>：应该是国内科技自媒体的上游信息源了。</li><li><a class=" external" href="https://paperswithcode.com/sota" rel="nofollow noreferrer" target="_blank"><span class="invisible">https://</span><span class="visible">paperswithcode.com/sota</span><span class="invisible"></span></a>：一些领域的sota方法排行榜。</li><li><a class=" wrap external" href="https://www.runoob.com/" rel="nofollow noreferrer" target="_blank">菜鸟教程 - 学的不仅是技术，更是梦想！</a>：拓展技术栈比较好的网站，无广告，非常舒服。</li></ol><h3>播客：</h3><p>现在播客好像也成了新的战场，推荐几个干货非常多的博主，都在“小宇宙app”（@小宇宙，快把广子给我，哈哈）：</p><ol><li>42章经：关注就完事儿了。都是直接访谈AI领域最一线的大佬，纯干货。</li><li>AI局内人：同上。</li><li>屠龙之术：同上；</li><li>张小珺：商业访谈录，最近深度采访月之暗面的杨植麟，王小川等大佬；</li></ol><p>好吧，我暂时就关注了这两个，欢迎大家补充。</p><p class="ztext-empty-paragraph"><br /></p><h3>了解一个领域的常见技巧：</h3><ol><li>谷歌学术搜关键词：找到survey，或者引用数比较高的论文，然后用ChatPaper总结，或者学术版GPT免费翻译，快速阅读。</li><li>如果是最新的论文：Arxiv搜关键词。</li><li>看paperwithcode的排行榜，比如：<a class=" external" href="https://paperswithcode.com/sota" rel="nofollow noreferrer" target="_blank"><span class="invisible">https://</span><span class="visible">paperswithcode.com/sota</span><span class="invisible"></span></a></li><li>如果是代码复现，可以Github搜：<b>awesome+xxx</b>,一般会有大佬给你整理好相关的资料。</li></ol><p class="ztext-empty-paragraph"><br /></p><h3>推：</h3><p>直接参考这个就好了：</p><p><a class=" wrap external" href="https://hub.baai.ac.cn/view/24040" rel="nofollow noreferrer" target="_blank">107 个值得关注的AI Twitter，61 个工具和 28 个通讯 - 智源社区</a></p><p class="ztext-empty-paragraph"><br /></p><p>欢迎大家评论区分享自己关注的其他优秀答主！</p><p>希望大家在收藏的同时，记得点赞和评论！</p>
]]></content:encoded>
<pubDate>Sun, 25 Feb 2024 17:55:19 GMT</pubDate>
</item>
</channel>
</rss>