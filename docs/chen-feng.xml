<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
<channel>
<title>星际码仔的知乎动态</title>
<link>https://www.zhihu.com/people/chen-feng-40-67-10/activities</link>

<item>
<title>星际码仔赞同了回答: 《楚门的世界》中，妻子既然不爱楚门为什么反复提及要与楚门生一个孩子？</title>
<link>https://www.zhihu.com/question/305864895/answer/2400604543</link>
<guid>https://www.zhihu.com/question/305864895/answer/2400604543</guid>
<content:encoded><![CDATA[
<div> 楚门的世界, 孩子, 荒谬, 反抗, 社会稳定,
<br>
反抗荒谬的勇气会因为有孩子而消失，西方哲学史上探讨个体在虚幻世界中的救赎问题，存在主义哲学家提出不同回答，安部公房在小说《砂女》中强化荒谬概念，楚门的世界可能延续了《砂女》逻辑，孩子在文本中具有象征意义，政府对年轻人不生孩子的问题头疼，因为孩子关乎社会稳定。总之，有孩子会牵绊个体，引发不同的思考与行为。<br> <br>总结: <div>
<p><b>因为楚门一旦有了孩子，即使有一天他发现了这个世界的荒谬性，他也会接受荒谬，丧失追求自由的勇气。</b></p><p>这并非我臆想，而是有着很明显的证据。因为《楚门的世界》中所指向的核心问题即是：个体如何在虚幻的世界获得救赎。</p><p>这一问题贯穿于几千年西方哲学史的始终。</p><p>西方哲学史上最伟大的哲学家柏拉图曾提出一个流传了几千年的的洞穴假说：</p><p> 他说：在一个地穴中有一批囚徒；他们自小呆在那里，被锁链束缚，不能转头，只能看面前洞壁上的影子。在他们后上方有一堆火，有一条横贯洞穴的小道；沿小道筑有一堵矮墙，如同木偶戏的屏风。有一些特定的人，扛着各种器具走过墙后的小道，而火光则把透出墙的器具投影到囚徒面前的洞壁上，这些器具就是根据现实中的实物所做的模型。</p><p>囚徒自然地认为影子是惟一真实的事物。如果他们中的一个囚徒碰巧获释，转过头来看到了火光与物体，他最初会感到眩晕（就像才从电影院走出来一样），但是没有关系，他会慢慢适应。此时他看到有路可走，便会逐渐走出洞穴，看到阳光下的真实世界，此时，他会意识到以前所生活的世界只不过是一个洞穴，而以前所认为的真实事物也只不过是影像而已。</p><p>也即是说在柏拉图看来，我们活着的现实世界是不真实的，那么面对人如何在这种假象中获得救赎就是一个真正的哲学问题。可以说，西方哲学史上不同哲学家对于这一问题的回答侧重点都不一。到了近代以来，存在主义的早期哲学家将这一问题强化。</p><p>在克尔凯格尔哪里表现为人如何实现对于上帝义务与伦理冲突的跳跃，尼采，舍斯托夫，萨特都给出了回答。然而我现在依旧认为以加缪给出的回答最为精辟。他认为人生的意义就在反抗荒谬，从而实现自由。</p><p>然而日本的存在主义文学家安部公房并不认为加缪给出的救赎方案可以一劳永逸。</p><p>     据说安部是村上春树和大江健三郎共同的偶像。他试图将加缪所提出荒谬概念本身进行强化。<b>也即是说在安部公房看来，加缪的荒谬还是力道不足，毕竟推石头有主客关系，于是他在小说《砂女》中继续将这一问题推进，塑造了一副极其荒谬的场景。</b></p><p><b>《砂女》中描写了一个因收集昆虫标本而误入沙村的男人。他面对着沙村里日复一日重复的挖沙劳动，感到前所未有的荒谬。他尝试了许多种方式逃跑，但最终还是失败了。后来直到他和这个沙坑里的女人有了小孩子之后，在女人被送往医院之后，他本来有逃跑的机会，但是他已经认命了。从此再也丧失了反抗的勇气。</b></p><p>《砂女》以隐喻的方式揭示了现代人的荒诞命运。</p><p>   《砂女》这部小说后来被改编成了电影，1964年《电影旬报》年度十佳电影奖第一名，并获得奥斯卡奖提名。《楚门的世界》的导演不可能没有听说过这部同时享誉文学史与电影史的作品。</p><p>         甚至可以说《楚门的世界》本身就是对于《砂女》文本逻辑的继承与发挥。《楚门的世界》中妻子扮演的依旧是《砂女》中那个女人的角色，她不爱楚门，但是她知道只要一旦孩子降临，楚门至此就会丧失反抗的勇气。</p><p>墙的那一边固然是真实与自由，这一边世界虽然虚假，但是血浓于水的亲情却是真的。</p><p>《砂女》的结尾亦是如此，男人看作怀孕的女人被送往医院，第一次心中涌动着温情，即使他不爱这个女人，但是他突然对于这个荒谬的沙坑产生了留恋。于是他最终臣服于沙坑里的生活方式，丧失了反抗的初心。</p><p>    其实有孩子的成年人都可以理解这个情绪，<b>很多人年少时都是傲气轻狂，充满了反抗荒谬与追求自由的勇气。可是这份勇气是何时开始消减的，大致就是当你有了孩子之后。你凡事都会处处想着他，所以即使每天面对着996的劳累与荒谬，面对着上司的指责与同事的中伤，面对着日复一日的轮回，估计这些事，放在你年轻时，肯定会拍案而起，但是，此时你还是会默默地负重前行。</b></p><p><b>总之，你无论你做什么事都会想着孩子。由此就会有牵绊，有不舍，有留恋。</b></p><p> 这一切都是因为有了孩子。不论是《砂女》抑或是《楚门的世界》孩子都具有某种象征意义，只要孩子的降临，同时也意味着这个男人丧失反抗的可能性的开始。</p><p>     其实，这也是为什么政府现在为年轻人不生孩子极其头疼，因为生孩子不仅仅关系劳动力的问题，更关系社会稳定。要是一个社会的年轻人都不结婚，都不要孩子了，那他随时就可以像楚门一样推倒那堵墙。</p>
]]></content:encoded>
<pubDate>Sun, 01 Oct 2023 12:48:27 GMT</pubDate>
<pubDate>Sun, 01 Oct 2023 12:48:27 GMT</pubDate>
</item>
<item>
<title>星际码仔发表了文章: 精华笔记 - 吴恩达 x Hugging Face《使用Gradio构建生成式AI应用》</title>
<link>https://zhuanlan.zhihu.com/p/651217104</link>
<guid>https://zhuanlan.zhihu.com/p/651217104</guid>
<content:encoded><![CDATA[
<div> Python Gradio Hugging Face API 输入 输出
生成文本摘要 命名实体识别 识别图像内容 文本生成图像 聊天机器人
总结:
Python代码和Gradio框架结合，使用Hugging Face API，可以轻松实现输入输出功能。通过模型完成生成文本摘要、命名实体识别、图像识别、文本生成图像、聊天机器人等任务。利用小型专家模型和蒸馏技术，提高模型性能并降低成本，使得应用更加高效。Gradio提供了友好的交互界面，便于用户体验和模型调试。整体上，这些任务展示了人工智能在不同领域的应用前景和潜力。 <div>
<p>你是否有过开发一个AI应用的想法，并且迫不及待地想要验证其是否可行呢？</p><p>你可能已经写好了AI部分的代码逻辑，但是还需要一个能够向他人展示你的创意，并让他们能够亲身体验的用户界面。这样，你才能更有效地验证你的想法，并收集用户的反馈加以改进，从而提升你的系统。</p><figure><img class="origin_image zh-lightbox-thumb" src="https://pic3.zhimg.com/v2-4292473970d559da80148c2d9aa607c6_1440w.jpg" /></figure><p>如果你正在寻找这样的解决方案，那么<code>Gradio</code>就是你的理想选择。</p><p><b>Gradio是一个可以让你轻松、方便地使用Python代码构建一个友好Web交互界面的UI框架。它可以支持各种数据类型的输入和输出，还可以方便地与Hugging Face上的各种开源模型集成</b>。</p><p>在这门课程中，我们将向你展示如何通过Gradio，仅用几行代码搭建一个交互程序，并利用Hugging Face上提供的一些现有模型，完成以下五个有趣的任务：</p><ul><li><b>生成文本摘要</b>：从一篇长文本中抽取出核心信息，生成一个简洁、精确的摘要。</li><li><b>命名实体识别</b>：从文本中识别出人名、地名、组织名等实体，并给它们分类。</li><li><b>识别图像内容</b>：从一张图片中识别出物体、场景、动作等内容，并用自然语言描述出来。</li><li><b>文本生成图像</b>：根据一段描述性的文本，生成一张与之相符合的图片。</li><li><b>搭建基于LLM的聊天机器人</b>：创建一个能够与用户进行自然对话的聊天机器人，并根据用户提供的信息生成个性化的回答。</li></ul><p>如果你对这些任务感兴趣，那么请继续阅读本课程，一起探索Gradio和Hugging Face的魅力吧！</p><h2><b>NLP任务接口</b> </h2><figure><img class="origin_image zh-lightbox-thumb" src="https://pic2.zhimg.com/v2-017a7b777fc0e84932e59c960ea975b1_1440w.jpg" /></figure><p>在<a class=" wrap external" href="https://mp.weixin.qq.com/s/L3yj7suL7Z_opEsk9lqeMA" rel="nofollow noreferrer" target="_blank">前面的课程</a>中，我们使用的都是ChatGPT一类的通用大型语言模型，但对于某些特定任务（如生成文本摘要）来说，<b>使用一个专门针对该任务设计的小型专家模型（Small specialist model），也可以表现得与通用大型语言模型一样出色</b>。</p><p>另一方面，<b>小型专家模型可能还更便宜地运行，以及更快地响应用户</b>。</p><p>在开始我们的任务之前，我们仍需完成以下两个前置步骤：</p><h3><b>加载 API 密钥</b></h3><div class="highlight"><pre><code class="language-text">import os
# 首次运行，需执行以下指令：pip install python-dotenv
from dotenv import load_dotenv, find_dotenv

load_dotenv(find_dotenv()) # 读取本地.env文件
hf_api_key = os.environ['HF_API_KEY']
</code></pre></div><p><code>HF_API_KEY</code> 指的是 Hugging Face API 的访问密钥，可以通过以下步骤获取：</p><ol><li>访问 Hugging Face 官网：<a class=" external" href="https://huggingface.co/" rel="nofollow noreferrer" target="_blank"><span class="invisible">https://</span><span class="visible">huggingface.co/</span><span class="invisible"></span></a></li><li>鼠标移至右上角用户名-点击“Settings”。</li><li>点击“API tokens”选项。</li><li>点击“New token”按钮。</li><li>输入自定义的 API token 名称。</li><li>点击“Create new API token”，以生成一个新的 API token。</li><li>复制 API token 并保存到.env文件。</li></ol><h3><b>编写辅助函数</b></h3><div class="highlight"><pre><code class="language-text">import requests, json

def get_completion(inputs, parameters=None,ENDPOINT_URL=None):
    headers = {
      "Authorization": f"Bearer {hf_api_key}",
      "Content-Type": "application/json"
    }
    data = { "inputs": inputs }
    if parameters is not None:
        data.update({"parameters": parameters})
    response = requests.request("POST",
                                ENDPOINT_URL, headers=headers,
                                data=json.dumps(data)
                               )
    return json.loads(response.content.decode("utf-8"))
</code></pre></div><p>该辅助函数是用于以API请求的方式访问指定端点进行推理，以完成特定任务的。</p><p><b>Hugging Face 提供了一个<code>Inference API</code>，允许通过简单的 HTTP 请求，免费测试和评估超过 80,000 个可公开访问的机器学习模型或我们自己的私有模型，但有速率限制</b>。</p><div class="highlight"><pre><code class="language-text">ENDPOINT_URL = https://api-inference.huggingface.co/models/&lt;MODEL_ID&gt;
</code></pre></div><p><code>&lt;MODEL_ID&gt;</code> 表示我们要运行的模型，可以到 Hugging Face 的模型中心自由挑选适合我们应用业务场景的模型。</p><figure><img class="origin_image zh-lightbox-thumb" src="https://pic3.zhimg.com/v2-211e87ef047e633e71f562e62ca4f2ea_1440w.jpg" /></figure><p>除了小型专家模型外，另外一个降低成本并提高速度的方法，就是<b>基于大型模型来训练一个性能非常相似的较小模型</b>，这个过程称为「蒸馏（Distillation）」。</p><p>比如我们接下来将使用的 <code>shleifer/distilbart-cnn-12-6</code> ，就是一个来自于 <code>facebook/bart-large-cnn</code> 的拥有 306M 参数的蒸馏模型。</p><p><code>bart-large-cnn</code>是文本摘要领域最先进的模型之一，由 Facebook 训练而成。</p><h2><b>生成文本摘要</b></h2><p>在这一部分里，我们将从一篇长文本中抽取出核心信息，生成一个简洁、精确的摘要。</p><h3><b>步骤1：定义 summarise 的函数，接受输入，调用 getCompletion 函数，并返回摘要。</b></h3><div class="highlight"><pre><code class="language-text">API_URL = "https://api-inference.huggingface.co/models/sshleifer/distilbart-cnn-12-6"
def summarize(input):
    output = get_completion(input, parameters = None, ENDPOINT_URL = API_URL)
    return output[0]['summary_text']
</code></pre></div><h3><b>步骤2：使用 Gradio 的 interface 函数，传入 summarise 函数 ，并将其输入和输出均设置为文本。</b></h3><div class="highlight"><pre><code class="language-text"># 首次运行，需执行以下指令：pip install gradio
import gradio as gr
demo = gr.Interface(fn=summarize, inputs="text", outputs="text")
</code></pre></div><h3><b>步骤3：调用 demo.launch 创建用户界面。</b></h3><div class="highlight"><pre><code class="language-text">demo.launch()
</code></pre></div><p>运行结果如下：</p><figure><img class="origin_image zh-lightbox-thumb" src="https://pic2.zhimg.com/v2-bf7599e780a88dc1f82549ff61991bed_1440w.jpg" /></figure><h3><b>（可选）步骤4：进一步优化用户界面，如添加标签、制定行数、增加标题和描述等。</b></h3><div class="highlight"><pre><code class="language-text">demo = gr.Interface(fn=summarize, 
                    inputs=[gr.Textbox(label="Text to summarize", lines=6)],
                    outputs=[gr.Textbox(label="Result", lines=3)],
                    title="Text summarization with distilbart-cnn",
                    description="Summarize any text using the `shleifer/distilbart-cnn-12-6` model under the hood!"
                   )
</code></pre></div><p>优化结果如下：</p><figure><img class="origin_image zh-lightbox-thumb" src="https://pic2.zhimg.com/v2-41ff9bdb329216aae6a72fa526c6a4a9_1440w.jpg" /></figure><h2><b>命名实体识别</b></h2><p>在这一部分里，我们将从文本中识别出人名、地名、组织名等实体，并给它们分类。</p><figure><img class="origin_image zh-lightbox-thumb" src="https://pic2.zhimg.com/v2-abadfd68baf185a0367299dd2605166d_1440w.jpg" /></figure><p>这里使用到的是 <code>dslim/bert-base-NER</code> 模型，这是一个针对 NER (Named Entity Recognition，命名实体识别) 任务微调的包含 108M 参数 的 <code>BERT</code> 模型。</p><p><b>BERT 模型是一个用于自然语言处理的机器学习模型，该模型在解析一段文本时，可以识别出文本中包含的人名、组织名、地名等特定实体。</b></p><p>比如，当我们运行以下代码后，它就会输出一个包含多个字典的列表，每个字典都包含一个实体的信息。</p><div class="highlight"><pre><code class="language-text">API_URL = "https://api-inference.huggingface.co/models/dslim/bert-base-NER"
text = "My name is Andrew, I'm building DeepLearningAI and I live in California"
get_completion(text, parameters=None, ENDPOINT_URL= API_URL)

[{
 'entity_group': 'PER',
 'score': 0.9990624785423279,
 'word': 'Andrew',
 'start': 11,
 'end': 17
}, {
 'entity_group': 'ORG',
 'score': 0.896050214767456,
 'word': 'DeepLearningAI',
 'start': 32,
 'end': 46
}, {
 'entity_group': 'LOC',
 'score': 0.999692440032959,
 'word': 'California',
 'start': 61,
 'end': 71
}]
</code></pre></div><p>其中，entity_group键表示的含义分别是：</p><table><tbody><tr><th>键值</th><th>含义</th></tr><tr><td>PER</td><td>人物</td></tr><tr><td>ORG</td><td>组织机构</td></tr><tr><td>LOC</td><td>位置</td></tr></tbody></table><p>我们可以用 Gradio 让输出更加直观易懂：</p><h3><b>步骤1：定义 ner 的函数，接受输入，调用 getCompletion 函数，并返回实体列表。</b></h3><div class="highlight"><pre><code class="language-text">def ner(input):
    output = get_completion(input, parameters=None, ENDPOINT_URL=API_URL)
    for entity in output:
        entity["entity"] = entity['entity_group']
    return {"text": input, "entities": output}
</code></pre></div><h3><b>步骤2：使用 Gradio 的 interface 函数，传入 ner 函数 ，将输入设置为文本，输出设置为高亮文本。</b></h3><div class="highlight"><pre><code class="language-text">demo = gr.Interface(fn=ner,
                    inputs=[gr.Textbox(label="Text to find entities", lines=2)],
                    outputs=[gr.HighlightedText(label="Text with entities")],
                    title="NER with dslim/bert-base-NER",
                    description="Find entities using the `dslim/bert-base-NER` model under the hood!",
                    allow_flagging="never",
                    examples=["My name is Andrew and I live in California", "My name is Poli and work at HuggingFace"])
</code></pre></div><p><code>HighlightedText</code> 组件的作用是接收并高亮显示NER模型输出的的实体。</p><p><code>examples</code> 参数用于提供示例，帮助用户通快速了解程序是如何工作的。</p><h3><b>步骤3：调用 demo.launch 创建用户界面。</b></h3><div class="highlight"><pre><code class="language-text">demo.launch()
</code></pre></div><p>运行结果如下：</p><figure><img class="origin_image zh-lightbox-thumb" src="https://pic1.zhimg.com/v2-383ddf40c93722b89dd561e191921064_1440w.jpg" /></figure><figure><img class="origin_image zh-lightbox-thumb" src="https://pic3.zhimg.com/v2-1c425ccffbe2b42154a6fbdd9b059b4e_1440w.jpg" /></figure><h3><b>（可选）步骤4：将被拆分成多个标记的字符重组为一个完整的单词。</b></h3><p>同样在<a class=" wrap external" href="https://mp.weixin.qq.com/s/7MCc8QFihLSW5mJ9NyHZ3A" rel="nofollow noreferrer" target="_blank">前面的课程</a>中我们有介绍过，<b>LLM的处理单元不是一个个「单词」，而是一个个「标记（Token）」。它会接收一系列的字符，并将字符组合在一起，形成代表常见字符序列的标记。每个标记可能对应一个单词，或者空格，或者标点符号。</b></p><p>当我们选用另外一个示例时可以看到，HuggingFace这个单词会被分解为多个块，也即多个标记。</p><figure><img class="origin_image zh-lightbox-thumb" src="https://pic4.zhimg.com/v2-5e2527a8914049439a83d8ea08bc5b73_1440w.jpg" /></figure><p>但是，由于我们可以从实体标签的开头字母判断单词的开头和中间部分：</p><table><tbody><tr><th>字母</th><th>含义</th></tr><tr><td>B</td><td>开始标记</td></tr><tr><td>I</td><td>中间标记</td></tr></tbody></table><p>因此，我们可以定义一个 <code>merge_tokens</code> 函数，让每个标记在可视化时合并为一个完整单词显示，原理其实就是检查标签的开头字母，并进行合并：</p><div class="highlight"><pre><code class="language-text">def merge_tokens(tokens):
    merged_tokens = []
    for token in tokens:
        if merged_tokens and token['entity'].startswith('I-') and merged_tokens[-1]['entity'].endswith(token['entity'][2:]):
            # 如果当前 token 延续了上一个 token 的实体，则将它们合并
            last_token = merged_tokens[-1]
            last_token['word'] += token['word'].replace('##', '')
            last_token['end'] = token['end']
            last_token['score'] = (last_token['score'] + token['score']) / 2
        else:
            # 否则，将 token 添加到列表中
            merged_tokens.append(token)

    return merged_tokens
</code></pre></div><p>再次运行，就可以看到正确的结果了：</p><div class="highlight"><pre><code class="language-text">def ner(input):
    output = get_completion(input, parameters=None, ENDPOINT_URL=API_URL)
    merged_tokens = merge_tokens(output)
    return {"text": input, "entities": merged_tokens}
</code></pre></div><figure><img class="origin_image zh-lightbox-thumb" src="https://pic2.zhimg.com/v2-c0e4858b821b41e8fb1e605d3c8df18d_1440w.jpg" /></figure><h2><b>图像描述应用</b> </h2><p>在这一部分里，我们将从一张图片中识别出物体、场景、动作等内容，并用自然语言描述出来。</p><p>这里使用到的是<code>Salesforce/blip-image-captioning-base</code>模型，这是一个图像描述模型，可以将图像作为输入，并输出该图像的描述。</p><figure><img class="origin_image zh-lightbox-thumb" src="https://pic1.zhimg.com/v2-f71236518b5385d9bab2598a78184e84_1440w.jpg" /></figure><p>使用的免费图像来源于: <a class=" external" href="https://free-images.com/" rel="nofollow noreferrer" target="_blank"><span class="invisible">https://</span><span class="visible">free-images.com/</span><span class="invisible"></span></a></p><h3><b>步骤1：定义 captioner 的函数，接受图像，调用 getCompletion 函数，并返回图像描述。</b></h3><div class="highlight"><pre><code class="language-text">import io
import base64 
# 将图像转为API所需的Base64格式
def image_to_base64_str(pil_image):
    byte_arr = io.BytesIO()
    pil_image.save(byte_arr, format='PNG')
    byte_arr = byte_arr.getvalue()
    return str(base64.b64encode(byte_arr).decode('utf-8'))

API_URL = "https://api-inference.huggingface.co/models/Salesforce/blip-image-captioning-base"
def captioner(image):
    base64_image = image_to_base64_str(image)
    result = get_completion(base64_image, parameters=None, ENDPOINT_URL=API_URL)
    return result[0]['generated_text']
</code></pre></div><h3><b>步骤2：使用 Gradio 的 interface 函数，传入 captioner 函数 ，将输入设置为图像，输出设置为文本。</b></h3><div class="highlight"><pre><code class="language-text">demo = gr.Interface(fn=captioner,
                    inputs=[gr.Image(label="Upload image", type="pil")],
                    outputs=[gr.Textbox(label="Caption")],
                    title="Image Captioning with BLIP",
                    description="Caption any image using the BLIP model",
                    allow_flagging="never")
</code></pre></div><h3><b>步骤3：调用 demo.launch 创建用户界面。</b></h3><div class="highlight"><pre><code class="language-text">demo.launch()
</code></pre></div><p>运行结果如下：</p><figure><img class="origin_image zh-lightbox-thumb" src="https://pic3.zhimg.com/v2-41bdfc79a22a6bcbf0040fd62149d33e_1440w.jpg" /></figure><figure><img class="origin_image zh-lightbox-thumb" src="https://pic2.zhimg.com/v2-f7ab8095b973473a97818e01c7cc440d_1440w.jpg" /></figure><h2><b>图像生成应用</b> </h2><p>在这一部分里，我们将根据一段描述性的文本，生成一张与之相符合的图片。</p><figure><img class="origin_image zh-lightbox-thumb" src="https://pic4.zhimg.com/v2-4b588342130d7b08f7688d6547fd697b_1440w.jpg" /></figure><p>这里使用到的是<code>runwayml/stable-diffusion-v1-5</code>模型，也就是我们所熟知的<code>Stable Diffusion</code>图像生成模型，我们使用API URL连接到了这个模型的服务器。</p><figure><img class="origin_image zh-lightbox-thumb" src="https://pic1.zhimg.com/v2-91fb8d035810179742858cb76c747814_1440w.jpg" /></figure><h3><b>步骤1：定义 generate 的函数，接受文本描述，调用 getCompletion 函数，并返回图像。</b></h3><div class="highlight"><pre><code class="language-text"># 将PIL图像转换为base64的辅助函数
# 这样你就可以将其发送到API
def base64_to_pil(img_base64):
    base64_decoded = base64.b64decode(img_base64)
    byte_stream = io.BytesIO(base64_decoded)
    pil_image = Image.open(byte_stream)
    return pil_image

API_URL = "https://api-inference.huggingface.co/models/Salesforce/blip-image-captioning-base"
def generate(prompt):
    output = get_completion(prompt，parameters=None, ENDPOINT_URL=API_URL)
    result_image = base64_to_pil(output)
    return result_image
</code></pre></div><h3><b>步骤2：使用 Gradio 的 interface 函数，传入 generate 函数 ，将输入设置为文本，输出设置为图像。</b></h3><div class="highlight"><pre><code class="language-text">demo = gr.Interface(fn=generate,
                    inputs=[gr.Textbox(label="Your prompt")],
                    outputs=[gr.Image(label="Result")],
                    title="Image Generation with Stable Diffusion",
                    description="Generate any image with Stable Diffusion",
                    allow_flagging="never",
                    examples=["a dog in a park","a mecha robot in a favela"])

</code></pre></div><h3><b>步骤3：调用 demo.launch 创建用户界面。</b></h3><div class="highlight"><pre><code class="language-text">demo.launch()
</code></pre></div><p>运行结果如下：</p><figure><img class="origin_image zh-lightbox-thumb" src="https://pic2.zhimg.com/v2-7bae87c435035ca33f34185f8649ba05_1440w.jpg" /></figure><figure><img class="origin_image zh-lightbox-thumb" src="https://pic3.zhimg.com/v2-88f5ec843526b536d91e576295a7867a_1440w.jpg" /></figure><figure><img class="origin_image zh-lightbox-thumb" src="https://pic1.zhimg.com/v2-e30fad77e41c2d0b66ef4c6d27431f70_1440w.jpg" /></figure><h3><b>（可选）步骤4：增加更多输入选项，以构建一个更高级的界面。</b></h3><div class="highlight"><pre><code class="language-text">with gr.Blocks() as demo:
    gr.Markdown("# Image Generation with Stable Diffusion")
    with gr.Row(): # gr.Row()用于将组件水平排列
        with gr.Column(scale=4): # scale值用于调整所占宽度比例
            prompt = gr.Textbox(label="Your prompt") 
        with gr.Column(scale=1, min_width=50):
            btn = gr.Button("Submit") 
    with gr.Accordion("Advanced options", open=False): # open=false表示默认折叠隐藏
            negative_prompt = gr.Textbox(label="Negative prompt")
            with gr.Row():
                with gr.Column(): # gr.Column()用于将组件垂直排列
                    steps = gr.Slider(label="Inference Steps", minimum=1, maximum=100, value=25,
                      info="In many steps will the denoiser denoise the image?")
                    guidance = gr.Slider(label="Guidance Scale", minimum=1, maximum=20, value=7,
                      info="Controls how much the text prompt influences the result")
                with gr.Column(): 
                    width = gr.Slider(label="Width", minimum=64, maximum=512, step=64, value=512)
                    height = gr.Slider(label="Height", minimum=64, maximum=512, step=64, value=512)
    output = gr.Image(label="Result")
            
    btn.click(fn=generate, inputs=[prompt,negative_prompt,steps,guidance,width,height], outputs=[output])
</code></pre></div><p>这些额外输入选项的作用如下：</p><table><tbody><tr><th>标签</th><th>含义</th><th>类型</th><th>作用</th></tr><tr><td>Your prompt</td><td>提示</td><td>文本框</td><td>描述想要生成的图片内容</td></tr><tr><td>Negative prompt</td><td>负提示</td><td>文本框</td><td>描述不想生成的图片内容</td></tr><tr><td>Inference Steps</td><td>推理步骤</td><td>滑块控件</td><td>控制推理过程的执行步数，步数越多结果越精细</td></tr><tr><td>Guidance Scale</td><td>指导尺度</td><td>滑块控件</td><td>控制文本提示对结果的影响程度，值越大生成结果越贴合提示内容</td></tr><tr><td>Width</td><td>宽度</td><td>滑块控件</td><td>设置图片宽度，单位：像素</td></tr><tr><td>Height</td><td>高度</td><td>滑块控件</td><td>设置图片高度，单位：像素</td></tr></tbody></table><p>运行结果如下：</p><figure><img class="origin_image zh-lightbox-thumb" src="https://pic4.zhimg.com/v2-8709952b915edc01877b1b45580a995b_1440w.jpg" /></figure><h2><b>“描述与生成”游戏</b> </h2><p>在这一部分里，我们将结合前面两节的内容做一个游戏应用，首先识别一张图像的内容，再将识别出的内容作为描述生成另外一张图像。</p><h3><b>步骤1：引入第3课和第4课的函数，captioner 函数用于接受图像并返回图像描述；generate 的函数用于接收图像描述并返回图像。</b></h3><div class="highlight"><pre><code class="language-text">def image_to_base64_str(pil_image):
    byte_arr = io.BytesIO()
    pil_image.save(byte_arr, format='PNG')
    byte_arr = byte_arr.getvalue()
    return str(base64.b64encode(byte_arr).decode('utf-8'))

def base64_to_pil(img_base64):
    base64_decoded = base64.b64decode(img_base64)
    byte_stream = io.BytesIO(base64_decoded)
    pil_image = Image.open(byte_stream)
    return pil_image

def captioner(image):
    base64_image = image_to_base64_str(image)
    result = get_completion(base64_image, None, ITT_ENDPOINT)
    return result[0]['generated_text']

def generate(prompt):
    output = get_completion(prompt, None, TTI_ENDPOINT)
    result_image = base64_to_pil(output)
    return result_image
</code></pre></div><h3><b>步骤2：使用 Gradio 的 interface 函数，传入合并了两个步骤的 caption_and_generate 函数 ，将输入设置为图像，输出分别设置为一个文本和一个图像。</b></h3><div class="highlight"><pre><code class="language-text">def caption_and_generate(image):
    caption = captioner(image)
    image = generate(caption)
    return [caption, image]

with gr.Blocks() as demo:
    gr.Markdown("# Describe-and-Generate game  ️")
    image_upload = gr.Image(label="Your first image",type="pil")
    btn_all = gr.Button("Caption and generate")
    caption = gr.Textbox(label="Generated caption")
    image_output = gr.Image(label="Generated Image")

    btn_all.click(fn=caption_and_generate, inputs=[image_upload], outputs=[caption, image_output])

gr.close_all()
</code></pre></div><h3><b>步骤3：调用 demo.launch 创建用户界面。</b></h3><div class="highlight"><pre><code class="language-text">demo.launch()
</code></pre></div><p>运行结果如下：</p><figure><img class="origin_image zh-lightbox-thumb" src="https://pic1.zhimg.com/v2-576e213dac2ab7907bc49d726ebabd44_1440w.jpg" /></figure><figure><img class="origin_image zh-lightbox-thumb" src="https://pic2.zhimg.com/v2-082526a92ecf4ba6caf831bf185cfa85_1440w.jpg" /></figure><h2><b>与任意LLM聊天</b> </h2><p>在这一部分里，我们将创建一个能够与用户进行自然对话的聊天机器人，并根据用户提供的信息生成个性化的回答。</p><p>这里使用的到是<code>falcon-40b-instruct</code>模型，这是在<a class=" wrap external" href="https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard" rel="nofollow noreferrer" target="_blank">Open LLM Leaderboard</a>上排名最高的开源LLM之一。</p><h3><b>步骤1：初始化Client，以借助text_generation库访问FalcomLM-instruct推理端点。</b></h3><div class="highlight"><pre><code class="language-text">import requests, json
from text_generation import Client

client = Client(os.environ['HF_API_FALCOM_BASE'], headers={"Authorization": f"Basic {hf_api_key}"}, timeout=120)
</code></pre></div><h3><b>步骤2：定义 format_chat_prompt 函数，以格式化提示及对话历史</b></h3><div class="highlight"><pre><code class="language-text">def format_chat_prompt(message, chat_history, instruction):
    prompt = f"System:{instruction}"
    for turn in chat_history:
        user_message, bot_message = turn
        prompt = f"{prompt}\nUser: {user_message}\nAssistant: {bot_message}"
    prompt = f"{prompt}\nUser: {message}\nAssistant:"
    return prompt
</code></pre></div><p>其中涉及到三种角色消息我们也已经介绍过很多次了：</p><ul><li>系统消息（System）：负责指定LLM整体的语言风格或者助手的行为；</li><li>助手消息（Assistant）：负责根据用户消息要求内容，以及系统消息的设定，输出一个合适的回应；</li><li>用户消息（User）：给出一个具体的指令。</li></ul><p>而这一步骤的目的是为模型提供记忆功能，使模型能够在理解上下文的前提下，回答后续问题。</p><div class="highlight"><pre><code class="language-text">User: what is the meaning of life?
Assistant:

User: what is the meaning of life?
Assistant: I'm sorry, I cannot provide a definitive answer to that question.It is a philosophical question
User: but why?
Assistant:
</code></pre></div><h3><b>步骤3：定义 respond 函数，以调用 Client 的 generate 函数，将消息发送给模型，并将响应追加到对话历史。</b></h3><div class="highlight"><pre><code class="language-text">def respond(message, chat_history):
        formatted_prompt = format_chat_prompt(message, chat_history)
        bot_message = client.generate(formatted_prompt,
                                     max_new_tokens=1024,
                                     stop_sequences=["\nUser:", "&lt;|endoftext|&gt;"]).generated_text
        chat_history.append((message, bot_message))
        return "", chat_history
</code></pre></div><p>这里有一个问题。</p><p>随着这个过程的持续进行，我们发送给模型的对话历史会越来越多，直到模型达到一次对话中可以处理的最大标记数限制。</p><p>因此，我们选择在这里将最大标记数（max_new_tokens）参数设置为1024，这将保留最后1024个标记的对话历史。</p><p>要想详细这个参数的特点，可以参考之前<a class=" wrap external" href="https://mp.weixin.qq.com/s/KrFV-fuP2MI3HQGmEM1sUA" rel="nofollow noreferrer" target="_blank">《精华笔记：吴恩达 x LangChain《基于LangChain的大语言模型应用开发》(上) 》</a>一文中的<code>ConversationalTokenBufferMemory</code>。</p><p>而另外一个停止序列（stop_sequences）参数的作用，则是为了预防助手消息冒认用户消息，确保对话历史中的用户消息来自于真正的用户而不是来自于模型。</p><p>比如，当我们只向模型提问了这一句时：</p><figure><img class="origin_image zh-lightbox-thumb" src="https://pic4.zhimg.com/v2-b675e346693d1b763bbe7599377abf87_1440w.jpg" /></figure><p>模型可能会在回答的同时，构造多了一个虚假的提问：</p><figure><img class="origin_image zh-lightbox-thumb" src="https://pic1.zhimg.com/v2-1916c2f51a2fb357160f697a91e6bb80_1440w.jpg" /></figure><h3><b>步骤4：使用 Gradio 的 Chatbot 函数，快速构建一个聊天机器人组件</b></h3><div class="highlight"><pre><code class="language-text">with gr.Blocks() as demo:
    chatbot = gr.Chatbot(height=240)
    msg = gr.Textbox(label="Prompt")
    with gr.Accordion(label="Advanced options",open=False):
        system = gr.Textbox(label="System message", lines=2, value="A conversation between a user and an LLM-based AI assistant. The assistant gives helpful and honest answers.")
        temperature = gr.Slider(label="temperature", minimum=0.1, maximum=1, value=0.7, step=0.1)
    btn = gr.Button("Submit")
    clear = gr.ClearButton(components=[msg, chatbot], value="Clear console")

    btn.click(respond, inputs=[msg, chatbot, system], outputs=[msg, chatbot])
    msg.submit(respond, inputs=[msg, chatbot, system], outputs=[msg, chatbot]) #Press enter to submit

</code></pre></div><p>聊天机器人组件可以简化我们发送对话历史给模型的过程。</p><p>其中提供的一些额外输入选项的作用如下：</p><table><tbody><tr><th>标签</th><th>含义</th><th>类型</th><th>作用</th></tr><tr><td>System message</td><td>系统消息</td><td>文本框</td><td>告诉LLM如何与你交流，比如设定角色或调整语气</td></tr><tr><td>temperature</td><td>温度</td><td>滑块控件</td><td>为0时，模型对于相同输入会始终做出相同回答；值越大，模型给出的回答变化越大。</td></tr></tbody></table><h3><b>（可选）步骤5：让模型改用实时流式的方式传输答案</b></h3><p>在这种方式下，我们会将标记逐个发送，并实时地看到它的完整回答，因此不需要等待整个答案准备好，最终呈现的效果就是与ChatGPT一样逐词输出。</p><div class="highlight"><pre><code class="language-text">def respond(message, chat_history, instruction, temperature=0.7):
    prompt = format_chat_prompt(message, chat_history, instruction)
    chat_history = chat_history + [[message, ""]]
    stream = client.generate_stream(prompt,
                                      max_new_tokens=1024,
                                      stop_sequences=["\nUser:", "&lt;|endoftext|&gt;"],
                                      temperature=temperature)
                                      #stop_sequences to not generate the user answer
    acc_text = ""
    #Streaming the tokens
    for idx, response in enumerate(stream):
            text_token = response.token.text

            if response.details:
                return

            if idx == 0 and text_token.startswith(" "):
                text_token = text_token[1:]

            acc_text += text_token
            last_turn = list(chat_history.pop(-1))
            last_turn[-1] += acc_text
            chat_history = chat_history + [last_turn]
            yield "", chat_history
            acc_text = ""
</code></pre></div><h2><b>参考</b> </h2><p>Inference API-(<a class=" external" href="https://huggingface.co/docs/api-inference/index" rel="nofollow noreferrer" target="_blank"><span class="invisible">https://</span><span class="visible">huggingface.co/docs/api</span><span class="invisible">-inference/index</span><span class="ellipsis"></span></a>)</p>
]]></content:encoded>
<pubDate>Sun, 20 Aug 2023 13:54:17 GMT</pubDate>
<pubDate>Sun, 20 Aug 2023 13:54:17 GMT</pubDate>
</item>
<item>
<title>星际码仔回答了问题: 如何看待huggingface.co已无法访问？</title>
<link>https://www.zhihu.com/question/599683557/answer/3174677323</link>
<guid>https://www.zhihu.com/question/599683557/answer/3174677323</guid>
<content:encoded><![CDATA[

<p>你是否有过开发一个AI应用的想法，并且迫不及待地想要验证其是否可行呢？</p><p>你可能已经写好了AI部分的代码逻辑，但是还需要一个能够向他人展示你的创意，并让他们能够亲身体验的用户界面。这样，你才能更有效地验证你的想法，并收集用户的反馈加以改进，从而提升你的系统。</p><figure><img class="origin_image zh-lightbox-thumb lazy" src="https://pic3.zhimg.com/v2-4292473970d559da80148c2d9aa607c6_1440w.jpg" /></figure><p>如果你正在寻找这样的解决方案，那么<code>Gradio</code>就是你的理想选择。</p><p><b>Gradio是一个可以让你轻松、方便地使用Python代码构建一个友好Web交互界面的UI框架。它可以支持各种数据类型的输入和输出，还可以方便地与Hugging Face上的各种开源模型集成</b>。</p><p>在这门课程中，我们将向你展示如何通过Gradio，仅用几行代码搭建一个交互程序，并利用Hugging Face上提供的一些现有模型，完成以下五个有趣的任务：</p><ul><li><b>生成文本摘要</b>：从一篇长文本中抽取出核心信息，生成一个简洁、精确的摘要。</li><li><b>命名实体识别</b>：从文本中识别出人名、地名、组织名等实体，并给它们分类。</li><li><b>识别图像内容</b>：从一张图片中识别出物体、场景、动作等内容，并用自然语言描述出来。</li><li><b>文本生成图像</b>：根据一段描述性的文本，生成一张与之相符合的图片。</li><li><b>搭建基于LLM的聊天机器人</b>：创建一个能够与用户进行自然对话的聊天机器人，并根据用户提供的信息生成个性化的回答。</li></ul><p>如果你对这些任务感兴趣，那么请继续阅读本课程，一起探索Gradio和Hugging Face的魅力吧！</p><h2><b>NLP任务接口</b> </h2><figure><img class="origin_image zh-lightbox-thumb lazy" src="https://pic2.zhimg.com/v2-017a7b777fc0e84932e59c960ea975b1_1440w.jpg" /></figure><p>在<a class=" wrap external" href="https://mp.weixin.qq.com/s/L3yj7suL7Z_opEsk9lqeMA" rel="nofollow noreferrer" target="_blank">前面的课程</a>中，我们使用的都是ChatGPT一类的通用大型语言模型，但对于某些特定任务（如生成文本摘要）来说，<b>使用一个专门针对该任务设计的小型专家模型（Small specialist model），也可以表现得与通用大型语言模型一样出色</b>。</p><p>另一方面，<b>小型专家模型可能还更便宜地运行，以及更快地响应用户</b>。</p><p>在开始我们的任务之前，我们仍需完成以下两个前置步骤：</p><h3><b>加载 API 密钥</b></h3><div class="highlight"><pre><code class="language-text">import os
# 首次运行，需执行以下指令：pip install python-dotenv
from dotenv import load_dotenv, find_dotenv

load_dotenv(find_dotenv()) # 读取本地.env文件
hf_api_key = os.environ['HF_API_KEY']
</code></pre></div><p><code>HF_API_KEY</code> 指的是 Hugging Face API 的访问密钥，可以通过以下步骤获取：</p><ol><li>访问 Hugging Face 官网：<a class=" external" href="https://huggingface.co/" rel="nofollow noreferrer" target="_blank"><span class="invisible">https://</span><span class="visible">huggingface.co/</span><span class="invisible"></span></a></li><li>鼠标移至右上角用户名-点击“Settings”。</li><li>点击“API tokens”选项。</li><li>点击“New token”按钮。</li><li>输入自定义的 API token 名称。</li><li>点击“Create new API token”，以生成一个新的 API token。</li><li>复制 API token 并保存到.env文件。</li></ol><h3><b>编写辅助函数</b></h3><div class="highlight"><pre><code class="language-text">import requests, json

def get_completion(inputs, parameters=None,ENDPOINT_URL=None):
    headers = {
      "Authorization": f"Bearer {hf_api_key}",
      "Content-Type": "application/json"
    }
    data = { "inputs": inputs }
    if parameters is not None:
        data.update({"parameters": parameters})
    response = requests.request("POST",
                                ENDPOINT_URL, headers=headers,
                                data=json.dumps(data)
                               )
    return json.loads(response.content.decode("utf-8"))
</code></pre></div><p>该辅助函数是用于以API请求的方式访问指定端点进行推理，以完成特定任务的。</p><p><b>Hugging Face 提供了一个<code>Inference API</code>，允许通过简单的 HTTP 请求，免费测试和评估超过 80,000 个可公开访问的机器学习模型或我们自己的私有模型，但有速率限制</b>。</p><div class="highlight"><pre><code class="language-text">ENDPOINT_URL = https://api-inference.huggingface.co/models/&lt;MODEL_ID&gt;
</code></pre></div><p><code>&lt;MODEL_ID&gt;</code> 表示我们要运行的模型，可以到 Hugging Face 的模型中心自由挑选适合我们应用业务场景的模型。</p><figure><img class="origin_image zh-lightbox-thumb lazy" src="https://pic3.zhimg.com/v2-211e87ef047e633e71f562e62ca4f2ea_1440w.jpg" /></figure><p>除了小型专家模型外，另外一个降低成本并提高速度的方法，就是<b>基于大型模型来训练一个性能非常相似的较小模型</b>，这个过程称为「蒸馏（Distillation）」。</p><p>比如我们接下来将使用的 <code>shleifer/distilbart-cnn-12-6</code> ，就是一个来自于 <code>facebook/bart-large-cnn</code> 的拥有 306M 参数的蒸馏模型。</p><p><code>bart-large-cnn</code>是文本摘要领域最先进的模型之一，由 Facebook 训练而成。</p><h2><b>生成文本摘要</b></h2><p>在这一部分里，我们将从一篇长文本中抽取出核心信息，生成一个简洁、精确的摘要。</p><h3><b>步骤1：定义 summarise 的函数，接受输入，调用 getCompletion 函数，并返回摘要。</b></h3><div class="highlight"><pre><code class="language-text">API_URL = "https://api-inference.huggingface.co/models/sshleifer/distilbart-cnn-12-6"
def summarize(input):
    output = get_completion(input, parameters = None, ENDPOINT_URL = API_URL)
    return output[0]['summary_text']
</code></pre></div><h3><b>步骤2：使用 Gradio 的 interface 函数，传入 summarise 函数 ，并将其输入和输出均设置为文本。</b></h3><div class="highlight"><pre><code class="language-text"># 首次运行，需执行以下指令：pip install gradio
import gradio as gr
demo = gr.Interface(fn=summarize, inputs="text", outputs="text")
</code></pre></div><h3><b>步骤3：调用 demo.launch 创建用户界面。</b></h3><div class="highlight"><pre><code class="language-text">demo.launch()
</code></pre></div><p>运行结果如下：</p><figure><img class="origin_image zh-lightbox-thumb lazy" src="https://pic2.zhimg.com/v2-bf7599e780a88dc1f82549ff61991bed_1440w.jpg" /></figure><h3><b>（可选）步骤4：进一步优化用户界面，如添加标签、制定行数、增加标题和描述等。</b></h3><div class="highlight"><pre><code class="language-text">demo = gr.Interface(fn=summarize, 
                    inputs=[gr.Textbox(label="Text to summarize", lines=6)],
                    outputs=[gr.Textbox(label="Result", lines=3)],
                    title="Text summarization with distilbart-cnn",
                    description="Summarize any text using the `shleifer/distilbart-cnn-12-6` model under the hood!"
                   )
</code></pre></div><p>优化结果如下：</p><figure><img class="origin_image zh-lightbox-thumb lazy" src="https://pic2.zhimg.com/v2-41ff9bdb329216aae6a72fa526c6a4a9_1440w.jpg" /></figure><h2><b>命名实体识别</b></h2><p>在这一部分里，我们将从文本中识别出人名、地名、组织名等实体，并给它们分类。</p><figure><img class="origin_image zh-lightbox-thumb lazy" src="https://pic2.zhimg.com/v2-abadfd68baf185a0367299dd2605166d_1440w.jpg" /></figure><p>这里使用到的是 <code>dslim/bert-base-NER</code> 模型，这是一个针对 NER (Named Entity Recognition，命名实体识别) 任务微调的包含 108M 参数 的 <code>BERT</code> 模型。</p><p><b>BERT 模型是一个用于自然语言处理的机器学习模型，该模型在解析一段文本时，可以识别出文本中包含的人名、组织名、地名等特定实体。</b></p><p>比如，当我们运行以下代码后，它就会输出一个包含多个字典的列表，每个字典都包含一个实体的信息。</p><div class="highlight"><pre><code class="language-text">API_URL = "https://api-inference.huggingface.co/models/dslim/bert-base-NER"
text = "My name is Andrew, I'm building DeepLearningAI and I live in California"
get_completion(text, parameters=None, ENDPOINT_URL= API_URL)

[{
 'entity_group': 'PER',
 'score': 0.9990624785423279,
 'word': 'Andrew',
 'start': 11,
 'end': 17
}, {
 'entity_group': 'ORG',
 'score': 0.896050214767456,
 'word': 'DeepLearningAI',
 'start': 32,
 'end': 46
}, {
 'entity_group': 'LOC',
 'score': 0.999692440032959,
 'word': 'California',
 'start': 61,
 'end': 71
}]
</code></pre></div><p>其中，entity_group键表示的含义分别是：</p><table><tbody><tr><th>键值</th><th>含义</th></tr><tr><td>PER</td><td>人物</td></tr><tr><td>ORG</td><td>组织机构</td></tr><tr><td>LOC</td><td>位置</td></tr></tbody></table><p>我们可以用 Gradio 让输出更加直观易懂：</p><h3><b>步骤1：定义 ner 的函数，接受输入，调用 getCompletion 函数，并返回实体列表。</b></h3><div class="highlight"><pre><code class="language-text">def ner(input):
    output = get_completion(input, parameters=None, ENDPOINT_URL=API_URL)
    for entity in output:
        entity["entity"] = entity['entity_group']
    return {"text": input, "entities": output}
</code></pre></div><h3><b>步骤2：使用 Gradio 的 interface 函数，传入 ner 函数 ，将输入设置为文本，输出设置为高亮文本。</b></h3><div class="highlight"><pre><code class="language-text">demo = gr.Interface(fn=ner,
                    inputs=[gr.Textbox(label="Text to find entities", lines=2)],
                    outputs=[gr.HighlightedText(label="Text with entities")],
                    title="NER with dslim/bert-base-NER",
                    description="Find entities using the `dslim/bert-base-NER` model under the hood!",
                    allow_flagging="never",
                    examples=["My name is Andrew and I live in California", "My name is Poli and work at HuggingFace"])
</code></pre></div><p><code>HighlightedText</code> 组件的作用是接收并高亮显示NER模型输出的的实体。</p><p><code>examples</code> 参数用于提供示例，帮助用户通快速了解程序是如何工作的。</p><h3><b>步骤3：调用 demo.launch 创建用户界面。</b></h3><div class="highlight"><pre><code class="language-text">demo.launch()
</code></pre></div><p>运行结果如下：</p><figure><img class="origin_image zh-lightbox-thumb lazy" src="https://pic1.zhimg.com/v2-383ddf40c93722b89dd561e191921064_1440w.jpg" /></figure><figure><img class="origin_image zh-lightbox-thumb lazy" src="https://pic3.zhimg.com/v2-1c425ccffbe2b42154a6fbdd9b059b4e_1440w.jpg" /></figure><h3><b>（可选）步骤4：将被拆分成多个标记的字符重组为一个完整的单词。</b></h3><p>同样在<a class=" wrap external" href="https://mp.weixin.qq.com/s/7MCc8QFihLSW5mJ9NyHZ3A" rel="nofollow noreferrer" target="_blank">前面的课程</a>中我们有介绍过，<b>LLM的处理单元不是一个个「单词」，而是一个个「标记（Token）」。它会接收一系列的字符，并将字符组合在一起，形成代表常见字符序列的标记。每个标记可能对应一个单词，或者空格，或者标点符号。</b></p><p>当我们选用另外一个示例时可以看到，HuggingFace这个单词会被分解为多个块，也即多个标记。</p><figure><img class="origin_image zh-lightbox-thumb lazy" src="https://pic4.zhimg.com/v2-5e2527a8914049439a83d8ea08bc5b73_1440w.jpg" /></figure><p>但是，由于我们可以从实体标签的开头字母判断单词的开头和中间部分：</p><table><tbody><tr><th>字母</th><th>含义</th></tr><tr><td>B</td><td>开始标记</td></tr><tr><td>I</td><td>中间标记</td></tr></tbody></table><p>因此，我们可以定义一个 <code>merge_tokens</code> 函数，让每个标记在可视化时合并为一个完整单词显示，原理其实就是检查标签的开头字母，并进行合并：</p><div class="highlight"><pre><code class="language-text">def merge_tokens(tokens):
    merged_tokens = []
    for token in tokens:
        if merged_tokens and token['entity'].startswith('I-') and merged_tokens[-1]['entity'].endswith(token['entity'][2:]):
            # 如果当前 token 延续了上一个 token 的实体，则将它们合并
            last_token = merged_tokens[-1]
            last_token['word'] += token['word'].replace('##', '')
            last_token['end'] = token['end']
            last_token['score'] = (last_token['score'] + token['score']) / 2
        else:
            # 否则，将 token 添加到列表中
            merged_tokens.append(token)

    return merged_tokens
</code></pre></div><p>再次运行，就可以看到正确的结果了：</p><div class="highlight"><pre><code class="language-text">def ner(input):
    output = get_completion(input, parameters=None, ENDPOINT_URL=API_URL)
    merged_tokens = merge_tokens(output)
    return {"text": input, "entities": merged_tokens}
</code></pre></div><figure><img class="origin_image zh-lightbox-thumb lazy" src="https://pic2.zhimg.com/v2-c0e4858b821b41e8fb1e605d3c8df18d_1440w.jpg" /></figure><h2><b>图像描述应用</b> </h2><p>在这一部分里，我们将从一张图片中识别出物体、场景、动作等内容，并用自然语言描述出来。</p><p>这里使用到的是<code>Salesforce/blip-image-captioning-base</code>模型，这是一个图像描述模型，可以将图像作为输入，并输出该图像的描述。</p><figure><img class="origin_image zh-lightbox-thumb lazy" src="https://pic1.zhimg.com/v2-f71236518b5385d9bab2598a78184e84_1440w.jpg" /></figure><p>使用的免费图像来源于: <a class=" external" href="https://free-images.com/" rel="nofollow noreferrer" target="_blank"><span class="invisible">https://</span><span class="visible">free-images.com/</span><span class="invisible"></span></a></p><h3><b>步骤1：定义 captioner 的函数，接受图像，调用 getCompletion 函数，并返回图像描述。</b></h3><div class="highlight"><pre><code class="language-text">import io
import base64 
# 将图像转为API所需的Base64格式
def image_to_base64_str(pil_image):
    byte_arr = io.BytesIO()
    pil_image.save(byte_arr, format='PNG')
    byte_arr = byte_arr.getvalue()
    return str(base64.b64encode(byte_arr).decode('utf-8'))

API_URL = "https://api-inference.huggingface.co/models/Salesforce/blip-image-captioning-base"
def captioner(image):
    base64_image = image_to_base64_str(image)
    result = get_completion(base64_image, parameters=None, ENDPOINT_URL=API_URL)
    return result[0]['generated_text']
</code></pre></div><h3><b>步骤2：使用 Gradio 的 interface 函数，传入 captioner 函数 ，将输入设置为图像，输出设置为文本。</b></h3><div class="highlight"><pre><code class="language-text">demo = gr.Interface(fn=captioner,
                    inputs=[gr.Image(label="Upload image", type="pil")],
                    outputs=[gr.Textbox(label="Caption")],
                    title="Image Captioning with BLIP",
                    description="Caption any image using the BLIP model",
                    allow_flagging="never")
</code></pre></div><h3><b>步骤3：调用 demo.launch 创建用户界面。</b></h3><div class="highlight"><pre><code class="language-text">demo.launch()
</code></pre></div><p>运行结果如下：</p><figure><img class="origin_image zh-lightbox-thumb lazy" src="https://pic3.zhimg.com/v2-41bdfc79a22a6bcbf0040fd62149d33e_1440w.jpg" /></figure><figure><img class="origin_image zh-lightbox-thumb lazy" src="https://pic2.zhimg.com/v2-f7ab8095b973473a97818e01c7cc440d_1440w.jpg" /></figure><h2><b>图像生成应用</b> </h2><p>在这一部分里，我们将根据一段描述性的文本，生成一张与之相符合的图片。</p><figure><img class="origin_image zh-lightbox-thumb lazy" src="https://pic4.zhimg.com/v2-4b588342130d7b08f7688d6547fd697b_1440w.jpg" /></figure><p>这里使用到的是<code>runwayml/stable-diffusion-v1-5</code>模型，也就是我们所熟知的<code>Stable Diffusion</code>图像生成模型，我们使用API URL连接到了这个模型的服务器。</p><figure><img class="origin_image zh-lightbox-thumb lazy" src="https://pic1.zhimg.com/v2-91fb8d035810179742858cb76c747814_1440w.jpg" /></figure><h3><b>步骤1：定义 generate 的函数，接受文本描述，调用 getCompletion 函数，并返回图像。</b></h3><div class="highlight"><pre><code class="language-text"># 将PIL图像转换为base64的辅助函数
# 这样你就可以将其发送到API
def base64_to_pil(img_base64):
    base64_decoded = base64.b64decode(img_base64)
    byte_stream = io.BytesIO(base64_decoded)
    pil_image = Image.open(byte_stream)
    return pil_image

API_URL = "https://api-inference.huggingface.co/models/Salesforce/blip-image-captioning-base"
def generate(prompt):
    output = get_completion(prompt，parameters=None, ENDPOINT_URL=API_URL)
    result_image = base64_to_pil(output)
    return result_image
</code></pre></div><h3><b>步骤2：使用 Gradio 的 interface 函数，传入 generate 函数 ，将输入设置为文本，输出设置为图像。</b></h3><div class="highlight"><pre><code class="language-text">demo = gr.Interface(fn=generate,
                    inputs=[gr.Textbox(label="Your prompt")],
                    outputs=[gr.Image(label="Result")],
                    title="Image Generation with Stable Diffusion",
                    description="Generate any image with Stable Diffusion",
                    allow_flagging="never",
                    examples=["a dog in a park","a mecha robot in a favela"])

</code></pre></div><h3><b>步骤3：调用 demo.launch 创建用户界面。</b></h3><div class="highlight"><pre><code class="language-text">demo.launch()
</code></pre></div><p>运行结果如下：</p><figure><img class="origin_image zh-lightbox-thumb lazy" src="https://pic2.zhimg.com/v2-7bae87c435035ca33f34185f8649ba05_1440w.jpg" /></figure><figure><img class="origin_image zh-lightbox-thumb lazy" src="https://pic3.zhimg.com/v2-88f5ec843526b536d91e576295a7867a_1440w.jpg" /></figure><figure><img class="origin_image zh-lightbox-thumb lazy" src="https://pic1.zhimg.com/v2-e30fad77e41c2d0b66ef4c6d27431f70_1440w.jpg" /></figure><h3><b>（可选）步骤4：增加更多输入选项，以构建一个更高级的界面。</b></h3><div class="highlight"><pre><code class="language-text">with gr.Blocks() as demo:
    gr.Markdown("# Image Generation with Stable Diffusion")
    with gr.Row(): # gr.Row()用于将组件水平排列
        with gr.Column(scale=4): # scale值用于调整所占宽度比例
            prompt = gr.Textbox(label="Your prompt") 
        with gr.Column(scale=1, min_width=50):
            btn = gr.Button("Submit") 
    with gr.Accordion("Advanced options", open=False): # open=false表示默认折叠隐藏
            negative_prompt = gr.Textbox(label="Negative prompt")
            with gr.Row():
                with gr.Column(): # gr.Column()用于将组件垂直排列
                    steps = gr.Slider(label="Inference Steps", minimum=1, maximum=100, value=25,
                      info="In many steps will the denoiser denoise the image?")
                    guidance = gr.Slider(label="Guidance Scale", minimum=1, maximum=20, value=7,
                      info="Controls how much the text prompt influences the result")
                with gr.Column(): 
                    width = gr.Slider(label="Width", minimum=64, maximum=512, step=64, value=512)
                    height = gr.Slider(label="Height", minimum=64, maximum=512, step=64, value=512)
    output = gr.Image(label="Result")
            
    btn.click(fn=generate, inputs=[prompt,negative_prompt,steps,guidance,width,height], outputs=[output])
</code></pre></div><p>这些额外输入选项的作用如下：</p><table><tbody><tr><th>标签</th><th>含义</th><th>类型</th><th>作用</th></tr><tr><td>Your prompt</td><td>提示</td><td>文本框</td><td>描述想要生成的图片内容</td></tr><tr><td>Negative prompt</td><td>负提示</td><td>文本框</td><td>描述不想生成的图片内容</td></tr><tr><td>Inference Steps</td><td>推理步骤</td><td>滑块控件</td><td>控制推理过程的执行步数，步数越多结果越精细</td></tr><tr><td>Guidance Scale</td><td>指导尺度</td><td>滑块控件</td><td>控制文本提示对结果的影响程度，值越大生成结果越贴合提示内容</td></tr><tr><td>Width</td><td>宽度</td><td>滑块控件</td><td>设置图片宽度，单位：像素</td></tr><tr><td>Height</td><td>高度</td><td>滑块控件</td><td>设置图片高度，单位：像素</td></tr></tbody></table><p>运行结果如下：</p><figure><img class="origin_image zh-lightbox-thumb lazy" src="https://pic4.zhimg.com/v2-8709952b915edc01877b1b45580a995b_1440w.jpg" /></figure><h2><b>“描述与生成”游戏</b> </h2><p>在这一部分里，我们将结合前面两节的内容做一个游戏应用，首先识别一张图像的内容，再将识别出的内容作为描述生成另外一张图像。</p><h3><b>步骤1：引入第3课和第4课的函数，captioner 函数用于接受图像并返回图像描述；generate 的函数用于接收图像描述并返回图像。</b></h3><div class="highlight"><pre><code class="language-text">def image_to_base64_str(pil_image):
    byte_arr = io.BytesIO()
    pil_image.save(byte_arr, format='PNG')
    byte_arr = byte_arr.getvalue()
    return str(base64.b64encode(byte_arr).decode('utf-8'))

def base64_to_pil(img_base64):
    base64_decoded = base64.b64decode(img_base64)
    byte_stream = io.BytesIO(base64_decoded)
    pil_image = Image.open(byte_stream)
    return pil_image

def captioner(image):
    base64_image = image_to_base64_str(image)
    result = get_completion(base64_image, None, ITT_ENDPOINT)
    return result[0]['generated_text']

def generate(prompt):
    output = get_completion(prompt, None, TTI_ENDPOINT)
    result_image = base64_to_pil(output)
    return result_image
</code></pre></div><h3><b>步骤2：使用 Gradio 的 interface 函数，传入合并了两个步骤的 caption_and_generate 函数 ，将输入设置为图像，输出分别设置为一个文本和一个图像。</b></h3><div class="highlight"><pre><code class="language-text">def caption_and_generate(image):
    caption = captioner(image)
    image = generate(caption)
    return [caption, image]

with gr.Blocks() as demo:
    gr.Markdown("# Describe-and-Generate game  ️")
    image_upload = gr.Image(label="Your first image",type="pil")
    btn_all = gr.Button("Caption and generate")
    caption = gr.Textbox(label="Generated caption")
    image_output = gr.Image(label="Generated Image")

    btn_all.click(fn=caption_and_generate, inputs=[image_upload], outputs=[caption, image_output])

gr.close_all()
</code></pre></div><h3><b>步骤3：调用 demo.launch 创建用户界面。</b></h3><div class="highlight"><pre><code class="language-text">demo.launch()
</code></pre></div><p>运行结果如下：</p><figure><img class="origin_image zh-lightbox-thumb lazy" src="https://pic1.zhimg.com/v2-576e213dac2ab7907bc49d726ebabd44_1440w.jpg" /></figure><figure><img class="origin_image zh-lightbox-thumb lazy" src="https://pic2.zhimg.com/v2-082526a92ecf4ba6caf831bf185cfa85_1440w.jpg" /></figure><h2><b>与任意LLM聊天</b> </h2><p>在这一部分里，我们将创建一个能够与用户进行自然对话的聊天机器人，并根据用户提供的信息生成个性化的回答。</p><p>这里使用的到是<code>falcon-40b-instruct</code>模型，这是在<a class=" wrap external" href="https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard" rel="nofollow noreferrer" target="_blank">Open LLM Leaderboard</a>上排名最高的开源LLM之一。</p><h3><b>步骤1：初始化Client，以借助text_generation库访问FalcomLM-instruct推理端点。</b></h3><div class="highlight"><pre><code class="language-text">import requests, json
from text_generation import Client

client = Client(os.environ['HF_API_FALCOM_BASE'], headers={"Authorization": f"Basic {hf_api_key}"}, timeout=120)
</code></pre></div><h3><b>步骤2：定义 format_chat_prompt 函数，以格式化提示及对话历史</b></h3><div class="highlight"><pre><code class="language-text">def format_chat_prompt(message, chat_history, instruction):
    prompt = f"System:{instruction}"
    for turn in chat_history:
        user_message, bot_message = turn
        prompt = f"{prompt}\nUser: {user_message}\nAssistant: {bot_message}"
    prompt = f"{prompt}\nUser: {message}\nAssistant:"
    return prompt
</code></pre></div><p>其中涉及到三种角色消息我们也已经介绍过很多次了：</p><ul><li>系统消息（System）：负责指定LLM整体的语言风格或者助手的行为；</li><li>助手消息（Assistant）：负责根据用户消息要求内容，以及系统消息的设定，输出一个合适的回应；</li><li>用户消息（User）：给出一个具体的指令。</li></ul><p>而这一步骤的目的是为模型提供记忆功能，使模型能够在理解上下文的前提下，回答后续问题。</p><div class="highlight"><pre><code class="language-text">User: what is the meaning of life?
Assistant:

User: what is the meaning of life?
Assistant: I'm sorry, I cannot provide a definitive answer to that question.It is a philosophical question
User: but why?
Assistant:
</code></pre></div><h3><b>步骤3：定义 respond 函数，以调用 Client 的 generate 函数，将消息发送给模型，并将响应追加到对话历史。</b></h3><div class="highlight"><pre><code class="language-text">def respond(message, chat_history):
        formatted_prompt = format_chat_prompt(message, chat_history)
        bot_message = client.generate(formatted_prompt,
                                     max_new_tokens=1024,
                                     stop_sequences=["\nUser:", "&lt;|endoftext|&gt;"]).generated_text
        chat_history.append((message, bot_message))
        return "", chat_history
</code></pre></div><p>这里有一个问题。</p><p>随着这个过程的持续进行，我们发送给模型的对话历史会越来越多，直到模型达到一次对话中可以处理的最大标记数限制。</p><p>因此，我们选择在这里将最大标记数（max_new_tokens）参数设置为1024，这将保留最后1024个标记的对话历史。</p><p>要想详细这个参数的特点，可以参考之前<a class=" wrap external" href="https://mp.weixin.qq.com/s/KrFV-fuP2MI3HQGmEM1sUA" rel="nofollow noreferrer" target="_blank">《精华笔记：吴恩达 x LangChain《基于LangChain的大语言模型应用开发》(上) 》</a>一文中的<code>ConversationalTokenBufferMemory</code>。</p><p>而另外一个停止序列（stop_sequences）参数的作用，则是为了预防助手消息冒认用户消息，确保对话历史中的用户消息来自于真正的用户而不是来自于模型。</p><p>比如，当我们只向模型提问了这一句时：</p><figure><img class="origin_image zh-lightbox-thumb lazy" src="https://pic4.zhimg.com/v2-b675e346693d1b763bbe7599377abf87_1440w.jpg" /></figure><p>模型可能会在回答的同时，构造多了一个虚假的提问：</p><figure><img class="origin_image zh-lightbox-thumb lazy" src="https://pic1.zhimg.com/v2-1916c2f51a2fb357160f697a91e6bb80_1440w.jpg" /></figure><h3><b>步骤4：使用 Gradio 的 Chatbot 函数，快速构建一个聊天机器人组件</b></h3><div class="highlight"><pre><code class="language-text">with gr.Blocks() as demo:
    chatbot = gr.Chatbot(height=240)
    msg = gr.Textbox(label="Prompt")
    with gr.Accordion(label="Advanced options",open=False):
        system = gr.Textbox(label="System message", lines=2, value="A conversation between a user and an LLM-based AI assistant. The assistant gives helpful and honest answers.")
        temperature = gr.Slider(label="temperature", minimum=0.1, maximum=1, value=0.7, step=0.1)
    btn = gr.Button("Submit")
    clear = gr.ClearButton(components=[msg, chatbot], value="Clear console")

    btn.click(respond, inputs=[msg, chatbot, system], outputs=[msg, chatbot])
    msg.submit(respond, inputs=[msg, chatbot, system], outputs=[msg, chatbot]) #Press enter to submit

</code></pre></div><p>聊天机器人组件可以简化我们发送对话历史给模型的过程。</p><p>其中提供的一些额外输入选项的作用如下：</p><table><tbody><tr><th>标签</th><th>含义</th><th>类型</th><th>作用</th></tr><tr><td>System message</td><td>系统消息</td><td>文本框</td><td>告诉LLM如何与你交流，比如设定角色或调整语气</td></tr><tr><td>temperature</td><td>温度</td><td>滑块控件</td><td>为0时，模型对于相同输入会始终做出相同回答；值越大，模型给出的回答变化越大。</td></tr></tbody></table><h3><b>（可选）步骤5：让模型改用实时流式的方式传输答案</b></h3><p>在这种方式下，我们会将标记逐个发送，并实时地看到它的完整回答，因此不需要等待整个答案准备好，最终呈现的效果就是与ChatGPT一样逐词输出。</p><div class="highlight"><pre><code class="language-text">def respond(message, chat_history, instruction, temperature=0.7):
    prompt = format_chat_prompt(message, chat_history, instruction)
    chat_history = chat_history + [[message, ""]]
    stream = client.generate_stream(prompt,
                                      max_new_tokens=1024,
                                      stop_sequences=["\nUser:", "&lt;|endoftext|&gt;"],
                                      temperature=temperature)
                                      #stop_sequences to not generate the user answer
    acc_text = ""
    #Streaming the tokens
    for idx, response in enumerate(stream):
            text_token = response.token.text

            if response.details:
                return

            if idx == 0 and text_token.startswith(" "):
                text_token = text_token[1:]

            acc_text += text_token
            last_turn = list(chat_history.pop(-1))
            last_turn[-1] += acc_text
            chat_history = chat_history + [last_turn]
            yield "", chat_history
            acc_text = ""
</code></pre></div><h2><b>参考</b> </h2><p>Inference API-(<a class=" external" href="https://huggingface.co/docs/api-inference/index" rel="nofollow noreferrer" target="_blank"><span class="invisible">https://</span><span class="visible">huggingface.co/docs/api</span><span class="invisible">-inference/index</span><span class="ellipsis"></span></a>)</p>
]]></content:encoded>
<pubDate>Sun, 20 Aug 2023 13:54:17 GMT</pubDate>
<pubDate>Sun, 20 Aug 2023 13:54:17 GMT</pubDate>
</item>
<item>
<title>星际码仔发表了文章: 精华笔记：吴恩达 x LangChain 《使用LangChain构建与数据对话的聊天机器人》（下）</title>
<link>https://zhuanlan.zhihu.com/p/651216604</link>
<guid>https://zhuanlan.zhihu.com/p/651216604</guid>
<content:encoded><![CDATA[

<h2><b>问答</b> </h2><figure><img class="origin_image zh-lightbox-thumb" src="https://pic4.zhimg.com/v2-fae0b59923a35a77a9c7abd9b8954767_1440w.jpg" /></figure><p>在这节课中，我们将学习如何利用检索到的文档来回答用户的问题。</p><figure><img class="origin_image zh-lightbox-thumb" src="https://pic2.zhimg.com/v2-fecca2e46a86d6b4cde1f9251bf66899_1440w.jpg" /></figure><p>整个过程可以拆分为以下几个步骤：</p><ol><li>用户输入一个问题（Question）</li><li>从向量存储（Store）中检索出与问题相关的文档分块（Relavant splits）</li><li>将这些分块连同系统提示（System:Prompt）和用户问题（Human:Question）一起作为输入传给语言模型（LLM）</li><li>语言模型根据输入生成答案（Answer）</li></ol><p>默认使用的是<code>stuff</code>方法，其特点如下：</p><table><tbody><tr><th>特点</th><th>优点</th><th>缺点</th></tr><tr><td>将所有检索到的分块放入同一个上下文窗口中，只需要对语言模型进行一次调用。</td><td>简单、廉价且效果不错。</td><td>当检索到的文档过多时，由于上下文窗口长度有限，可能无法将所有分块都传入。</td></tr></tbody></table><figure><img class="origin_image zh-lightbox-thumb" src="https://pic2.zhimg.com/v2-095df7631a7da54b7d6568b5a2074cb1_1440w.jpg" /></figure><p>为了解决上下文窗口长度限制的问题，我们可以使用<code>Map-reduce</code>、<code>Refine</code>和<code>Map-rerank</code>三种方法，这些方法我们在之前的课程中已经简要介绍过了，今天我们将进一步深入了解。</p><h2><b>stuff</b></h2><h3><b>步骤1：加载之前保存的向量数据库</b></h3><div class="highlight"><pre><code class="language-text">from langchain.vectorstores import Chroma
from langchain.embeddings.openai import OpenAIEmbeddings
persist_directory = 'docs/chroma/'
embedding = OpenAIEmbeddings()
vectordb = Chroma(persist_directory=persist_directory, embedding_function=embedding)
</code></pre></div><h3><b>步骤2：初始化将用于回答问题的语言模型</b></h3><div class="highlight"><pre><code class="language-text">llm_name = "gpt-3.5-turbo"
from langchain.chat_models import ChatOpenAI
llm = ChatOpenAI(model_name=llm_name, temperature=0)
</code></pre></div><p>temperature参数设置为0，可以帮助我们得到更准确的答案，因为它降低了语言模型的可变性，通常能给我们最高置信度、最可靠的答案。</p><h3><b>步骤3：导入、创建、调用检索问答链，输入问题，并获取答案</b></h3><div class="highlight"><pre><code class="language-text">question = "What are major topics for this class?"

from langchain.chains import RetrievalQA
qa_chain = RetrievalQA.from_chain_type(
    llm,
    retriever=vectordb.as_retriever()
)
result = qa_chain({"query": question})
result["result"]
</code></pre></div><blockquote>❝ 'The major topic for this class is machine learning. Additionally, the class may cover statistics and algebra as refreshers in the discussion sections. Later in the quarter, the discussion sections will also cover extensions for the material taught in the main lectures.'<br /> </blockquote><h3><b>步骤4：使用提示模板优化输出结果</b></h3><p>提示模板是一种可以帮助语言模型生成更符合要求的输出结果的技巧，我们在<a class=" wrap external" href="https://mp.weixin.qq.com/s/KrFV-fuP2MI3HQGmEM1sUA" rel="nofollow noreferrer" target="_blank">上一门课</a>中已经介绍过了。这里我们使用的提示模板主要是为了让输出结果更简洁、更少编造、更礼貌。</p><div class="highlight"><pre><code class="language-text">from langchain.prompts import PromptTemplate

# 构建提示词
# {context}：上下文占位符，用于放置文档内容
# {question}：问题占位符，放置要查询的问题
template = """Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. Use three sentences maximum. Keep the answer as concise as possible. Always say "thanks for asking!" at the end of the answer. 
{context}
Question: {question}
Helpful Answer:"""
QA_CHAIN_PROMPT = PromptTemplate.from_template(template)

# 运行链
# return_source_documents=True用于支持查看检索到的文档
qa_chain = RetrievalQA.from_chain_type(
    llm,
    retriever=vectordb.as_retriever(),
    return_source_documents=True,
    chain_type_kwargs={"prompt": QA_CHAIN_PROMPT}
)

question = "Is probability a class topic?"
result = qa_chain({"query": question})
result["result"]
</code></pre></div><blockquote>❝ 'Yes, probability is assumed to be a prerequisite for this class. The instructor assumes familiarity with basic probability and statistics, and will go over some of the prerequisites in the discussion sections as a refresher course. Thanks for asking!'<br /> </blockquote><h3><b>步骤5：查看返回的源文档，理解其从哪里获取数据</b></h3><div class="highlight"><pre><code class="language-text">result["source_documents"][0]
</code></pre></div><blockquote>❝ Document(page_content="of this class will not be very program ming intensive, although we will do some \nprogramming, mostly in either MATLAB or Octa ve. I'll say a bit more about that later.  \nI also assume familiarity with basic proba bility and statistics. So most undergraduate \nstatistics class, like Stat 116 taught here at Stanford, will be more than enough. I'm gonna \nassume all of you know what ra ndom variables are, that all of you know what expectation \nis, what a variance or a random variable is. And in case of some of you, it's been a while \nsince you've seen some of this material. At some of the discussion sections, we'll actually \ngo over some of the prerequisites, sort of as  a refresher course under prerequisite class. \nI'll say a bit more about that later as well.  \nLastly, I also assume familiarity with basi c linear algebra. And again, most undergraduate \nlinear algebra courses are more than enough. So if you've taken courses like Math 51, \n103, Math 113 or CS205 at Stanford, that would be more than enough. Basically, I'm \ngonna assume that all of you know what matrix es and vectors are, that you know how to \nmultiply matrices and vectors and multiply matrix and matrices, that you know what a matrix inverse is. If you know what an eigenvect or of a matrix is, that'd be even better. \nBut if you don't quite know or if you're not qu ite sure, that's fine, too. We'll go over it in \nthe review sections.", metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture01.pdf', 'page': 4})<br /> </blockquote><h2><b>Map-reduce</b></h2><figure><img class="origin_image zh-lightbox-thumb" src="https://pic3.zhimg.com/v2-91bca31bc6e7f299ac8617a654a1d7e6_1440w.jpg" /></figure><p>Map-reduce方法的特点如下：</p><table><tbody><tr><th>特点</th><th>优点</th><th>缺点</th></tr><tr><td>1.将每个文档单独发送到语言模型中，根据单个文档生成答案；</td><td>可以处理任意数量的文档。</td><td>1.涉及到对语言模型的多次调用，速度较慢；</td></tr><tr><td>2.将所有这些答案组合在一起，再调用语言模型生成最终答案。</td><td></td><td>2.信息可能分散在不同的文档中，无法基于同一个上下文获取信息，结果可能不准确。</td></tr></tbody></table><div class="highlight"><pre><code class="language-text">qa_chain_mr = RetrievalQA.from_chain_type(
    llm,
    retriever=vectordb.as_retriever(),
    chain_type="map_reduce"
)

result = qa_chain_mr({"query": question})
result["result"]
</code></pre></div><blockquote>❝ 'There is no clear answer to this question based on the given portion of the document. The document mentions familiarity with basic probability and statistics as a prerequisite for the class, and there is a brief mention of probability in the text, but it is not clear if it is a main topic of the class.'<br /> </blockquote><h3><b>使用LangSmith平台了解这些链内部的调用情况</b></h3><p>LangSmith 是一个用于构建生产级 LLM 应用程序的平台。</p><p>它可以让您轻松地调试、测试、评估和监控基于任何 LLM 框架构建的链和智能代理，并与使用 LLM 构建的开源框架 LangChain 完美集成。</p><p>要体验这个平台的功能，你需要：</p><ol><li>前往<a class=" wrap external" href="https://smith.langchain.com/" rel="nofollow noreferrer" target="_blank">LangSmith平台</a>注册（可能需要排队）</li><li>创建 API 密钥</li><li>在以下代码中使用这个 API 密钥</li><li>取消注释以下代码，并重新运行MapReduce链</li></ol><div class="highlight"><pre><code class="language-text">import os
os.environ["LANGCHAIN_TRACING_V2"] = "true"
os.environ["LANGCHAIN_ENDPOINT"] = "https://api.langchain.plus"
os.environ["LANGCHAIN_API_KEY"] = "..." # 替换...为你的 API 密钥
</code></pre></div><p>之后，就可以切换到LangSmith平台，找到刚刚运行的RetrievalQA，查看输入、输出以及调用链了：</p><figure><img class="origin_image zh-lightbox-thumb" src="https://pic2.zhimg.com/v2-3ddd9df44dc173187dda4340bd3548a1_1440w.jpg" /></figure><p>可以看到，MapReduceDocumentChain中涉及到了对语言模型的四次独立调用，点击其中一个，就可以看到每个文档的具体输入和输出：</p><figure><img class="origin_image zh-lightbox-thumb" src="https://pic1.zhimg.com/v2-f2dd689cdf41a54da86cc7ee97ef3a48_1440w.jpg" /></figure><p>并且，可以看到，它们在最后的链中被合并为了StuffDocumentChain，也即把所有结果放到了最终调用中。点击进入就可以看到，系统消息中包含了来自前面文档的四个摘要：</p><figure><img class="origin_image zh-lightbox-thumb" src="https://pic2.zhimg.com/v2-c1c97bee5c31b7ba560ab6f75afaa8ad_1440w.jpg" /></figure><h2><b>Refine</b></h2><figure><img class="origin_image zh-lightbox-thumb" src="https://pic3.zhimg.com/v2-640d154953d5e7cac1932d11f2c11cce_1440w.jpg" /></figure><p>Refine方法的特点如下：</p><table><tbody><tr><th>特点</th><th>优点</th><th>缺点</th></tr><tr><td>迭代地处理多个文档，基于前一个文档的答案来构建一个更好的答案。</td><td>允许组合信息，更鼓励信息的传递</td><td>速度较慢</td></tr></tbody></table><div class="highlight"><pre><code class="language-text">qa_chain_mr = RetrievalQA.from_chain_type(
    llm,
    retriever=vectordb.as_retriever(),
    chain_type="refine"
)
result = qa_chain_mr({"query": question})
result["result"]
</code></pre></div><blockquote>❝ "The main topic of the class is machine learning algorithms, including linear regression and classification. Basic probability and statistics, as well as linear algebra, are prerequisites for the class, but the instructor will provide a refresher course on these topics in some of the discussion sections. Later in the quarter, the discussion sections will also cover extensions for the material taught in the main lectures. The instructor will focus on a few important extensions that there wasn't enough time to cover in the main lectures."<br /> </blockquote><p>现在还有一个问题，我们目前使用的链是没有“记忆”这个概念的，这就导致了它不会记住之前的问题或答案。为了解决这个问题，我们需要引入“记忆”功能，这就是我们下一节要讲的内容。</p><h2><b>Chat</b> </h2><figure><img class="origin_image zh-lightbox-thumb" src="https://pic1.zhimg.com/v2-0a64be73f13422b3419f2e6b70ec8c3c_1440w.jpg" /></figure><p>在这节课中，我们将构建一个完整的问答聊天机器人，它将结合我们之前讲过的所有组件，并引入“聊天历史”这个概念，让它在回答问题时能够考虑到之前的对话或信息，也就是说，它能记住你之前说过什么。</p><h3><b>步骤1：初始化用于保存大量文档内容的向量数据库</b></h3><div class="highlight"><pre><code class="language-text">from langchain.vectorstores import Chroma
from langchain.embeddings.openai import OpenAIEmbeddings
persist_directory = 'docs/chroma/'
embedding = OpenAIEmbeddings()
vectordb = Chroma(persist_directory=persist_directory, embedding_function=embedding)
</code></pre></div><h3><b>步骤2：初始化将作为聊天机器人使用的语言模型</b></h3><div class="highlight"><pre><code class="language-text">llm_name = "gpt-3.5-turbo"
from langchain.chat_models import ChatOpenAI
llm = ChatOpenAI(model_name=llm_name, temperature=0)
</code></pre></div><h3><b>步骤3：初始化提示模板，让输出结果更简介、更少编造、更礼貌</b></h3><div class="highlight"><pre><code class="language-text"># 构建提示
from langchain.prompts import PromptTemplate
template = """Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. Use three sentences maximum. Keep the answer as concise as possible. Always say "thanks for asking!" at the end of the answer. 
{context}
Question: {question}
Helpful Answer:"""
QA_CHAIN_PROMPT = PromptTemplate(input_variables=["context", "question"],template=template,)

</code></pre></div><h3><b>步骤4：创建检索QA链，用于合并检索到的文本片段并调用语言模型</b></h3><div class="highlight"><pre><code class="language-text"># 运行链
from langchain.chains import RetrievalQA
qa_chain = RetrievalQA.from_chain_type(llm,
                                       retriever=vectordb.as_retriever(),
                                       return_source_documents=True,
                                       chain_type_kwargs={"prompt": QA_CHAIN_PROMPT})
</code></pre></div><h3><b>步骤5：使用ConversationBufferMemory增加聊天机器人的记忆功能</b></h3><div class="highlight"><pre><code class="language-text">from langchain.memory import ConversationBufferMemory
memory = ConversationBufferMemory(
    memory_key="chat_history",
    return_messages=True
)
</code></pre></div><p>ConversationBufferMemory提供了一个聊天消息历史的记忆缓冲区，并且每次都会将这部分历史消息传入聊天机器人。</p><p>return_messages=True表示将返回一个列表类型的聊天历史记录，而不是一个字符串。</p><h3><b>步骤6：创建ConversationalRetrievalChain（对话检索链），传入语言模型、检索器和记忆系统</b></h3><div class="highlight"><pre><code class="language-text">from langchain.chains import ConversationalRetrievalChain
retriever=vectordb.as_retriever()
qa = ConversationalRetrievalChain.from_llm(
    llm,
    retriever=retriever,
    memory=memory
)
</code></pre></div><p>ConversationalRetrievalChain会在RetrievalQAChain的基础上，将聊天历史和新提的问题整合成一个新的独立问题，以传递给向量存储库，查找相关文档。</p><h3><b>步骤7：使用PyPDFLoader加载所要参考的文档</b></h3><div class="highlight"><pre><code class="language-text">from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter
from langchain.vectorstores import DocArrayInMemorySearch
from langchain.document_loaders import TextLoader
from langchain.chains import RetrievalQA,  ConversationalRetrievalChain
from langchain.memory import ConversationBufferMemory
from langchain.chat_models import ChatOpenAI
from langchain.document_loaders import TextLoader
from langchain.document_loaders import PyPDFLoader

def load_db(file, chain_type, k):
    # 加载文档
    loader = PyPDFLoader(file)
    documents = loader.load()
    ...
</code></pre></div><h3><b>步骤8：分割文档，为每个分块创建嵌入，并存储到向量存储库中。</b></h3><div class="highlight"><pre><code class="language-text">def load_db(file, chain_type, k):
    ...
    # 分隔文档
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)
    docs = text_splitter.split_documents(documents)
    # 定义嵌入
    embeddings = OpenAIEmbeddings()
    # 基于文档数据创建向量数据库
    db = DocArrayInMemorySearch.from_documents(docs, embeddings)
    ...
</code></pre></div><h3><b>步骤9：从向量数据库创建一个基于“相似度”的检索器。</b></h3><div class="highlight"><pre><code class="language-text">def load_db(file, chain_type, k):
    ...
    # 定义检索器
    retriever = db.as_retriever(search_type="similarity", search_kwargs={"k": k})
    ...
</code></pre></div><h3><b>步骤10：创建对话检索链，用于将聊天历史和新提的问题整合成一个新的独立问题</b></h3><div class="highlight"><pre><code class="language-text">def load_db(file, chain_type, k):
    ...
    # create a chatbot chain. Memory is managed externally.
    qa = ConversationalRetrievalChain.from_llm(
        llm=ChatOpenAI(model_name=llm_name, temperature=0), 
        chain_type=chain_type, 
        retriever=retriever, 
        return_source_documents=True,
        return_generated_question=True,
    )
    return qa 
</code></pre></div><p>需要注意的是，这里我们并没有传入记忆系统，而是将记忆管理交给了GUI，这意味着聊天历史需要在链之外进行维护。</p><h3><b>步骤11：提供一个与聊天机器人交互的用户界面</b></h3><div class="highlight"><pre><code class="language-text">import panel as pn
import param

class cbfs(param.Parameterized):
    chat_history = param.List([])
    answer = param.String("")
    db_query  = param.String("")
    db_response = param.List([])
    
    def __init__(self,  **params):
        super(cbfs, self).__init__( **params)
        self.panels = []
        self.loaded_file = "docs/cs229_lectures/MachineLearning-Lecture01.pdf"
        self.qa = load_db(self.loaded_file,"stuff", 4)
    
    def call_load_db(self, count):
        if count == 0 or file_input.value is None:  # init or no file specified :
            return pn.pane.Markdown(f"Loaded File: {self.loaded_file}")
        else:
            file_input.save("temp.pdf")  # local copy
            self.loaded_file = file_input.filename
            button_load.button_style="outline"
            self.qa = load_db("temp.pdf", "stuff", 4)
            button_load.button_style="solid"
        self.clr_history()
        return pn.pane.Markdown(f"Loaded File: {self.loaded_file}")

    def convchain(self, query):
        if not query:
            return pn.WidgetBox(pn.Row('User:', pn.pane.Markdown("", width=600)), scroll=True)
        result = self.qa({"question": query, "chat_history": self.chat_history})
        self.chat_history.extend([(query, result["answer"])])
        self.db_query = result["generated_question"]
        self.db_response = result["source_documents"]
        self.answer = result['answer'] 
        self.panels.extend([
            pn.Row('User:', pn.pane.Markdown(query, width=600)),
            pn.Row('ChatBot:', pn.pane.Markdown(self.answer, width=600, style={'background-color': '#F6F6F6'}))
        ])
        inp.value = ''  #clears loading indicator when cleared
        return pn.WidgetBox(*self.panels,scroll=True)

    @param.depends('db_query ', )
    def get_lquest(self):
        if not self.db_query :
            return pn.Column(
                pn.Row(pn.pane.Markdown(f"Last question to DB:", styles={'background-color': '#F6F6F6'})),
                pn.Row(pn.pane.Str("no DB accesses so far"))
            )
        return pn.Column(
            pn.Row(pn.pane.Markdown(f"DB query:", styles={'background-color': '#F6F6F6'})),
            pn.pane.Str(self.db_query )
        )

    @param.depends('db_response', )
    def get_sources(self):
        if not self.db_response:
            return 
        rlist=[pn.Row(pn.pane.Markdown(f"Result of DB lookup:", styles={'background-color': '#F6F6F6'}))]
        for doc in self.db_response:
            rlist.append(pn.Row(pn.pane.Str(doc)))
        return pn.WidgetBox(*rlist, width=600, scroll=True)

    @param.depends('convchain', 'clr_history') 
    def get_chats(self):
        if not self.chat_history:
            return pn.WidgetBox(pn.Row(pn.pane.Str("No History Yet")), width=600, scroll=True)
        rlist=[pn.Row(pn.pane.Markdown(f"Current Chat History variable", styles={'background-color': '#F6F6F6'}))]
        for exchange in self.chat_history:
            rlist.append(pn.Row(pn.pane.Str(exchange)))
        return pn.WidgetBox(*rlist, width=600, scroll=True)

    def clr_history(self,count=0):
        self.chat_history = []
        return 

cb = cbfs()

file_input = pn.widgets.FileInput(accept='.pdf')
button_load = pn.widgets.Button(name="Load DB", button_type='primary')
button_clearhistory = pn.widgets.Button(name="Clear History", button_type='warning')
button_clearhistory.on_click(cb.clr_history)
inp = pn.widgets.TextInput( placeholder='Enter text here…')

bound_button_load = pn.bind(cb.call_load_db, button_load.param.clicks)
conversation = pn.bind(cb.convchain, inp) 

jpg_pane = pn.pane.Image( './img/convchain.jpg')

tab1 = pn.Column(
    pn.Row(inp),
    pn.layout.Divider(),
    pn.panel(conversation,  loading_indicator=True, height=300),
    pn.layout.Divider(),
)
tab2= pn.Column(
    pn.panel(cb.get_lquest),
    pn.layout.Divider(),
    pn.panel(cb.get_sources ),
)
tab3= pn.Column(
    pn.panel(cb.get_chats),
    pn.layout.Divider(),
)
tab4=pn.Column(
    pn.Row( file_input, button_load, bound_button_load),
    pn.Row( button_clearhistory, pn.pane.Markdown("Clears chat history. Can use to start a new topic" )),
    pn.layout.Divider(),
    pn.Row(jpg_pane.clone(width=400))
)
dashboard = pn.Column(
    pn.Row(pn.pane.Markdown('# ChatWithYourData_Bot')),
    pn.Tabs(('Conversation', tab1), ('Database', tab2), ('Chat History', tab3),('Configure', tab4))
)
dashboard
</code></pre></div><h3><b>步骤12：在运行起来的用户界面上进行实际的问答对话。</b></h3><figure><img class="origin_image zh-lightbox-thumb" src="https://pic1.zhimg.com/v2-b60e4437857858a11df9a67a1f1423c4_1440w.jpg" /></figure>
]]></content:encoded>
<pubDate>Sun, 20 Aug 2023 13:49:07 GMT</pubDate>
<pubDate>Sun, 20 Aug 2023 13:49:07 GMT</pubDate>
</item>
<item>
<title>星际码仔回答了问题: 有哪些API接口可以用来做聊天机器人？</title>
<link>https://www.zhihu.com/question/381697812/answer/3174670861</link>
<guid>https://www.zhihu.com/question/381697812/answer/3174670861</guid>
<content:encoded><![CDATA[

<h2><b>问答</b> </h2><figure><img class="origin_image zh-lightbox-thumb lazy" src="https://pic4.zhimg.com/v2-fae0b59923a35a77a9c7abd9b8954767_1440w.jpg" /></figure><p>在这节课中，我们将学习如何利用检索到的文档来回答用户的问题。</p><figure><img class="origin_image zh-lightbox-thumb lazy" src="https://pic2.zhimg.com/v2-fecca2e46a86d6b4cde1f9251bf66899_1440w.jpg" /></figure><p>整个过程可以拆分为以下几个步骤：</p><ol><li>用户输入一个问题（Question）</li><li>从向量存储（Store）中检索出与问题相关的文档分块（Relavant splits）</li><li>将这些分块连同系统提示（System:Prompt）和用户问题（Human:Question）一起作为输入传给语言模型（LLM）</li><li>语言模型根据输入生成答案（Answer）</li></ol><p>默认使用的是<code>stuff</code>方法，其特点如下：</p><table><tbody><tr><th>特点</th><th>优点</th><th>缺点</th></tr><tr><td>将所有检索到的分块放入同一个上下文窗口中，只需要对语言模型进行一次调用。</td><td>简单、廉价且效果不错。</td><td>当检索到的文档过多时，由于上下文窗口长度有限，可能无法将所有分块都传入。</td></tr></tbody></table><figure><img class="origin_image zh-lightbox-thumb lazy" src="https://pic2.zhimg.com/v2-095df7631a7da54b7d6568b5a2074cb1_1440w.jpg" /></figure><p>为了解决上下文窗口长度限制的问题，我们可以使用<code>Map-reduce</code>、<code>Refine</code>和<code>Map-rerank</code>三种方法，这些方法我们在之前的课程中已经简要介绍过了，今天我们将进一步深入了解。</p><h2><b>stuff</b></h2><h3><b>步骤1：加载之前保存的向量数据库</b></h3><div class="highlight"><pre><code class="language-text">from langchain.vectorstores import Chroma
from langchain.embeddings.openai import OpenAIEmbeddings
persist_directory = 'docs/chroma/'
embedding = OpenAIEmbeddings()
vectordb = Chroma(persist_directory=persist_directory, embedding_function=embedding)
</code></pre></div><h3><b>步骤2：初始化将用于回答问题的语言模型</b></h3><div class="highlight"><pre><code class="language-text">llm_name = "gpt-3.5-turbo"
from langchain.chat_models import ChatOpenAI
llm = ChatOpenAI(model_name=llm_name, temperature=0)
</code></pre></div><p>temperature参数设置为0，可以帮助我们得到更准确的答案，因为它降低了语言模型的可变性，通常能给我们最高置信度、最可靠的答案。</p><h3><b>步骤3：导入、创建、调用检索问答链，输入问题，并获取答案</b></h3><div class="highlight"><pre><code class="language-text">question = "What are major topics for this class?"

from langchain.chains import RetrievalQA
qa_chain = RetrievalQA.from_chain_type(
    llm,
    retriever=vectordb.as_retriever()
)
result = qa_chain({"query": question})
result["result"]
</code></pre></div><blockquote>❝ 'The major topic for this class is machine learning. Additionally, the class may cover statistics and algebra as refreshers in the discussion sections. Later in the quarter, the discussion sections will also cover extensions for the material taught in the main lectures.'<br /> </blockquote><h3><b>步骤4：使用提示模板优化输出结果</b></h3><p>提示模板是一种可以帮助语言模型生成更符合要求的输出结果的技巧，我们在<a class=" wrap external" href="https://mp.weixin.qq.com/s/KrFV-fuP2MI3HQGmEM1sUA" rel="nofollow noreferrer" target="_blank">上一门课</a>中已经介绍过了。这里我们使用的提示模板主要是为了让输出结果更简洁、更少编造、更礼貌。</p><div class="highlight"><pre><code class="language-text">from langchain.prompts import PromptTemplate

# 构建提示词
# {context}：上下文占位符，用于放置文档内容
# {question}：问题占位符，放置要查询的问题
template = """Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. Use three sentences maximum. Keep the answer as concise as possible. Always say "thanks for asking!" at the end of the answer. 
{context}
Question: {question}
Helpful Answer:"""
QA_CHAIN_PROMPT = PromptTemplate.from_template(template)

# 运行链
# return_source_documents=True用于支持查看检索到的文档
qa_chain = RetrievalQA.from_chain_type(
    llm,
    retriever=vectordb.as_retriever(),
    return_source_documents=True,
    chain_type_kwargs={"prompt": QA_CHAIN_PROMPT}
)

question = "Is probability a class topic?"
result = qa_chain({"query": question})
result["result"]
</code></pre></div><blockquote>❝ 'Yes, probability is assumed to be a prerequisite for this class. The instructor assumes familiarity with basic probability and statistics, and will go over some of the prerequisites in the discussion sections as a refresher course. Thanks for asking!'<br /> </blockquote><h3><b>步骤5：查看返回的源文档，理解其从哪里获取数据</b></h3><div class="highlight"><pre><code class="language-text">result["source_documents"][0]
</code></pre></div><blockquote>❝ Document(page_content="of this class will not be very program ming intensive, although we will do some \nprogramming, mostly in either MATLAB or Octa ve. I'll say a bit more about that later.  \nI also assume familiarity with basic proba bility and statistics. So most undergraduate \nstatistics class, like Stat 116 taught here at Stanford, will be more than enough. I'm gonna \nassume all of you know what ra ndom variables are, that all of you know what expectation \nis, what a variance or a random variable is. And in case of some of you, it's been a while \nsince you've seen some of this material. At some of the discussion sections, we'll actually \ngo over some of the prerequisites, sort of as  a refresher course under prerequisite class. \nI'll say a bit more about that later as well.  \nLastly, I also assume familiarity with basi c linear algebra. And again, most undergraduate \nlinear algebra courses are more than enough. So if you've taken courses like Math 51, \n103, Math 113 or CS205 at Stanford, that would be more than enough. Basically, I'm \ngonna assume that all of you know what matrix es and vectors are, that you know how to \nmultiply matrices and vectors and multiply matrix and matrices, that you know what a matrix inverse is. If you know what an eigenvect or of a matrix is, that'd be even better. \nBut if you don't quite know or if you're not qu ite sure, that's fine, too. We'll go over it in \nthe review sections.", metadata={'source': 'docs/cs229_lectures/MachineLearning-Lecture01.pdf', 'page': 4})<br /> </blockquote><h2><b>Map-reduce</b></h2><figure><img class="origin_image zh-lightbox-thumb lazy" src="https://pic3.zhimg.com/v2-91bca31bc6e7f299ac8617a654a1d7e6_1440w.jpg" /></figure><p>Map-reduce方法的特点如下：</p><table><tbody><tr><th>特点</th><th>优点</th><th>缺点</th></tr><tr><td>1.将每个文档单独发送到语言模型中，根据单个文档生成答案；</td><td>可以处理任意数量的文档。</td><td>1.涉及到对语言模型的多次调用，速度较慢；</td></tr><tr><td>2.将所有这些答案组合在一起，再调用语言模型生成最终答案。</td><td></td><td>2.信息可能分散在不同的文档中，无法基于同一个上下文获取信息，结果可能不准确。</td></tr></tbody></table><div class="highlight"><pre><code class="language-text">qa_chain_mr = RetrievalQA.from_chain_type(
    llm,
    retriever=vectordb.as_retriever(),
    chain_type="map_reduce"
)

result = qa_chain_mr({"query": question})
result["result"]
</code></pre></div><blockquote>❝ 'There is no clear answer to this question based on the given portion of the document. The document mentions familiarity with basic probability and statistics as a prerequisite for the class, and there is a brief mention of probability in the text, but it is not clear if it is a main topic of the class.'<br /> </blockquote><h3><b>使用LangSmith平台了解这些链内部的调用情况</b></h3><p>LangSmith 是一个用于构建生产级 LLM 应用程序的平台。</p><p>它可以让您轻松地调试、测试、评估和监控基于任何 LLM 框架构建的链和智能代理，并与使用 LLM 构建的开源框架 LangChain 完美集成。</p><p>要体验这个平台的功能，你需要：</p><ol><li>前往<a class=" wrap external" href="https://smith.langchain.com/" rel="nofollow noreferrer" target="_blank">LangSmith平台</a>注册（可能需要排队）</li><li>创建 API 密钥</li><li>在以下代码中使用这个 API 密钥</li><li>取消注释以下代码，并重新运行MapReduce链</li></ol><div class="highlight"><pre><code class="language-text">import os
os.environ["LANGCHAIN_TRACING_V2"] = "true"
os.environ["LANGCHAIN_ENDPOINT"] = "https://api.langchain.plus"
os.environ["LANGCHAIN_API_KEY"] = "..." # 替换...为你的 API 密钥
</code></pre></div><p>之后，就可以切换到LangSmith平台，找到刚刚运行的RetrievalQA，查看输入、输出以及调用链了：</p><figure><img class="origin_image zh-lightbox-thumb lazy" src="https://pic2.zhimg.com/v2-3ddd9df44dc173187dda4340bd3548a1_1440w.jpg" /></figure><p>可以看到，MapReduceDocumentChain中涉及到了对语言模型的四次独立调用，点击其中一个，就可以看到每个文档的具体输入和输出：</p><figure><img class="origin_image zh-lightbox-thumb lazy" src="https://pic1.zhimg.com/v2-f2dd689cdf41a54da86cc7ee97ef3a48_1440w.jpg" /></figure><p>并且，可以看到，它们在最后的链中被合并为了StuffDocumentChain，也即把所有结果放到了最终调用中。点击进入就可以看到，系统消息中包含了来自前面文档的四个摘要：</p><figure><img class="origin_image zh-lightbox-thumb lazy" src="https://pic2.zhimg.com/v2-c1c97bee5c31b7ba560ab6f75afaa8ad_1440w.jpg" /></figure><h2><b>Refine</b></h2><figure><img class="origin_image zh-lightbox-thumb lazy" src="https://pic3.zhimg.com/v2-640d154953d5e7cac1932d11f2c11cce_1440w.jpg" /></figure><p>Refine方法的特点如下：</p><table><tbody><tr><th>特点</th><th>优点</th><th>缺点</th></tr><tr><td>迭代地处理多个文档，基于前一个文档的答案来构建一个更好的答案。</td><td>允许组合信息，更鼓励信息的传递</td><td>速度较慢</td></tr></tbody></table><div class="highlight"><pre><code class="language-text">qa_chain_mr = RetrievalQA.from_chain_type(
    llm,
    retriever=vectordb.as_retriever(),
    chain_type="refine"
)
result = qa_chain_mr({"query": question})
result["result"]
</code></pre></div><blockquote>❝ "The main topic of the class is machine learning algorithms, including linear regression and classification. Basic probability and statistics, as well as linear algebra, are prerequisites for the class, but the instructor will provide a refresher course on these topics in some of the discussion sections. Later in the quarter, the discussion sections will also cover extensions for the material taught in the main lectures. The instructor will focus on a few important extensions that there wasn't enough time to cover in the main lectures."<br /> </blockquote><p>现在还有一个问题，我们目前使用的链是没有“记忆”这个概念的，这就导致了它不会记住之前的问题或答案。为了解决这个问题，我们需要引入“记忆”功能，这就是我们下一节要讲的内容。</p><h2><b>Chat</b> </h2><figure><img class="origin_image zh-lightbox-thumb lazy" src="https://pic1.zhimg.com/v2-0a64be73f13422b3419f2e6b70ec8c3c_1440w.jpg" /></figure><p>在这节课中，我们将构建一个完整的问答聊天机器人，它将结合我们之前讲过的所有组件，并引入“聊天历史”这个概念，让它在回答问题时能够考虑到之前的对话或信息，也就是说，它能记住你之前说过什么。</p><h3><b>步骤1：初始化用于保存大量文档内容的向量数据库</b></h3><div class="highlight"><pre><code class="language-text">from langchain.vectorstores import Chroma
from langchain.embeddings.openai import OpenAIEmbeddings
persist_directory = 'docs/chroma/'
embedding = OpenAIEmbeddings()
vectordb = Chroma(persist_directory=persist_directory, embedding_function=embedding)
</code></pre></div><h3><b>步骤2：初始化将作为聊天机器人使用的语言模型</b></h3><div class="highlight"><pre><code class="language-text">llm_name = "gpt-3.5-turbo"
from langchain.chat_models import ChatOpenAI
llm = ChatOpenAI(model_name=llm_name, temperature=0)
</code></pre></div><h3><b>步骤3：初始化提示模板，让输出结果更简介、更少编造、更礼貌</b></h3><div class="highlight"><pre><code class="language-text"># 构建提示
from langchain.prompts import PromptTemplate
template = """Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. Use three sentences maximum. Keep the answer as concise as possible. Always say "thanks for asking!" at the end of the answer. 
{context}
Question: {question}
Helpful Answer:"""
QA_CHAIN_PROMPT = PromptTemplate(input_variables=["context", "question"],template=template,)

</code></pre></div><h3><b>步骤4：创建检索QA链，用于合并检索到的文本片段并调用语言模型</b></h3><div class="highlight"><pre><code class="language-text"># 运行链
from langchain.chains import RetrievalQA
qa_chain = RetrievalQA.from_chain_type(llm,
                                       retriever=vectordb.as_retriever(),
                                       return_source_documents=True,
                                       chain_type_kwargs={"prompt": QA_CHAIN_PROMPT})
</code></pre></div><h3><b>步骤5：使用ConversationBufferMemory增加聊天机器人的记忆功能</b></h3><div class="highlight"><pre><code class="language-text">from langchain.memory import ConversationBufferMemory
memory = ConversationBufferMemory(
    memory_key="chat_history",
    return_messages=True
)
</code></pre></div><p>ConversationBufferMemory提供了一个聊天消息历史的记忆缓冲区，并且每次都会将这部分历史消息传入聊天机器人。</p><p>return_messages=True表示将返回一个列表类型的聊天历史记录，而不是一个字符串。</p><h3><b>步骤6：创建ConversationalRetrievalChain（对话检索链），传入语言模型、检索器和记忆系统</b></h3><div class="highlight"><pre><code class="language-text">from langchain.chains import ConversationalRetrievalChain
retriever=vectordb.as_retriever()
qa = ConversationalRetrievalChain.from_llm(
    llm,
    retriever=retriever,
    memory=memory
)
</code></pre></div><p>ConversationalRetrievalChain会在RetrievalQAChain的基础上，将聊天历史和新提的问题整合成一个新的独立问题，以传递给向量存储库，查找相关文档。</p><h3><b>步骤7：使用PyPDFLoader加载所要参考的文档</b></h3><div class="highlight"><pre><code class="language-text">from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter
from langchain.vectorstores import DocArrayInMemorySearch
from langchain.document_loaders import TextLoader
from langchain.chains import RetrievalQA,  ConversationalRetrievalChain
from langchain.memory import ConversationBufferMemory
from langchain.chat_models import ChatOpenAI
from langchain.document_loaders import TextLoader
from langchain.document_loaders import PyPDFLoader

def load_db(file, chain_type, k):
    # 加载文档
    loader = PyPDFLoader(file)
    documents = loader.load()
    ...
</code></pre></div><h3><b>步骤8：分割文档，为每个分块创建嵌入，并存储到向量存储库中。</b></h3><div class="highlight"><pre><code class="language-text">def load_db(file, chain_type, k):
    ...
    # 分隔文档
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)
    docs = text_splitter.split_documents(documents)
    # 定义嵌入
    embeddings = OpenAIEmbeddings()
    # 基于文档数据创建向量数据库
    db = DocArrayInMemorySearch.from_documents(docs, embeddings)
    ...
</code></pre></div><h3><b>步骤9：从向量数据库创建一个基于“相似度”的检索器。</b></h3><div class="highlight"><pre><code class="language-text">def load_db(file, chain_type, k):
    ...
    # 定义检索器
    retriever = db.as_retriever(search_type="similarity", search_kwargs={"k": k})
    ...
</code></pre></div><h3><b>步骤10：创建对话检索链，用于将聊天历史和新提的问题整合成一个新的独立问题</b></h3><div class="highlight"><pre><code class="language-text">def load_db(file, chain_type, k):
    ...
    # create a chatbot chain. Memory is managed externally.
    qa = ConversationalRetrievalChain.from_llm(
        llm=ChatOpenAI(model_name=llm_name, temperature=0), 
        chain_type=chain_type, 
        retriever=retriever, 
        return_source_documents=True,
        return_generated_question=True,
    )
    return qa 
</code></pre></div><p>需要注意的是，这里我们并没有传入记忆系统，而是将记忆管理交给了GUI，这意味着聊天历史需要在链之外进行维护。</p><h3><b>步骤11：提供一个与聊天机器人交互的用户界面</b></h3><div class="highlight"><pre><code class="language-text">import panel as pn
import param

class cbfs(param.Parameterized):
    chat_history = param.List([])
    answer = param.String("")
    db_query  = param.String("")
    db_response = param.List([])
    
    def __init__(self,  **params):
        super(cbfs, self).__init__( **params)
        self.panels = []
        self.loaded_file = "docs/cs229_lectures/MachineLearning-Lecture01.pdf"
        self.qa = load_db(self.loaded_file,"stuff", 4)
    
    def call_load_db(self, count):
        if count == 0 or file_input.value is None:  # init or no file specified :
            return pn.pane.Markdown(f"Loaded File: {self.loaded_file}")
        else:
            file_input.save("temp.pdf")  # local copy
            self.loaded_file = file_input.filename
            button_load.button_style="outline"
            self.qa = load_db("temp.pdf", "stuff", 4)
            button_load.button_style="solid"
        self.clr_history()
        return pn.pane.Markdown(f"Loaded File: {self.loaded_file}")

    def convchain(self, query):
        if not query:
            return pn.WidgetBox(pn.Row('User:', pn.pane.Markdown("", width=600)), scroll=True)
        result = self.qa({"question": query, "chat_history": self.chat_history})
        self.chat_history.extend([(query, result["answer"])])
        self.db_query = result["generated_question"]
        self.db_response = result["source_documents"]
        self.answer = result['answer'] 
        self.panels.extend([
            pn.Row('User:', pn.pane.Markdown(query, width=600)),
            pn.Row('ChatBot:', pn.pane.Markdown(self.answer, width=600, style={'background-color': '#F6F6F6'}))
        ])
        inp.value = ''  #clears loading indicator when cleared
        return pn.WidgetBox(*self.panels,scroll=True)

    @param.depends('db_query ', )
    def get_lquest(self):
        if not self.db_query :
            return pn.Column(
                pn.Row(pn.pane.Markdown(f"Last question to DB:", styles={'background-color': '#F6F6F6'})),
                pn.Row(pn.pane.Str("no DB accesses so far"))
            )
        return pn.Column(
            pn.Row(pn.pane.Markdown(f"DB query:", styles={'background-color': '#F6F6F6'})),
            pn.pane.Str(self.db_query )
        )

    @param.depends('db_response', )
    def get_sources(self):
        if not self.db_response:
            return 
        rlist=[pn.Row(pn.pane.Markdown(f"Result of DB lookup:", styles={'background-color': '#F6F6F6'}))]
        for doc in self.db_response:
            rlist.append(pn.Row(pn.pane.Str(doc)))
        return pn.WidgetBox(*rlist, width=600, scroll=True)

    @param.depends('convchain', 'clr_history') 
    def get_chats(self):
        if not self.chat_history:
            return pn.WidgetBox(pn.Row(pn.pane.Str("No History Yet")), width=600, scroll=True)
        rlist=[pn.Row(pn.pane.Markdown(f"Current Chat History variable", styles={'background-color': '#F6F6F6'}))]
        for exchange in self.chat_history:
            rlist.append(pn.Row(pn.pane.Str(exchange)))
        return pn.WidgetBox(*rlist, width=600, scroll=True)

    def clr_history(self,count=0):
        self.chat_history = []
        return 

cb = cbfs()

file_input = pn.widgets.FileInput(accept='.pdf')
button_load = pn.widgets.Button(name="Load DB", button_type='primary')
button_clearhistory = pn.widgets.Button(name="Clear History", button_type='warning')
button_clearhistory.on_click(cb.clr_history)
inp = pn.widgets.TextInput( placeholder='Enter text here…')

bound_button_load = pn.bind(cb.call_load_db, button_load.param.clicks)
conversation = pn.bind(cb.convchain, inp) 

jpg_pane = pn.pane.Image( './img/convchain.jpg')

tab1 = pn.Column(
    pn.Row(inp),
    pn.layout.Divider(),
    pn.panel(conversation,  loading_indicator=True, height=300),
    pn.layout.Divider(),
)
tab2= pn.Column(
    pn.panel(cb.get_lquest),
    pn.layout.Divider(),
    pn.panel(cb.get_sources ),
)
tab3= pn.Column(
    pn.panel(cb.get_chats),
    pn.layout.Divider(),
)
tab4=pn.Column(
    pn.Row( file_input, button_load, bound_button_load),
    pn.Row( button_clearhistory, pn.pane.Markdown("Clears chat history. Can use to start a new topic" )),
    pn.layout.Divider(),
    pn.Row(jpg_pane.clone(width=400))
)
dashboard = pn.Column(
    pn.Row(pn.pane.Markdown('# ChatWithYourData_Bot')),
    pn.Tabs(('Conversation', tab1), ('Database', tab2), ('Chat History', tab3),('Configure', tab4))
)
dashboard
</code></pre></div><h3><b>步骤12：在运行起来的用户界面上进行实际的问答对话。</b></h3><figure><img class="origin_image zh-lightbox-thumb lazy" src="https://pic1.zhimg.com/v2-b60e4437857858a11df9a67a1f1423c4_1440w.jpg" /></figure>
]]></content:encoded>
<pubDate>Sun, 20 Aug 2023 13:49:07 GMT</pubDate>
<pubDate>Sun, 20 Aug 2023 13:49:07 GMT</pubDate>
</item>
<item>
<title>星际码仔赞同了文章: 【吴恩达 X HuggingFace】使用Gradio快速构建生成式AI应用-文字版</title>
<link>https://zhuanlan.zhihu.com/p/647240843</link>
<guid>https://zhuanlan.zhihu.com/p/647240843</guid>
<content:encoded><![CDATA[

<h2>介绍</h2><p>吴恩达 DeepLearningAI 与 HuggingFace 共同推出了一门新课，教大家如何使用 Gradio 快速构建生成式AI的应用。</p><p>课程由Hugging Face与DeepLearning.ai合作推出,讲师是来自Hugging Face的Apolinario Passos。</p><p>链接：<a class=" external" href="https://www.deeplearning.ai/short-courses/building-generative-ai-applications-with-gradio/" rel="nofollow noreferrer" target="_blank"><span class="invisible">https://www.</span><span class="visible">deeplearning.ai/short-c</span><span class="invisible">ourses/building-generative-ai-applications-with-gradio/</span><span class="ellipsis"></span></a></p><ul><li>大约1小时，学习曲线非常平缓,无需复杂的代码基础</li><li>交互界面让非技术人员也可以轻松使用</li><li>基于Hugging Face的预训练模型,效果直接可见</li></ul><p>在本课程中,我们将学习如何使用Gradio来构建生成式AI应用的用户界面。Gradio是一个直接在Python中通过友好的网页界面演示机器学习模型的快速便捷的工具。</p><p>具体涵盖的内容包括:</p><ul><li> 使用几行代码创建一个非程序员也可以使用的应用,实现文本摘要和命名识别功能。<br /> </li><li> 创建一个图像描述应用,允许用户上传图像,使用图像到文本模型自动描述图像,并在应用中显示图像和描述。<br /> </li><li> 创建一个图像生成应用,输入文本并使用散布模型生成相应的图像,在应用中显示生成的图像。<br /> </li><li> 结合前两项功能,上传图像、描述图像、使用描述生成新图像。<br /> </li><li> 创建一个界面与开源大型语言模型LLM进行交互。<br /> </li></ul><p>对每个应用,讲师将展示在构建好机器学习部分之后,如何快速使用Gradio构建很酷的演示demo，让其他人可以交互和体验你所构建的系统。</p><p>通过本课程的学习,你将能够快速构建交互式的AI应用和原型,从而加速项目验证和上线。</p><h2>第一课：文本摘要应用和命名实体识别应用</h2><h3>课程内容</h3><p>这是关于使用Gradle构建生成式AI应用的课程的第一课。在本课中,构建了两个自然语言处理应用:文本摘要应用和命名实体识别应用。</p><ul><li> 使用distilbart-cnn模型构建了一个文本摘要应用。该模型专门用于文本摘要任务。使用Gradio创建了一个简单的用户界面,允许用户输入文本并生成摘要。<br /> </li><li> 使用了一个BERT模型构建了一个命名实体识别应用。该模型可以识别文本中的人名、地名和机构名实体。使用Gradio创建了一个用户界面,显示了带有实体高亮的原始文本。<br /> </li></ul><p>关于小型专家模型,课程中的主要观点包括:</p><ul><li> 相比大型通用语言模型,小型专家模型在特定NLP任务上的效果可以一样好。<br /> </li><li> 小型模型计算速度更快,部署使用更高效。大型模型计算资源消耗大,响应速度慢。<br /> </li><li> 小型模型训练成本更低。大型模型需要大量数据、计算资源和时间来训练。<br /> </li><li> 本课使用的DistilBART和BERT模型都是小型专家模型。DistilBART是BART的蒸馏版本,专为文本摘要任务设计。BERT经过微调,非常适合命名实体识别。<br /> </li><li> 小型模型往往是通过知识蒸馏从大型模型中学习而来,能够保留大型模型的部分性能。<br /> </li><li> 对于生成对话、长文本生成等任务,大型模型效果更好。但对于专门的NLP任务,小型专家模型可以是一个更好的选择。<br /> </li><li> 构建文本摘要应用程序<br /> </li></ul><p><b>获取API token</b></p><p><code>HF_API_KEY</code> 是指 Hugging Face API 的访问密钥，您可以通过以下步骤获取：</p><ul><li>访问 Hugging Face 的官方网站：<a class=" wrap external" href="https://huggingface.co/" rel="nofollow noreferrer" target="_blank">https://huggingface.co/ </a></li><li>登录后，点击右上角的用户名，选择“Settings”。</li><li>在“Settings”页面中，选择“API tokens”选项卡。</li><li>在“API tokens”页面中，点击“New token”按钮。</li><li>在弹出的对话框中，输入一个名称来标识这个新的 API token，例如“my-hf-api-key”。</li><li>点击“Create new API token”按钮，系统会生成一个新的 API token。</li><li>将这个新生成的 API token 复制并保存好，它将作为您在代码中使用 <code>HF_API_KEY</code> 变量的值。</li></ul><p>请注意，Hugging Face API 是一个免费的开放式 API，但是有一些访问频率和配额的限制。使用 API 时请遵循 Hugging Face API 的使用规则。</p><p><b>使用API请求进行推理</b></p><p>选择要运行的模型。转到模型中心并选择您想要使用的模型。</p><div class="highlight"><pre><code class="language-text">ENDPOINT = https://api-inference.huggingface.co/models/&lt;MODEL_ID&gt;</code></pre></div><p>官方文档：<a class=" external" href="https://huggingface.co/docs/api-inference/index" rel="nofollow noreferrer" target="_blank"><span class="invisible">https://</span><span class="visible">huggingface.co/docs/api</span><span class="invisible">-inference/index</span><span class="ellipsis"></span></a></p><h3>构建文本摘要应用程序</h3><div class="highlight"><pre><code class="language-python"><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">requests</span><span class="o">,</span> <span class="nn">json</span>
<span class="kn">from</span> <span class="nn">dotenv</span> <span class="kn">import</span> <span class="n">load_dotenv</span><span class="p">,</span> <span class="n">find_dotenv</span> <span class="c1">#pip install python-dotenv</span>

<span class="n">load_dotenv</span><span class="p">(</span><span class="n">find_dotenv</span><span class="p">())</span> <span class="c1"># read local .env file</span>
<span class="n">hf_api_key</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s1">'HF_API_KEY'</span><span class="p">]</span>


<span class="c1">#Summarization endpoint</span>
<span class="k">def</span> <span class="nf">get_completion</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">parameters</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span><span class="n">ENDPOINT_URL</span><span class="o">=</span><span class="s2">"https://api-inference.huggingface.co/models/sshleifer/distilbart-cnn-12-6"</span><span class="p">):</span>
    <span class="n">headers</span> <span class="o">=</span> <span class="p">{</span>
      <span class="s2">"Authorization"</span><span class="p">:</span> <span class="n">f</span><span class="s2">"Bearer {hf_api_key}"</span><span class="p">,</span>
      <span class="s2">"Content-Type"</span><span class="p">:</span> <span class="s2">"application/json"</span>
    <span class="p">}</span>
    <span class="n">data</span> <span class="o">=</span> <span class="p">{</span> <span class="s2">"inputs"</span><span class="p">:</span> <span class="n">inputs</span> <span class="p">}</span>
    <span class="k">if</span> <span class="n">parameters</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">data</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="s2">"parameters"</span><span class="p">:</span> <span class="n">parameters</span><span class="p">})</span>
    <span class="n">response</span> <span class="o">=</span> <span class="n">requests</span><span class="o">.</span><span class="n">request</span><span class="p">(</span><span class="s2">"POST"</span><span class="p">,</span>
                                <span class="n">ENDPOINT_URL</span><span class="p">,</span> <span class="n">headers</span><span class="o">=</span><span class="n">headers</span><span class="p">,</span>
                                <span class="n">data</span><span class="o">=</span><span class="n">json</span><span class="o">.</span><span class="n">dumps</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
                               <span class="p">)</span>
    <span class="k">return</span> <span class="n">json</span><span class="o">.</span><span class="n">loads</span><span class="p">(</span><span class="n">response</span><span class="o">.</span><span class="n">content</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="s2">"utf-8"</span><span class="p">))</span>

<span class="n">text</span> <span class="o">=</span> <span class="p">(</span><span class="s1">'''The tower is 324 metres (1,063 ft) tall, about the same height
</span><span class="s1">        as an 81-storey building, and the tallest structure in Paris. 
</span><span class="s1">        Its base is square, measuring 125 metres (410 ft) on each side. 
</span><span class="s1">        During its construction, the Eiffel Tower surpassed the Washington 
</span><span class="s1">        Monument to become the tallest man-made structure in the world,
</span><span class="s1">        a title it held for 41 years until the Chrysler Building
</span><span class="s1">        in New York City was finished in 1930. It was the first structure 
</span><span class="s1">        to reach a height of 300 metres. Due to the addition of a broadcasting 
</span><span class="s1">        aerial at the top of the tower in 1957, it is now taller than the 
</span><span class="s1">        Chrysler Building by 5.2 metres (17 ft). Excluding transmitters, the 
</span><span class="s1">        Eiffel Tower is the second tallest free-standing structure in France 
</span><span class="s1">        after the Millau Viaduct.'''</span><span class="p">)</span>

<span class="n">result</span> <span class="o">=</span> <span class="n">get_completion</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">result</span><span class="p">)</span></code></pre></div><p>结果：</p><div class="highlight"><pre><code class="language-json"><span class="p">[{</span><span class="err">'summary_text':</span> <span class="err">'</span> <span class="err">The</span> <span class="err">tower</span> <span class="err">is</span> <span class="err">324</span> <span class="err">metres</span> <span class="err">(1,063</span> <span class="err">ft)</span> <span class="err">tall,</span> <span class="err">about</span> <span class="err">the</span> <span class="err">same</span> <span class="err">height</span> <span class="err">as</span> <span class="err">an</span> <span class="err">81-storey</span> <span class="err">building</span> <span class="err">.</span> <span class="err">It</span> <span class="err">is</span> <span class="err">the</span> <span class="err">tallest</span> <span class="err">structure</span> <span class="err">in</span> <span class="err">Paris</span> <span class="err">and</span> <span class="err">the</span> <span class="err">second</span> <span class="err">tallest</span> <span class="err">free-standing</span> <span class="err">structure</span> <span class="err">in</span> <span class="err">France</span> <span class="err">after</span> <span class="err">the</span> <span class="err">Millau</span> <span class="err">Viaduct</span> <span class="err">.</span> <span class="err">It</span> <span class="err">was</span> <span class="err">the</span> <span class="err">first</span> <span class="err">structure</span> <span class="err">in</span> <span class="err">the</span> <span class="err">world</span> <span class="err">to</span> <span class="err">reach</span> <span class="err">a</span> <span class="err">height</span> <span class="err">of</span> <span class="err">300</span> <span class="err">metres</span> <span class="err">.'</span><span class="p">}]</span></code></pre></div><p><b>如何在本地运行</b></p><p>参考：<a class=" external" href="https://huggingface.co/docs/transformers/quicktour" rel="nofollow noreferrer" target="_blank"><span class="invisible">https://</span><span class="visible">huggingface.co/docs/tra</span><span class="invisible">nsformers/quicktour</span><span class="ellipsis"></span></a></p><div class="highlight"><pre><code class="language-text">pip install torch
 pip install transformers
from transformers import pipeline

get_completion = pipeline("summarization", model="shleifer/distilbart-cnn-12-6")


def summarize(input):
    output = get_completion(input)
    return output[0]['summary_text']


text = ('''The tower is 324 metres (1,063 ft) tall, about the same height
        as an 81-storey building, and the tallest structure in Paris. 
        Its base is square, measuring 125 metres (410 ft) on each side. 
        During its construction, the Eiffel Tower surpassed the Washington 
        Monument to become the tallest man-made structure in the world,
        a title it held for 41 years until the Chrysler Building
        in New York City was finished in 1930. It was the first structure 
        to reach a height of 300 metres. Due to the addition of a broadcasting 
        aerial at the top of the tower in 1957, it is now taller than the 
        Chrysler Building by 5.2 metres (17 ft). Excluding transmitters, the 
        Eiffel Tower is the second tallest free-standing structure in France 
        after the Millau Viaduct.''')

get_completion(text)</code></pre></div><p><b>使用gradio</b></p><div class="highlight"><pre><code class="language-python"><span class="kn">import</span> <span class="nn">gradio</span> <span class="kn">as</span> <span class="nn">gr</span>


<span class="k">def</span> <span class="nf">summarize</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">get_completion</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">output</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s1">'summary_text'</span><span class="p">]</span>


<span class="n">demo</span> <span class="o">=</span> <span class="n">gr</span><span class="o">.</span><span class="n">Interface</span><span class="p">(</span><span class="n">fn</span><span class="o">=</span><span class="n">summarize</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="s2">"text"</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="s2">"text"</span><span class="p">)</span>
<span class="n">demo</span><span class="o">.</span><span class="n">launch</span><span class="p">(</span><span class="n">share</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">server_port</span><span class="o">=</span><span class="mi">8010</span><span class="p">)</span></code></pre></div><p><b>运行</b></p><p class="ztext-empty-paragraph"><br /></p><figure><img class="origin_image zh-lightbox-thumb" src="https://pic4.zhimg.com/v2-3bf5dc394d3b87b1184a5455bb6adc7f_1440w.jpg" /><figcaption>image-20230729153148105</figcaption></figure><p class="ztext-empty-paragraph"><br /></p><p><b>继续优化</b></p><div class="highlight"><pre><code class="language-python"><span class="n">demo</span> <span class="o">=</span> <span class="n">gr</span><span class="o">.</span><span class="n">Interface</span><span class="p">(</span><span class="n">fn</span><span class="o">=</span><span class="n">summarize</span><span class="p">,</span>
                    <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">gr</span><span class="o">.</span><span class="n">Textbox</span><span class="p">(</span><span class="n">label</span><span class="o">=</span><span class="s1">'Text to summarize'</span><span class="p">,</span><span class="n">lines</span><span class="o">=</span><span class="mi">6</span><span class="p">)],</span>
                    <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="n">gr</span><span class="o">.</span><span class="n">Textbox</span><span class="p">(</span><span class="n">label</span><span class="o">=</span><span class="s2">"Result"</span><span class="p">,</span><span class="n">lines</span><span class="o">=</span><span class="mi">6</span><span class="p">)],</span>
                    <span class="n">title</span><span class="o">=</span><span class="s2">"Text summarization with distilbart-cnn"</span><span class="p">,</span>
                    <span class="n">description</span><span class="o">=</span><span class="s2">"Summarize any text using the `shleifer/distilbart-cnn-12-6` model under the hood!"</span>
                    <span class="p">)</span>
<span class="n">demo</span><span class="o">.</span><span class="n">launch</span><span class="p">(</span><span class="n">share</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">server_port</span><span class="o">=</span><span class="mi">8010</span><span class="p">)</span></code></pre></div><p class="ztext-empty-paragraph"><br /></p><figure><img class="origin_image zh-lightbox-thumb" src="https://pic1.zhimg.com/v2-d1c18fc483ddd5324a91467523e2915c_1440w.jpg" /><figcaption>image-20230729155626725</figcaption></figure><p class="ztext-empty-paragraph"><br /></p><h3>构建命名实体识别应用程序</h3><div class="highlight"><pre><code class="language-python"><span class="n">text</span> <span class="o">=</span> <span class="s2">"My name is Andrew, I'm building DeepLearningAI and I live in California"</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">get_completion</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">parameters</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">ENDPOINT_URL</span><span class="o">=</span><span class="s1">'https://api-inference.huggingface.co/models/dslim/bert-base-NER'</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">result</span><span class="p">)</span></code></pre></div><p>运行结果</p><div class="highlight"><pre><code class="language-json"><span class="p">[{</span><span class="err">'entity_group':</span> <span class="err">'PER',</span> <span class="err">'score':</span> <span class="err">0.9990624785423279,</span> <span class="err">'word':</span> <span class="err">'Andrew',</span> <span class="err">'start':</span> <span class="err">11,</span> <span class="err">'end':</span> <span class="err">17</span><span class="p">},</span> <span class="p">{</span><span class="err">'entity_group':</span> <span class="err">'ORG',</span> <span class="err">'score':</span> <span class="err">0.896050214767456,</span> <span class="err">'word':</span> <span class="err">'DeepLearningAI',</span> <span class="err">'start':</span> <span class="err">32,</span> <span class="err">'end':</span> <span class="err">46</span><span class="p">},</span> <span class="p">{</span><span class="err">'entity_group':</span> <span class="err">'LOC',</span> <span class="err">'score':</span> <span class="err">0.999692440032959,</span> <span class="err">'word':</span> <span class="err">'California',</span> <span class="err">'start':</span> <span class="err">61,</span> <span class="err">'end':</span> <span class="err">71</span><span class="p">}]</span></code></pre></div><p><b>使用gradio</b></p><div class="highlight"><pre><code class="language-python"><span class="k">def</span> <span class="nf">ner</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">get_completion</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">parameters</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">ENDPOINT_URL</span><span class="o">=</span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s1">'HF_API_NER_BASE'</span><span class="p">])</span>
    <span class="k">for</span> <span class="n">entity</span> <span class="ow">in</span> <span class="n">output</span><span class="p">:</span>
        <span class="n">entity</span><span class="p">[</span><span class="s2">"entity"</span><span class="p">]</span> <span class="o">=</span> <span class="n">entity</span><span class="p">[</span><span class="s1">'entity_group'</span><span class="p">]</span>
    <span class="k">return</span> <span class="p">{</span><span class="s2">"text"</span><span class="p">:</span> <span class="nb">input</span><span class="p">,</span> <span class="s2">"entities"</span><span class="p">:</span> <span class="n">output</span><span class="p">}</span>
<span class="n">demo</span> <span class="o">=</span> <span class="n">gr</span><span class="o">.</span><span class="n">Interface</span><span class="p">(</span><span class="n">fn</span><span class="o">=</span><span class="n">ner</span><span class="p">,</span>
                    <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">gr</span><span class="o">.</span><span class="n">Textbox</span><span class="p">(</span><span class="n">label</span><span class="o">=</span><span class="s2">"Text to find entities"</span><span class="p">,</span> <span class="n">lines</span><span class="o">=</span><span class="mi">2</span><span class="p">)],</span>
                    <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="n">gr</span><span class="o">.</span><span class="n">HighlightedText</span><span class="p">(</span><span class="n">label</span><span class="o">=</span><span class="s2">"Text with entities"</span><span class="p">)],</span>
                    <span class="n">title</span><span class="o">=</span><span class="s2">"NER with dslim/bert-base-NER"</span><span class="p">,</span>
                    <span class="n">description</span><span class="o">=</span><span class="s2">"Find entities using the `dslim/bert-base-NER` model under the hood!"</span><span class="p">,</span>
                    <span class="n">allow_flagging</span><span class="o">=</span><span class="s2">"never"</span><span class="p">,</span>
                    <span class="c1"># Here we introduce a new tag, examples, easy to use examples for your application</span>
                    <span class="n">examples</span><span class="o">=</span><span class="p">[</span><span class="s2">"My name is Andrew and I live in California"</span><span class="p">,</span> <span class="s2">"My name is Poli and work at HuggingFace"</span><span class="p">])</span>
<span class="n">demo</span><span class="o">.</span><span class="n">launch</span><span class="p">()</span></code></pre></div><p>输入：<i>My name is Andrew, I'm building DeepLearningAI and I live in California</i></p><p class="ztext-empty-paragraph"><br /></p><figure><img class="origin_image zh-lightbox-thumb" src="https://pic2.zhimg.com/v2-cda66fce3bac1d3e61a32c6c1d203839_1440w.jpg" /><figcaption>image-20230729172549710</figcaption></figure><p class="ztext-empty-paragraph"><br /></p><h2>第二课：图片描述应用程序</h2><h3>课程内容</h3><p>这部分介绍了如何使用开源的图像到文本模型构建图像描述应用。</p><p>使用的是Salesforce Blip图像描述模型,可以将图像作为输入,输出图像的描述。该模型是通过大量图像和描述文本对进行训练的。</p><ul><li> 测试模型功能,传入图像URL,模型可以生成相应的描述文本。<br /> </li><li> 使用Gradio构建了用户界面,可以上传图像并获得描述。主要是使用Gradio的Image组件作为输入。<br /> </li></ul><p>界面与前面教程中的类似,但可以上传图像,对不同的图像都可以生成比较好的描述。</p><p>一些具体的例子:</p><ul><li> 上传狗的图片,可以生成“戴圣诞帽子的狗”的描述。<br /> </li><li> 上传飞翔的鸟,生成“在空中飞的鸟”。<br /> </li><li> 上传奶牛,生成“两头奶牛站在有湖的田野中”等。<br /> </li></ul><h3>构建应用程序</h3><p><a class=" external" href="https://api-inference.huggingface.co/models/Salesforce/blip-image-captioning-base" rel="nofollow noreferrer" target="_blank"><span class="invisible">https://</span><span class="visible">api-inference.huggingface.co</span><span class="invisible">/models/Salesforce/blip-image-captioning-base</span><span class="ellipsis"></span></a></p><div class="highlight"><pre><code class="language-python"><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">requests</span><span class="o">,</span> <span class="nn">json</span>

<span class="n">hf_api_key</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s1">'HF_API_KEY'</span><span class="p">]</span>


<span class="c1"># Image-to-text endpoint</span>
<span class="k">def</span> <span class="nf">get_completion</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">parameters</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">ENDPOINT_URL</span><span class="o">=</span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s1">'HF_API_ITT_BASE'</span><span class="p">]):</span>
    <span class="n">headers</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">"Authorization"</span><span class="p">:</span> <span class="n">f</span><span class="s2">"Bearer {hf_api_key}"</span><span class="p">,</span>
        <span class="s2">"Content-Type"</span><span class="p">:</span> <span class="s2">"application/json"</span>
    <span class="p">}</span>
    <span class="n">data</span> <span class="o">=</span> <span class="p">{</span><span class="s2">"inputs"</span><span class="p">:</span> <span class="n">inputs</span><span class="p">}</span>
    <span class="k">if</span> <span class="n">parameters</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">data</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="s2">"parameters"</span><span class="p">:</span> <span class="n">parameters</span><span class="p">})</span>
    <span class="n">response</span> <span class="o">=</span> <span class="n">requests</span><span class="o">.</span><span class="n">request</span><span class="p">(</span><span class="s2">"POST"</span><span class="p">,</span>
                                <span class="n">ENDPOINT_URL</span><span class="p">,</span>
                                <span class="n">headers</span><span class="o">=</span><span class="n">headers</span><span class="p">,</span>
                                <span class="n">data</span><span class="o">=</span><span class="n">json</span><span class="o">.</span><span class="n">dumps</span><span class="p">(</span><span class="n">data</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">json</span><span class="o">.</span><span class="n">loads</span><span class="p">(</span><span class="n">response</span><span class="o">.</span><span class="n">content</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="s2">"utf-8"</span><span class="p">))</span>


<span class="n">image_url</span> <span class="o">=</span> <span class="s2">"https://free-images.com/sm/9596/dog_animal_greyhound_983023.jpg"</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">get_completion</span><span class="p">(</span><span class="n">image_url</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">result</span><span class="p">)</span></code></pre></div><h3>使用gradio</h3><p>使用过程中遇到一个错误：</p><p>ImportError: cannot import name 'TypeAliasType' from 'typing_extensions'</p><p><b>解决办法：</b></p><div class="highlight"><pre><code class="language-text">pip install -U typing_extensions</code></pre></div><p>较老版本的typing_extensions中可能没有TypeAliasType。升级到最新版本可以解决。</p><div class="highlight"><pre><code class="language-python"><span class="n">demo</span> <span class="o">=</span> <span class="n">gr</span><span class="o">.</span><span class="n">Interface</span><span class="p">(</span><span class="n">fn</span><span class="o">=</span><span class="n">captioner</span><span class="p">,</span>
                    <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">gr</span><span class="o">.</span><span class="n">Image</span><span class="p">(</span><span class="n">label</span><span class="o">=</span><span class="s2">"Upload image"</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="s2">"pil"</span><span class="p">)],</span>
                    <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="n">gr</span><span class="o">.</span><span class="n">Textbox</span><span class="p">(</span><span class="n">label</span><span class="o">=</span><span class="s2">"Caption"</span><span class="p">)],</span>
                    <span class="n">title</span><span class="o">=</span><span class="s2">"Image Captioning with BLIP"</span><span class="p">,</span>
                    <span class="n">description</span><span class="o">=</span><span class="s2">"Caption any image using the BLIP model"</span><span class="p">,</span>
                    <span class="n">allow_flagging</span><span class="o">=</span><span class="s2">"never"</span><span class="p">,</span>
                    <span class="n">examples</span><span class="o">=</span><span class="p">[</span><span class="s2">"christmas_dog.jpeg"</span><span class="p">])</span>
<span class="n">demo</span><span class="o">.</span><span class="n">launch</span><span class="p">()</span></code></pre></div><p class="ztext-empty-paragraph"><br /></p><figure><img class="origin_image zh-lightbox-thumb" src="https://pic1.zhimg.com/v2-292db0f1203a8852c57ff3de46824b5c_1440w.jpg" /><figcaption>image-20230729215509358</figcaption></figure><p class="ztext-empty-paragraph"><br /></p><h2>第三课：图像生成应用</h2><h3>课程内容</h3><p>这节课的第一部分主要是构建了一个图像生成的应用:</p><ol><li>使用了基于Diffusion模型的图像生成技术,可以从文本描述中生成图像。</li><li>通过简单的Python代码调用模型接口,只需要提供文本提示即可生成图像。</li><li>使用Gradio构建了一个简洁的网页界面,可以自定义文本提示来生成图像。</li><li>接口可以控制生成图像的大小和细节程度等参数。</li></ol><p>这节课的第二部分介绍了Gradio Blocks,它可以创建更复杂的用户界面布局。</p><p>主要的Block元素包括:</p><ul><li>Row - 行</li><li>Column - 列</li><li>Button - 按钮</li><li>Textbox - 文本框</li><li>Accordion - 手风琴折叠面板</li></ul><p>通过这些元素的组合,我们可以自定义界面布局。</p><p>例如,把Prompt放在独立框中,高级选项收纳在手风琴中;用Row和Column布局各个元素等。</p><p>相比Gradio Interface更简单的代码,Blocks需要写更复杂的代码来定义布局。这是一种代码简洁与界面简洁的权衡。</p><p>总体而言,Gradio Blocks提供了更多控制界面外观的能力,让我们可以自定义生成更符合产品需求的用户界面。</p><h3>构建应用程序</h3><div class="highlight"><pre><code class="language-python"><span class="kn">import</span> <span class="nn">json</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">from</span> <span class="nn">io</span> <span class="kn">import</span> <span class="n">BytesIO</span>

<span class="kn">from</span> <span class="nn">PIL</span> <span class="kn">import</span> <span class="n">Image</span>
<span class="kn">import</span> <span class="nn">requests</span>

<span class="kn">from</span> <span class="nn">load_env</span> <span class="kn">import</span> <span class="n">loadenv</span>


<span class="k">def</span> <span class="nf">get_completion</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">parameters</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">ENDPOINT_URL</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="n">headers</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">"Authorization"</span><span class="p">:</span> <span class="n">f</span><span class="s2">"Bearer {os.environ['HF_API_KEY']}"</span><span class="p">,</span>
        <span class="s2">"Content-Type"</span><span class="p">:</span> <span class="s2">"application/json"</span>
    <span class="p">}</span>
    <span class="k">if</span> <span class="n">ENDPOINT_URL</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">ENDPOINT_URL</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s1">'HF_API_TTI_BASE'</span><span class="p">]</span>
    <span class="n">data</span> <span class="o">=</span> <span class="p">{</span><span class="s2">"inputs"</span><span class="p">:</span> <span class="n">inputs</span><span class="p">}</span>
    <span class="k">if</span> <span class="n">parameters</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">data</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="s2">"parameters"</span><span class="p">:</span> <span class="n">parameters</span><span class="p">})</span>
    <span class="n">response</span> <span class="o">=</span> <span class="n">requests</span><span class="o">.</span><span class="n">request</span><span class="p">(</span><span class="s2">"POST"</span><span class="p">,</span>
                                <span class="n">ENDPOINT_URL</span><span class="p">,</span>
                                <span class="n">headers</span><span class="o">=</span><span class="n">headers</span><span class="p">,</span>
                                <span class="n">data</span><span class="o">=</span><span class="n">json</span><span class="o">.</span><span class="n">dumps</span><span class="p">(</span><span class="n">data</span><span class="p">))</span>
    <span class="n">img</span> <span class="o">=</span> <span class="n">Image</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="n">BytesIO</span><span class="p">(</span><span class="n">response</span><span class="o">.</span><span class="n">content</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">img</span>


<span class="n">loadenv</span><span class="p">()</span>
<span class="n">prompt</span> <span class="o">=</span> <span class="s2">"a dog in a park"</span>
<span class="n">img</span> <span class="o">=</span> <span class="n">get_completion</span><span class="p">(</span><span class="n">prompt</span><span class="p">)</span>
<span class="n">img</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="s1">'img.jpg'</span><span class="p">)</span></code></pre></div><p><b>运行后输出图片</b></p><p class="ztext-empty-paragraph"><br /></p><figure><img class="origin_image zh-lightbox-thumb" src="https://pic4.zhimg.com/v2-c86e1f2d8e04543ecff8c2457ce563c3_1440w.jpg" /><figcaption>image-20230731115827883</figcaption></figure><p class="ztext-empty-paragraph"><br /></p><h3>使用gradio</h3><div class="highlight"><pre><code class="language-python"><span class="k">def</span> <span class="nf">generate</span><span class="p">(</span><span class="n">prompt</span><span class="p">):</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">get_completion</span><span class="p">(</span><span class="n">prompt</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">output</span>

<span class="n">demo</span> <span class="o">=</span> <span class="n">gr</span><span class="o">.</span><span class="n">Interface</span><span class="p">(</span><span class="n">fn</span><span class="o">=</span><span class="n">generate</span><span class="p">,</span>
                    <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">gr</span><span class="o">.</span><span class="n">Textbox</span><span class="p">(</span><span class="n">label</span><span class="o">=</span><span class="s2">"Your prompt"</span><span class="p">)],</span>
                    <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="n">gr</span><span class="o">.</span><span class="n">Image</span><span class="p">(</span><span class="n">label</span><span class="o">=</span><span class="s2">"Result"</span><span class="p">)],</span>
                    <span class="n">title</span><span class="o">=</span><span class="s2">"Image Generation with Stable Diffusion"</span><span class="p">,</span>
                    <span class="n">description</span><span class="o">=</span><span class="s2">"Generate any image with Stable Diffusion"</span><span class="p">,</span>
                    <span class="n">allow_flagging</span><span class="o">=</span><span class="s2">"never"</span><span class="p">,</span>
                    <span class="n">examples</span><span class="o">=</span><span class="p">[</span><span class="s2">"the spirit of a tamagotchi wandering in the city of Vienna"</span><span class="p">,</span><span class="s2">"a mecha robot in a favela"</span><span class="p">])</span>

<span class="n">demo</span><span class="o">.</span><span class="n">launch</span><span class="p">()</span></code></pre></div><p><b>运行后效果</b></p><p class="ztext-empty-paragraph"><br /></p><figure><img class="origin_image zh-lightbox-thumb" src="https://pic1.zhimg.com/v2-29a8a1a2e3bc477ba966c749af397180_1440w.jpg" /><figcaption>image-20230731120646823</figcaption></figure><p class="ztext-empty-paragraph"><br /></p><h3>让这个界面更高级一点</h3><p>增加一些参数：</p><div class="highlight"><pre><code class="language-python"><span class="k">def</span> <span class="nf">generate</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">negative_prompt</span><span class="p">,</span> <span class="n">steps</span><span class="p">,</span> <span class="n">guidance</span><span class="p">,</span> <span class="n">width</span><span class="p">,</span> <span class="n">height</span><span class="p">):</span>
    <span class="n">params</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">"negative_prompt"</span><span class="p">:</span> <span class="n">negative_prompt</span><span class="p">,</span>
        <span class="s2">"num_inference_steps"</span><span class="p">:</span> <span class="n">steps</span><span class="p">,</span>
        <span class="s2">"guidance_scale"</span><span class="p">:</span> <span class="n">guidance</span><span class="p">,</span>
        <span class="s2">"width"</span><span class="p">:</span> <span class="n">width</span><span class="p">,</span>
        <span class="s2">"height"</span><span class="p">:</span> <span class="n">height</span>
    <span class="p">}</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">get_completion</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span><span class="n">parameters</span><span class="o">=</span><span class="n">params</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">output</span>

<span class="n">demo</span> <span class="o">=</span> <span class="n">gr</span><span class="o">.</span><span class="n">Interface</span><span class="p">(</span><span class="n">fn</span><span class="o">=</span><span class="n">generate</span><span class="p">,</span>
                    <span class="n">inputs</span><span class="o">=</span><span class="p">[</span>
                        <span class="n">gr</span><span class="o">.</span><span class="n">Textbox</span><span class="p">(</span><span class="n">label</span><span class="o">=</span><span class="s2">"Your prompt"</span><span class="p">),</span>
                        <span class="n">gr</span><span class="o">.</span><span class="n">Textbox</span><span class="p">(</span><span class="n">label</span><span class="o">=</span><span class="s2">"Negative prompt"</span><span class="p">),</span>
                        <span class="n">gr</span><span class="o">.</span><span class="n">Slider</span><span class="p">(</span><span class="n">label</span><span class="o">=</span><span class="s2">"Inference Steps"</span><span class="p">,</span> <span class="n">minimum</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">maximum</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="mi">25</span><span class="p">,</span>
                                 <span class="n">info</span><span class="o">=</span><span class="s2">"In how many steps will the denoiser denoise the image?"</span><span class="p">),</span>
                        <span class="n">gr</span><span class="o">.</span><span class="n">Slider</span><span class="p">(</span><span class="n">label</span><span class="o">=</span><span class="s2">"Guidance Scale"</span><span class="p">,</span> <span class="n">minimum</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">maximum</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="mi">7</span><span class="p">,</span>
                                  <span class="n">info</span><span class="o">=</span><span class="s2">"Controls how much the text prompt influences the result"</span><span class="p">),</span>
                        <span class="n">gr</span><span class="o">.</span><span class="n">Slider</span><span class="p">(</span><span class="n">label</span><span class="o">=</span><span class="s2">"Width"</span><span class="p">,</span> <span class="n">minimum</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">maximum</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="mi">512</span><span class="p">),</span>
                        <span class="n">gr</span><span class="o">.</span><span class="n">Slider</span><span class="p">(</span><span class="n">label</span><span class="o">=</span><span class="s2">"Height"</span><span class="p">,</span> <span class="n">minimum</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">maximum</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="mi">512</span><span class="p">),</span>
                    <span class="p">],</span>
                    <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="n">gr</span><span class="o">.</span><span class="n">Image</span><span class="p">(</span><span class="n">label</span><span class="o">=</span><span class="s2">"Result"</span><span class="p">)],</span>
                    <span class="n">title</span><span class="o">=</span><span class="s2">"Image Generation with Stable Diffusion"</span><span class="p">,</span>
                    <span class="n">description</span><span class="o">=</span><span class="s2">"Generate any image with Stable Diffusion"</span><span class="p">,</span>
                    <span class="n">allow_flagging</span><span class="o">=</span><span class="s2">"never"</span>
                    <span class="p">)</span>
<span class="n">demo</span><span class="o">.</span><span class="n">launch</span><span class="p">()</span></code></pre></div><p>我们来更详细的解释一下这个代码中构建的输入控件:</p><ol><li> gr.Textbox(label="Your prompt")<br /> </li></ol><ul><li>这是一个文本输入框,用于输入主要的提示文字,来描述想要生成的图片内容。</li></ul><ol><li> gr.Textbox(label="Negative prompt")  <br /> </li></ol><ul><li>另一个文本框,用于输入负向提示,描述哪些内容不想生成,过滤不需要的结果。</li></ul><ol><li> gr.Slider(label="Inference Steps")<br /> </li></ol><ul><li>滑块控件,用于控制推理步数,也就是图片生成过程中噪声减少的迭代次数,值越大结果越精细。</li></ul><ol><li> gr.Slider(label="Guidance Scale")<br /> </li></ol><ul><li>控制文本提示的影响程度,值越大生成结果越符合提示内容。</li></ul><ol><li> gr.Slider(label="Width")<br /> </li></ol><ul><li>设置图片宽度,单位是像素。</li></ul><ol><li> gr.Slider(label="Height")<br /> </li></ol><ul><li>设置图片高度,单位是像素。</li></ul><p>所以通过这些输入控件,用户可以灵活地提供文本提示,并控制各种参数,来指导Stable Diffusion生成所需的图片结果。</p><p>这些输入会被传递给generate()函数,并用于驱动图片生成过程。</p><p><b>看看运行效果</b></p><p class="ztext-empty-paragraph"><br /></p><figure><img class="origin_image zh-lightbox-thumb" src="https://pic1.zhimg.com/v2-ef15eea6745841ba9d6dedf7ed5b7e80_1440w.jpg" /><figcaption>image-20230731122620466</figcaption></figure><p class="ztext-empty-paragraph"><br /></p><p>一些设计"Your prompt"的技巧:</p><ul><li>清楚详细地描述场景和对象特征</li><li>使用积极的词语,避免模糊含义的描述</li><li>避免过于抽象或模棱两可的概念</li></ul><p>设置"Negative prompt"的技巧:</p><ul><li>描述具体的错误内容以过滤不需要的结果</li><li>从一些不理想的生成结果中总结过滤词</li><li>多次迭代细调提示词,观察对结果的影响</li></ul><h3>使用gr.Blocks()进行布局</h3><p><code>gr.Blocks()</code>是Gradio中一个方便的组件,它可以将多个控件组合并排或者垂直显示,实现更复杂的布局。</p><p><b>来看两个示例</b></p><p><b>示例一</b></p><div class="highlight"><pre><code class="language-python"><span class="k">with</span> <span class="n">gr</span><span class="o">.</span><span class="n">Blocks</span><span class="p">()</span> <span class="k">as</span> <span class="n">demo</span><span class="p">:</span>
    <span class="n">gr</span><span class="o">.</span><span class="n">Markdown</span><span class="p">(</span><span class="s2">"# Image Generation with Stable Diffusion"</span><span class="p">)</span>
    <span class="n">prompt</span> <span class="o">=</span> <span class="n">gr</span><span class="o">.</span><span class="n">Textbox</span><span class="p">(</span><span class="n">label</span><span class="o">=</span><span class="s2">"Your prompt"</span><span class="p">)</span>
    <span class="k">with</span> <span class="n">gr</span><span class="o">.</span><span class="n">Row</span><span class="p">():</span>
        <span class="k">with</span> <span class="n">gr</span><span class="o">.</span><span class="n">Column</span><span class="p">():</span>
            <span class="n">negative_prompt</span> <span class="o">=</span> <span class="n">gr</span><span class="o">.</span><span class="n">Textbox</span><span class="p">(</span><span class="n">label</span><span class="o">=</span><span class="s2">"Negative prompt"</span><span class="p">)</span>
            <span class="n">steps</span> <span class="o">=</span> <span class="n">gr</span><span class="o">.</span><span class="n">Slider</span><span class="p">(</span><span class="n">label</span><span class="o">=</span><span class="s2">"Inference Steps"</span><span class="p">,</span> <span class="n">minimum</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">maximum</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="mi">25</span><span class="p">,</span>
                      <span class="n">info</span><span class="o">=</span><span class="s2">"In many steps will the denoiser denoise the image?"</span><span class="p">)</span>
            <span class="n">guidance</span> <span class="o">=</span> <span class="n">gr</span><span class="o">.</span><span class="n">Slider</span><span class="p">(</span><span class="n">label</span><span class="o">=</span><span class="s2">"Guidance Scale"</span><span class="p">,</span> <span class="n">minimum</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">maximum</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="mi">7</span><span class="p">,</span>
                      <span class="n">info</span><span class="o">=</span><span class="s2">"Controls how much the text prompt influences the result"</span><span class="p">)</span>
            <span class="n">width</span> <span class="o">=</span> <span class="n">gr</span><span class="o">.</span><span class="n">Slider</span><span class="p">(</span><span class="n">label</span><span class="o">=</span><span class="s2">"Width"</span><span class="p">,</span> <span class="n">minimum</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">maximum</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="mi">512</span><span class="p">)</span>
            <span class="n">height</span> <span class="o">=</span> <span class="n">gr</span><span class="o">.</span><span class="n">Slider</span><span class="p">(</span><span class="n">label</span><span class="o">=</span><span class="s2">"Height"</span><span class="p">,</span> <span class="n">minimum</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">maximum</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="mi">512</span><span class="p">)</span>
            <span class="n">btn</span> <span class="o">=</span> <span class="n">gr</span><span class="o">.</span><span class="n">Button</span><span class="p">(</span><span class="s2">"Submit"</span><span class="p">)</span>
        <span class="k">with</span> <span class="n">gr</span><span class="o">.</span><span class="n">Column</span><span class="p">():</span>
            <span class="n">output</span> <span class="o">=</span> <span class="n">gr</span><span class="o">.</span><span class="n">Image</span><span class="p">(</span><span class="n">label</span><span class="o">=</span><span class="s2">"Result"</span><span class="p">)</span>

    <span class="n">btn</span><span class="o">.</span><span class="n">click</span><span class="p">(</span><span class="n">fn</span><span class="o">=</span><span class="n">generate</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">prompt</span><span class="p">,</span><span class="n">negative_prompt</span><span class="p">,</span><span class="n">steps</span><span class="p">,</span><span class="n">guidance</span><span class="p">,</span><span class="n">width</span><span class="p">,</span><span class="n">height</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="n">output</span><span class="p">])</span>

<span class="n">demo</span><span class="o">.</span><span class="n">launch</span><span class="p">()</span></code></pre></div><p class="ztext-empty-paragraph"><br /></p><figure><img class="origin_image zh-lightbox-thumb" src="https://pic1.zhimg.com/v2-0d372f3ca6806084ade2cc9321145440_1440w.jpg" /><figcaption>image-20230731124205339</figcaption></figure><p class="ztext-empty-paragraph"><br /></p><ol><li>顶部是一个gr.Markdown组件,显示了"Image Generation with Stable Diffusion"作为页面标题。</li><li>接下来是prompt文本输入框,用于输入主要的图像生成提示词。</li><li>然后是一个gr.Row组件,开始了一个水平方向的布局。</li><li>在第一个gr.Column内,从上向下垂直排列了:</li><li>negative_prompt:负向提示词文本框</li><li>steps: 推理步数滑块</li><li>guidance: 指导比例滑块</li><li>width: 图片宽度滑块</li><li>height: 图片高度滑块</li><li>在第二个gr.Column内,放置了一个output图像框,用于显示生成结果。</li><li>页面底部是一个提交按钮btn。</li><li>点击btn时,会将所有输入传递给generate函数,并将结果渲染到output中。</li></ol><p><b>示例二</b></p><div class="highlight"><pre><code class="language-python"><span class="k">with</span> <span class="n">gr</span><span class="o">.</span><span class="n">Blocks</span><span class="p">()</span> <span class="k">as</span> <span class="n">demo</span><span class="p">:</span>
    <span class="n">gr</span><span class="o">.</span><span class="n">Markdown</span><span class="p">(</span><span class="s2">"# Image Generation with Stable Diffusion"</span><span class="p">)</span>
    <span class="k">with</span> <span class="n">gr</span><span class="o">.</span><span class="n">Row</span><span class="p">():</span>
        <span class="k">with</span> <span class="n">gr</span><span class="o">.</span><span class="n">Column</span><span class="p">(</span><span class="n">scale</span><span class="o">=</span><span class="mi">4</span><span class="p">):</span>
            <span class="n">prompt</span> <span class="o">=</span> <span class="n">gr</span><span class="o">.</span><span class="n">Textbox</span><span class="p">(</span><span class="n">label</span><span class="o">=</span><span class="s2">"Your prompt"</span><span class="p">)</span> <span class="c1">#Give prompt some real estate</span>
        <span class="k">with</span> <span class="n">gr</span><span class="o">.</span><span class="n">Column</span><span class="p">(</span><span class="n">scale</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">min_width</span><span class="o">=</span><span class="mi">50</span><span class="p">):</span>
            <span class="n">btn</span> <span class="o">=</span> <span class="n">gr</span><span class="o">.</span><span class="n">Button</span><span class="p">(</span><span class="s2">"Submit"</span><span class="p">)</span> <span class="c1">#Submit button side by side!</span>
    <span class="k">with</span> <span class="n">gr</span><span class="o">.</span><span class="n">Accordion</span><span class="p">(</span><span class="s2">"Advanced options"</span><span class="p">,</span> <span class="nb">open</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span> <span class="c1">#Let's hide the advanced options!</span>
            <span class="n">negative_prompt</span> <span class="o">=</span> <span class="n">gr</span><span class="o">.</span><span class="n">Textbox</span><span class="p">(</span><span class="n">label</span><span class="o">=</span><span class="s2">"Negative prompt"</span><span class="p">)</span>
            <span class="k">with</span> <span class="n">gr</span><span class="o">.</span><span class="n">Row</span><span class="p">():</span>
                <span class="k">with</span> <span class="n">gr</span><span class="o">.</span><span class="n">Column</span><span class="p">():</span>
                    <span class="n">steps</span> <span class="o">=</span> <span class="n">gr</span><span class="o">.</span><span class="n">Slider</span><span class="p">(</span><span class="n">label</span><span class="o">=</span><span class="s2">"Inference Steps"</span><span class="p">,</span> <span class="n">minimum</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">maximum</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="mi">25</span><span class="p">,</span>
                      <span class="n">info</span><span class="o">=</span><span class="s2">"In many steps will the denoiser denoise the image?"</span><span class="p">)</span>
                    <span class="n">guidance</span> <span class="o">=</span> <span class="n">gr</span><span class="o">.</span><span class="n">Slider</span><span class="p">(</span><span class="n">label</span><span class="o">=</span><span class="s2">"Guidance Scale"</span><span class="p">,</span> <span class="n">minimum</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">maximum</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="mi">7</span><span class="p">,</span>
                      <span class="n">info</span><span class="o">=</span><span class="s2">"Controls how much the text prompt influences the result"</span><span class="p">)</span>
                <span class="k">with</span> <span class="n">gr</span><span class="o">.</span><span class="n">Column</span><span class="p">():</span>
                    <span class="n">width</span> <span class="o">=</span> <span class="n">gr</span><span class="o">.</span><span class="n">Slider</span><span class="p">(</span><span class="n">label</span><span class="o">=</span><span class="s2">"Width"</span><span class="p">,</span> <span class="n">minimum</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">maximum</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="mi">512</span><span class="p">)</span>
                    <span class="n">height</span> <span class="o">=</span> <span class="n">gr</span><span class="o">.</span><span class="n">Slider</span><span class="p">(</span><span class="n">label</span><span class="o">=</span><span class="s2">"Height"</span><span class="p">,</span> <span class="n">minimum</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">maximum</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="mi">512</span><span class="p">)</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">gr</span><span class="o">.</span><span class="n">Image</span><span class="p">(</span><span class="n">label</span><span class="o">=</span><span class="s2">"Result"</span><span class="p">)</span> <span class="c1">#Move the output up too</span>

    <span class="n">btn</span><span class="o">.</span><span class="n">click</span><span class="p">(</span><span class="n">fn</span><span class="o">=</span><span class="n">generate</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">prompt</span><span class="p">,</span><span class="n">negative_prompt</span><span class="p">,</span><span class="n">steps</span><span class="p">,</span><span class="n">guidance</span><span class="p">,</span><span class="n">width</span><span class="p">,</span><span class="n">height</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="n">output</span><span class="p">])</span></code></pre></div><p class="ztext-empty-paragraph"><br /></p><figure><img class="origin_image zh-lightbox-thumb" src="https://pic1.zhimg.com/v2-7a904e838b214915c60b8dd2c270e07c_1440w.jpg" /><figcaption>image-20230731124349242</figcaption></figure><p class="ztext-empty-paragraph"><br /></p><p>这个代码展示了一些更高级的Gradio布局技巧:</p><ol><li>使用gr.Column的scale参数调整列的相对宽度比例。prompt文本框设置为4,按钮设置为1,让prompt占更多宽度。</li><li>设置min_width保证按钮列不会过窄。</li><li>使用gr.Accordion将高级选项打包起来,默认折叠隐藏。</li><li>嵌套gr.Row和gr.Column来进行复杂的多列布局:</li><li>外层Row分两列</li><li>左列嵌套Column垂直排列高级选项</li><li>右列嵌套Column垂直排列宽高滑块</li><li>将结果图像框移到顶部,放在提示文本下方。</li><li>按钮变为提示文本右侧的独立一列。</li></ol><h2>第四课：一个图像传话游戏的应用</h2><h3>课程内容</h3><p>本节课程将之前学到的图像描述和图像生成两个模型组合起来,构建了一个图像传话游戏的应用。</p><p>应用使用Gradio构建,定义了两个按钮,一个按钮生成图像描述,另一个按钮基于描述生成新图像。生成的图像可以再次输入模型,产生新的描述和图像,实现循环游戏。课程还展示了优化设计,使用单按钮一次完成描述生成和图像生成。</p><p>通过构建这个应用,回顾和运用了之前课程学习的知识点,包括图像描述、图像生成以及使用Gradio构建界面。</p><p>应用的核心是调用了多个预训练的生成AI模型,包括图像描述模型和图像生成Diffusion模型,通过简单的Python代码即可实现强大的功能。</p><h3>构建应用程序</h3><p><b>只使用图片描述</b></p><div class="highlight"><pre><code class="language-python"><span class="k">with</span> <span class="n">gr</span><span class="o">.</span><span class="n">Blocks</span><span class="p">()</span> <span class="k">as</span> <span class="n">demo</span><span class="p">:</span>
    <span class="n">gr</span><span class="o">.</span><span class="n">Markdown</span><span class="p">(</span><span class="s2">"# Describe-and-Generate game  ️"</span><span class="p">)</span>
    <span class="n">image_upload</span> <span class="o">=</span> <span class="n">gr</span><span class="o">.</span><span class="n">Image</span><span class="p">(</span><span class="n">label</span><span class="o">=</span><span class="s2">"Your first image"</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="s2">"pil"</span><span class="p">)</span>
    <span class="n">btn_caption</span> <span class="o">=</span> <span class="n">gr</span><span class="o">.</span><span class="n">Button</span><span class="p">(</span><span class="s2">"Generate caption"</span><span class="p">)</span>
    <span class="n">caption</span> <span class="o">=</span> <span class="n">gr</span><span class="o">.</span><span class="n">Textbox</span><span class="p">(</span><span class="n">label</span><span class="o">=</span><span class="s2">"Generated caption"</span><span class="p">)</span>

    <span class="n">btn_caption</span><span class="o">.</span><span class="n">click</span><span class="p">(</span><span class="n">fn</span><span class="o">=</span><span class="n">captioner</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">image_upload</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="n">caption</span><span class="p">])</span></code></pre></div><p class="ztext-empty-paragraph"><br /></p><figure><img class="origin_image zh-lightbox-thumb" src="https://pic4.zhimg.com/v2-2d74d351bbc1bfa1458e7c773a0f0103_1440w.jpg" /><figcaption>image-20230731180841880</figcaption></figure><p class="ztext-empty-paragraph"><br /></p><p><b>增加图片生成</b></p><div class="highlight"><pre><code class="language-python"><span class="k">with</span> <span class="n">gr</span><span class="o">.</span><span class="n">Blocks</span><span class="p">()</span> <span class="k">as</span> <span class="n">demo</span><span class="p">:</span>
    <span class="n">gr</span><span class="o">.</span><span class="n">Markdown</span><span class="p">(</span><span class="s2">"# Describe-and-Generate game  ️"</span><span class="p">)</span>
    <span class="n">image_upload</span> <span class="o">=</span> <span class="n">gr</span><span class="o">.</span><span class="n">Image</span><span class="p">(</span><span class="n">label</span><span class="o">=</span><span class="s2">"Your first image"</span><span class="p">,</span><span class="nb">type</span><span class="o">=</span><span class="s2">"pil"</span><span class="p">)</span>
    <span class="n">btn_caption</span> <span class="o">=</span> <span class="n">gr</span><span class="o">.</span><span class="n">Button</span><span class="p">(</span><span class="s2">"Generate caption"</span><span class="p">)</span>
    <span class="n">caption</span> <span class="o">=</span> <span class="n">gr</span><span class="o">.</span><span class="n">Textbox</span><span class="p">(</span><span class="n">label</span><span class="o">=</span><span class="s2">"Generated caption"</span><span class="p">)</span>
    <span class="n">btn_image</span> <span class="o">=</span> <span class="n">gr</span><span class="o">.</span><span class="n">Button</span><span class="p">(</span><span class="s2">"Generate image"</span><span class="p">)</span>
    <span class="n">image_output</span> <span class="o">=</span> <span class="n">gr</span><span class="o">.</span><span class="n">Image</span><span class="p">(</span><span class="n">label</span><span class="o">=</span><span class="s2">"Generated Image"</span><span class="p">)</span>
    <span class="n">btn_caption</span><span class="o">.</span><span class="n">click</span><span class="p">(</span><span class="n">fn</span><span class="o">=</span><span class="n">captioner</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">image_upload</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="n">caption</span><span class="p">])</span>
    <span class="n">btn_image</span><span class="o">.</span><span class="n">click</span><span class="p">(</span><span class="n">fn</span><span class="o">=</span><span class="n">generate</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">caption</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="n">image_output</span><span class="p">])</span></code></pre></div><p>这个代码实现了一个"描述并生成"的游戏流程:</p><ol><li> image_upload 图像上传框,用于上传第一张图片。<br /> </li><li> btn_caption 按钮,点击后为图像生成描述文字。<br /> </li><li> caption 文本框,显示生成的描述文字。<br /> </li><li> btn_image 按钮,基于生成的描述再生成一张图片。 <br /> </li><li> image_output 图像框,显示生成的图片。<br /> </li></ol><p>交互流程是:</p><ol><li> 上传一张图片<br /> </li><li> 点击 btn_caption 生成描述文字<br /> </li><li> 查看生成的描述文字<br /> </li><li> 点击 btn_image 基于描述生成新图片<br /> </li><li> 查看生成的新图片<br /> </li></ol><p>这个可以作为一个描述图像Caption和图像生成Text-to-Image模型的组合示例。</p><p>利用Gradio的事件绑定和交互流程设计,实现了一个端到端的图像描述生成游戏Demo。</p><p class="ztext-empty-paragraph"><br /></p><figure><img class="origin_image zh-lightbox-thumb" src="https://pic1.zhimg.com/v2-d1427c8d3bb4d23f113a083a2db3c464_1440w.jpg" /><figcaption>image-20230731182333933</figcaption></figure><p class="ztext-empty-paragraph"><br /></p><p><b>一次性完成描述图像Caption和图像生成</b></p><div class="highlight"><pre><code class="language-python"><span class="k">def</span> <span class="nf">caption_and_generate</span><span class="p">(</span><span class="n">image</span><span class="p">):</span>
    <span class="n">caption</span> <span class="o">=</span> <span class="n">captioner</span><span class="p">(</span><span class="n">image</span><span class="p">)</span>
    <span class="n">image</span> <span class="o">=</span> <span class="n">generate</span><span class="p">(</span><span class="n">caption</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">caption</span><span class="p">,</span> <span class="n">image</span><span class="p">]</span>

<span class="k">with</span> <span class="n">gr</span><span class="o">.</span><span class="n">Blocks</span><span class="p">()</span> <span class="k">as</span> <span class="n">demo</span><span class="p">:</span>
    <span class="n">gr</span><span class="o">.</span><span class="n">Markdown</span><span class="p">(</span><span class="s2">"# Describe-and-Generate game  ️"</span><span class="p">)</span>
    <span class="n">image_upload</span> <span class="o">=</span> <span class="n">gr</span><span class="o">.</span><span class="n">Image</span><span class="p">(</span><span class="n">label</span><span class="o">=</span><span class="s2">"Your first image"</span><span class="p">,</span><span class="nb">type</span><span class="o">=</span><span class="s2">"pil"</span><span class="p">)</span>
    <span class="n">btn_all</span> <span class="o">=</span> <span class="n">gr</span><span class="o">.</span><span class="n">Button</span><span class="p">(</span><span class="s2">"Caption and generate"</span><span class="p">)</span>
    <span class="n">caption</span> <span class="o">=</span> <span class="n">gr</span><span class="o">.</span><span class="n">Textbox</span><span class="p">(</span><span class="n">label</span><span class="o">=</span><span class="s2">"Generated caption"</span><span class="p">)</span>
    <span class="n">image_output</span> <span class="o">=</span> <span class="n">gr</span><span class="o">.</span><span class="n">Image</span><span class="p">(</span><span class="n">label</span><span class="o">=</span><span class="s2">"Generated Image"</span><span class="p">)</span>

    <span class="n">btn_all</span><span class="o">.</span><span class="n">click</span><span class="p">(</span><span class="n">fn</span><span class="o">=</span><span class="n">caption_and_generate</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">image_upload</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="n">caption</span><span class="p">,</span> <span class="n">image_output</span><span class="p">])</span></code></pre></div><p class="ztext-empty-paragraph"><br /></p><figure><img class="origin_image zh-lightbox-thumb" src="https://pic1.zhimg.com/v2-e0f84c722a9caa68b4ce55a369b374ac_1440w.jpg" /><figcaption>image-20230731182615427</figcaption></figure><p class="ztext-empty-paragraph"><br /></p><h2>第五课：基于开源LLM的聊天应用</h2><h3>课程内容</h3><p>本节课构建了一个与开源大型语言模型Falcon 40B聊天的应用。</p><ol><li>Falcon 40B是当前最好的开源语言模型之一。</li><li>使用text-generation库调用Falcon 40B的问答API接口。</li><li>首先仅仅在代码中与模型聊天,后续通过Gradio构建聊天界面。</li><li>Gradio聊天界面可以保存对话历史上下文。</li><li>在聊天过程中,需要将之前对话记录与新消息一起发送给模型,才能进行连续对话。</li><li>Gradio Chatbot组件可以简化发送历史对话上下文的过程。</li><li>通过text-generation库+Gradio Chatbot组件即可实现与开源语言模型的流畅聊天。</li><li>本课程使用免费的开源模型构建应用,可以自由自定义。</li><li>展示了利用开源模型和Gradio进行快速原型开发和验证的流程。</li><li>可以基于本节课 content构建更复杂的聊天机器人应用。</li></ol><h3>构建应用程序</h3><p>我们将使用 <code>falcon-40b-instruct</code>，它是在 <a class=" wrap external" href="https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard" rel="nofollow noreferrer" target="_blank">  Open LLM Leaderboard</a> 上排名最高的开源LLM之一，使用 <a class=" wrap external" href="https://huggingface.co/inference-endpoints" rel="nofollow noreferrer" target="_blank">Inference Endpoint</a> 进行推理。</p><div class="highlight"><pre><code class="language-python"><span class="c1"># Helper function</span>
<span class="kn">import</span> <span class="nn">requests</span><span class="o">,</span> <span class="nn">json</span>
<span class="kn">from</span> <span class="nn">text_generation</span> <span class="kn">import</span> <span class="n">Client</span>

<span class="c1">#FalcomLM-instruct endpoint on the text_generation library</span>
<span class="n">client</span> <span class="o">=</span> <span class="n">Client</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s1">'HF_API_FALCOM_BASE'</span><span class="p">],</span> <span class="n">headers</span><span class="o">=</span><span class="p">{</span><span class="s2">"Authorization"</span><span class="p">:</span> <span class="n">f</span><span class="s2">"Basic {hf_api_key}"</span><span class="p">},</span> <span class="n">timeout</span><span class="o">=</span><span class="mi">120</span><span class="p">)</span>

<span class="n">prompt</span> <span class="o">=</span> <span class="s2">"Has math been invented or discovered?"</span>
<span class="n">client</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">256</span><span class="p">)</span><span class="o">.</span><span class="n">generated_text</span></code></pre></div><p><b>输出</b></p><div class="highlight"><pre><code class="language-text">'\nMath has been both invented and discovered. It is a human invention in the sense that it is a system of rules and concepts that we have created to help us understand the world around us. However, it is also a discovery in the sense that it is a fundamental aspect of the universe that we have uncovered through our observations and experiments.'</code></pre></div><p><b>使用gradio</b></p><div class="highlight"><pre><code class="language-text">#Back to Lesson 2, time flies!
import gradio as gr
def generate(input, slider):
    output = client.generate(input, max_new_tokens=slider).generated_text
    return output

demo = gr.Interface(fn=generate, inputs=[gr.Textbox(label="Prompt"), gr.Slider(label="Max new tokens", value=20,  maximum=1024, minimum=1)], outputs=[gr.Textbox(label="Completion")])
gr.close_all()
demo.launch(share=True, server_port=int(os.environ['PORT1']))</code></pre></div><p class="ztext-empty-paragraph"><br /></p><figure><img class="origin_image zh-lightbox-thumb" src="https://pic3.zhimg.com/v2-e2ed3ee1bc01dbd3085beba44c75522e_1440w.jpg" /><figcaption>image-20230801100917681</figcaption></figure><p class="ztext-empty-paragraph"><br /></p><p><b>使用<code>gr.Chatbot()</code></b></p><p>gr.Chatbot组件用来显示对话历史。gr.Textbox组件msg用于输入提示。gr.Button组件btn触发对话。</p><div class="highlight"><pre><code class="language-python"><span class="kn">import</span> <span class="nn">random</span>

<span class="k">def</span> <span class="nf">respond</span><span class="p">(</span><span class="n">message</span><span class="p">,</span> <span class="n">chat_history</span><span class="p">):</span>
        <span class="c1">#No LLM here, just respond with a random pre-made message</span>
        <span class="n">bot_message</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">([</span><span class="s2">"Tell me more about it"</span><span class="p">,</span> 
                                     <span class="s2">"Cool, but I'm not interested"</span><span class="p">,</span> 
                                     <span class="s2">"Hmmmm, ok then"</span><span class="p">])</span> 
        <span class="n">chat_history</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">message</span><span class="p">,</span> <span class="n">bot_message</span><span class="p">))</span>
        <span class="k">return</span> <span class="s2">""</span><span class="p">,</span> <span class="n">chat_history</span>

<span class="k">with</span> <span class="n">gr</span><span class="o">.</span><span class="n">Blocks</span><span class="p">()</span> <span class="k">as</span> <span class="n">demo</span><span class="p">:</span>
    <span class="n">chatbot</span> <span class="o">=</span> <span class="n">gr</span><span class="o">.</span><span class="n">Chatbot</span><span class="p">(</span><span class="n">height</span><span class="o">=</span><span class="mi">240</span><span class="p">)</span> <span class="c1">#just to fit the notebook</span>
    <span class="n">msg</span> <span class="o">=</span> <span class="n">gr</span><span class="o">.</span><span class="n">Textbox</span><span class="p">(</span><span class="n">label</span><span class="o">=</span><span class="s2">"Prompt"</span><span class="p">)</span>
    <span class="n">btn</span> <span class="o">=</span> <span class="n">gr</span><span class="o">.</span><span class="n">Button</span><span class="p">(</span><span class="s2">"Submit"</span><span class="p">)</span>
    <span class="n">clear</span> <span class="o">=</span> <span class="n">gr</span><span class="o">.</span><span class="n">ClearButton</span><span class="p">(</span><span class="n">components</span><span class="o">=</span><span class="p">[</span><span class="n">msg</span><span class="p">,</span> <span class="n">chatbot</span><span class="p">],</span> <span class="n">value</span><span class="o">=</span><span class="s2">"Clear console"</span><span class="p">)</span>

    <span class="n">btn</span><span class="o">.</span><span class="n">click</span><span class="p">(</span><span class="n">respond</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">msg</span><span class="p">,</span> <span class="n">chatbot</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="n">msg</span><span class="p">,</span> <span class="n">chatbot</span><span class="p">])</span>
    <span class="n">msg</span><span class="o">.</span><span class="n">submit</span><span class="p">(</span><span class="n">respond</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">msg</span><span class="p">,</span> <span class="n">chatbot</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="n">msg</span><span class="p">,</span> <span class="n">chatbot</span><span class="p">])</span> <span class="c1">#Press enter to submit</span>
<span class="n">gr</span><span class="o">.</span><span class="n">close_all</span><span class="p">()</span>
<span class="n">demo</span><span class="o">.</span><span class="n">launch</span><span class="p">(</span><span class="n">share</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">server_port</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s1">'PORT2'</span><span class="p">]))</span></code></pre></div><p><b>效果长这样</b></p><p class="ztext-empty-paragraph"><br /></p><figure><img class="origin_image zh-lightbox-thumb" src="https://pic2.zhimg.com/v2-635af3e11f8ae2ea93ce86ce0c2174dd_1440w.jpg" /><figcaption>image-20230801101540011</figcaption></figure><p class="ztext-empty-paragraph"><br /></p><p><b>接下来结合llm模型使用gradio</b></p><div class="highlight"><pre><code class="language-python"><span class="k">def</span> <span class="nf">format_chat_prompt</span><span class="p">(</span><span class="n">message</span><span class="p">,</span> <span class="n">chat_history</span><span class="p">):</span>
    <span class="n">prompt</span> <span class="o">=</span> <span class="s2">""</span>
    <span class="k">for</span> <span class="n">turn</span> <span class="ow">in</span> <span class="n">chat_history</span><span class="p">:</span>
        <span class="n">user_message</span><span class="p">,</span> <span class="n">bot_message</span> <span class="o">=</span> <span class="n">turn</span>
        <span class="n">prompt</span> <span class="o">=</span> <span class="n">f</span><span class="s2">"{prompt}</span><span class="se">\n</span><span class="s2">User: {user_message}</span><span class="se">\n</span><span class="s2">Assistant: {bot_message}"</span>
    <span class="n">prompt</span> <span class="o">=</span> <span class="n">f</span><span class="s2">"{prompt}</span><span class="se">\n</span><span class="s2">User: {message}</span><span class="se">\n</span><span class="s2">Assistant:"</span>
    <span class="k">return</span> <span class="n">prompt</span>

<span class="k">def</span> <span class="nf">respond</span><span class="p">(</span><span class="n">message</span><span class="p">,</span> <span class="n">chat_history</span><span class="p">):</span>
        <span class="n">formatted_prompt</span> <span class="o">=</span> <span class="n">format_chat_prompt</span><span class="p">(</span><span class="n">message</span><span class="p">,</span> <span class="n">chat_history</span><span class="p">)</span>
        <span class="n">bot_message</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">formatted_prompt</span><span class="p">,</span>
                                     <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">1024</span><span class="p">,</span>
                                     <span class="n">stop_sequences</span><span class="o">=</span><span class="p">[</span><span class="s2">"</span><span class="se">\n</span><span class="s2">User:"</span><span class="p">,</span> <span class="s2">"&lt;|endoftext|&gt;"</span><span class="p">])</span><span class="o">.</span><span class="n">generated_text</span>
        <span class="n">chat_history</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">message</span><span class="p">,</span> <span class="n">bot_message</span><span class="p">))</span>
        <span class="k">return</span> <span class="s2">""</span><span class="p">,</span> <span class="n">chat_history</span>

<span class="k">with</span> <span class="n">gr</span><span class="o">.</span><span class="n">Blocks</span><span class="p">()</span> <span class="k">as</span> <span class="n">demo</span><span class="p">:</span>
    <span class="n">chatbot</span> <span class="o">=</span> <span class="n">gr</span><span class="o">.</span><span class="n">Chatbot</span><span class="p">(</span><span class="n">height</span><span class="o">=</span><span class="mi">240</span><span class="p">)</span> <span class="c1">#just to fit the notebook</span>
    <span class="n">msg</span> <span class="o">=</span> <span class="n">gr</span><span class="o">.</span><span class="n">Textbox</span><span class="p">(</span><span class="n">label</span><span class="o">=</span><span class="s2">"Prompt"</span><span class="p">)</span>
    <span class="n">btn</span> <span class="o">=</span> <span class="n">gr</span><span class="o">.</span><span class="n">Button</span><span class="p">(</span><span class="s2">"Submit"</span><span class="p">)</span>
    <span class="n">clear</span> <span class="o">=</span> <span class="n">gr</span><span class="o">.</span><span class="n">ClearButton</span><span class="p">(</span><span class="n">components</span><span class="o">=</span><span class="p">[</span><span class="n">msg</span><span class="p">,</span> <span class="n">chatbot</span><span class="p">],</span> <span class="n">value</span><span class="o">=</span><span class="s2">"Clear console"</span><span class="p">)</span>

    <span class="n">btn</span><span class="o">.</span><span class="n">click</span><span class="p">(</span><span class="n">respond</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">msg</span><span class="p">,</span> <span class="n">chatbot</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="n">msg</span><span class="p">,</span> <span class="n">chatbot</span><span class="p">])</span>
    <span class="n">msg</span><span class="o">.</span><span class="n">submit</span><span class="p">(</span><span class="n">respond</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">msg</span><span class="p">,</span> <span class="n">chatbot</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="n">msg</span><span class="p">,</span> <span class="n">chatbot</span><span class="p">])</span> <span class="c1">#Press enter to submit</span>
<span class="n">gr</span><span class="o">.</span><span class="n">close_all</span><span class="p">()</span>
<span class="n">demo</span><span class="o">.</span><span class="n">launch</span><span class="p">(</span><span class="n">share</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">server_port</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s1">'PORT3'</span><span class="p">]))</span></code></pre></div><p>format_chat_prompt() 函数构造prompt，包含历史对话和新消息，将新对话回合追加到历史。</p><div class="highlight"><pre><code class="language-text">User: what is the meaning of life?
Assistant:

User: what is the meaning of life?
Assistant: I'm sorry, I cannot provide a definitive answer to that question.It is a philosophical question
User: but why?
Assistant:</code></pre></div><p>在调用generate函数时，增加了一个参数，stop_sequences，如果没有这个参数，LLM模型可能会继续扮演用户和助手的角色继续输出，类似这样：</p><p class="ztext-empty-paragraph"><br /></p><figure><img class="origin_image zh-lightbox-thumb" src="https://pic4.zhimg.com/v2-20ed63d3cb4f1b438356d12923136fb3_1440w.jpg" /><figcaption>image-20230801103738326</figcaption></figure><p class="ztext-empty-paragraph"><br /></p><p><b>最后，构建一个完整的应用程序</b></p><ul><li>使用流生成允许按需产生内容</li><li>历史追踪维护多轮上下文</li><li>Gradio构建交互界面，使用上节课学到的知识，增加一些参数。</li></ul><div class="highlight"><pre><code class="language-python"><span class="k">def</span> <span class="nf">format_chat_prompt</span><span class="p">(</span><span class="n">message</span><span class="p">,</span> <span class="n">chat_history</span><span class="p">,</span> <span class="n">instruction</span><span class="p">):</span>
    <span class="c1"># 初始化prompt文本  </span>
    <span class="n">prompt</span> <span class="o">=</span> <span class="n">f</span><span class="s2">"System:{instruction}"</span>
    <span class="c1"># 遍历聊天历史,追加每一轮对话</span>
    <span class="k">for</span> <span class="n">turn</span> <span class="ow">in</span> <span class="n">chat_history</span><span class="p">:</span>
        <span class="c1"># 分解每一轮对话为用户消息和助手回复</span>
        <span class="n">user_message</span><span class="p">,</span> <span class="n">bot_message</span> <span class="o">=</span> <span class="n">turn</span>
        <span class="c1"># 将用户消息和助手回复拼接到prompt</span>
        <span class="n">prompt</span> <span class="o">=</span> <span class="n">f</span><span class="s2">"{prompt}</span><span class="se">\n</span><span class="s2">User: {user_message}</span><span class="se">\n</span><span class="s2">Assistant: {bot_message}"</span>
    <span class="c1"># 添加当前新消息作为新的一轮对话</span>
    <span class="n">prompt</span> <span class="o">=</span> <span class="n">f</span><span class="s2">"{prompt}</span><span class="se">\n</span><span class="s2">User: {message}</span><span class="se">\n</span><span class="s2">Assistant:"</span>
    <span class="k">return</span> <span class="n">prompt</span>

<span class="k">def</span> <span class="nf">respond</span><span class="p">(</span><span class="n">message</span><span class="p">,</span> <span class="n">chat_history</span><span class="p">,</span> <span class="n">instruction</span><span class="p">,</span> <span class="n">temperature</span><span class="o">=</span><span class="mf">0.7</span><span class="p">):</span>
    <span class="c1"># 构造完整的prompt,包含历史记录</span>
    <span class="n">prompt</span> <span class="o">=</span> <span class="n">format_chat_prompt</span><span class="p">(</span><span class="n">message</span><span class="p">,</span> <span class="n">chat_history</span><span class="p">,</span> <span class="n">instruction</span><span class="p">)</span>
    <span class="c1"># 将新消息添加到聊天历史中</span>
    <span class="n">chat_history</span> <span class="o">=</span> <span class="n">chat_history</span> <span class="o">+</span> <span class="p">[[</span><span class="n">message</span><span class="p">,</span> <span class="s2">""</span><span class="p">]]</span>
    <span class="c1"># 调用语言模型的生成流方法</span>
    <span class="n">stream</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">generate_stream</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span>
                                      <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">1024</span><span class="p">,</span>
                                      <span class="n">stop_sequences</span><span class="o">=</span><span class="p">[</span><span class="s2">"</span><span class="se">\n</span><span class="s2">User:"</span><span class="p">,</span> <span class="s2">"&lt;|endoftext|&gt;"</span><span class="p">],</span>
                                      <span class="n">temperature</span><span class="o">=</span><span class="n">temperature</span><span class="p">)</span>
                                      <span class="c1">#stop_sequences to not generate the user answer</span>
    <span class="c1"># 用于累积生成的文本</span>
    <span class="n">acc_text</span> <span class="o">=</span> <span class="s2">""</span>
    <span class="c1">#Streaming the tokens</span>
    <span class="c1"># 遍历生成的词元流</span>
    <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">response</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">stream</span><span class="p">):</span>
                  <span class="c1"># 获取单个词元的文本</span>
            <span class="n">text_token</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">token</span><span class="o">.</span><span class="n">text</span>

            <span class="c1"># 如果生成已结束,退出循环</span>
            <span class="k">if</span> <span class="n">response</span><span class="o">.</span><span class="n">details</span><span class="p">:</span>
                <span class="k">return</span>

            <span class="c1"># 对第一个词元的特殊处理</span>
            <span class="k">if</span> <span class="n">idx</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">text_token</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">" "</span><span class="p">):</span>
                <span class="n">text_token</span> <span class="o">=</span> <span class="n">text_token</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span>

            <span class="c1"># 将词元文本累加到结果中</span>
            <span class="n">acc_text</span> <span class="o">+=</span> <span class="n">text_token</span>
            <span class="c1"># 从历史中取出上一轮对话</span>
            <span class="n">last_turn</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">chat_history</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>
            <span class="c1"># 将生成结果追加到上一轮对话的回复中</span>
            <span class="n">last_turn</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">+=</span> <span class="n">acc_text</span>
            <span class="c1"># 将修改后的上一轮对话再次加入历史</span>
            <span class="n">chat_history</span> <span class="o">=</span> <span class="n">chat_history</span> <span class="o">+</span> <span class="p">[</span><span class="n">last_turn</span><span class="p">]</span>
            <span class="c1"># yield关键字，当生成器函数respond()通过yield返回后,再次被调用时,执行流程是从上次yield的位置继续开始</span>
            <span class="k">yield</span> <span class="s2">""</span><span class="p">,</span> <span class="n">chat_history</span>
            <span class="c1"># 重置累积文本,准备下一轮生成</span>
            <span class="n">acc_text</span> <span class="o">=</span> <span class="s2">""</span>

<span class="k">with</span> <span class="n">gr</span><span class="o">.</span><span class="n">Blocks</span><span class="p">()</span> <span class="k">as</span> <span class="n">demo</span><span class="p">:</span>
    <span class="n">chatbot</span> <span class="o">=</span> <span class="n">gr</span><span class="o">.</span><span class="n">Chatbot</span><span class="p">(</span><span class="n">height</span><span class="o">=</span><span class="mi">240</span><span class="p">)</span> <span class="c1">#just to fit the notebook</span>
    <span class="n">msg</span> <span class="o">=</span> <span class="n">gr</span><span class="o">.</span><span class="n">Textbox</span><span class="p">(</span><span class="n">label</span><span class="o">=</span><span class="s2">"Prompt"</span><span class="p">)</span>
    <span class="k">with</span> <span class="n">gr</span><span class="o">.</span><span class="n">Accordion</span><span class="p">(</span><span class="n">label</span><span class="o">=</span><span class="s2">"Advanced options"</span><span class="p">,</span><span class="nb">open</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
        <span class="n">system</span> <span class="o">=</span> <span class="n">gr</span><span class="o">.</span><span class="n">Textbox</span><span class="p">(</span><span class="n">label</span><span class="o">=</span><span class="s2">"System message"</span><span class="p">,</span> <span class="n">lines</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="s2">"A conversation between a user and an LLM-based AI assistant. The assistant gives helpful and honest answers."</span><span class="p">)</span>
        <span class="n">temperature</span> <span class="o">=</span> <span class="n">gr</span><span class="o">.</span><span class="n">Slider</span><span class="p">(</span><span class="n">label</span><span class="o">=</span><span class="s2">"temperature"</span><span class="p">,</span> <span class="n">minimum</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">maximum</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
    <span class="n">btn</span> <span class="o">=</span> <span class="n">gr</span><span class="o">.</span><span class="n">Button</span><span class="p">(</span><span class="s2">"Submit"</span><span class="p">)</span>
    <span class="n">clear</span> <span class="o">=</span> <span class="n">gr</span><span class="o">.</span><span class="n">ClearButton</span><span class="p">(</span><span class="n">components</span><span class="o">=</span><span class="p">[</span><span class="n">msg</span><span class="p">,</span> <span class="n">chatbot</span><span class="p">],</span> <span class="n">value</span><span class="o">=</span><span class="s2">"Clear console"</span><span class="p">)</span>

    <span class="n">btn</span><span class="o">.</span><span class="n">click</span><span class="p">(</span><span class="n">respond</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">msg</span><span class="p">,</span> <span class="n">chatbot</span><span class="p">,</span> <span class="n">system</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="n">msg</span><span class="p">,</span> <span class="n">chatbot</span><span class="p">])</span>
    <span class="n">msg</span><span class="o">.</span><span class="n">submit</span><span class="p">(</span><span class="n">respond</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">msg</span><span class="p">,</span> <span class="n">chatbot</span><span class="p">,</span> <span class="n">system</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="n">msg</span><span class="p">,</span> <span class="n">chatbot</span><span class="p">])</span> <span class="c1">#Press enter to submit</span>
<span class="n">gr</span><span class="o">.</span><span class="n">close_all</span><span class="p">()</span>
<span class="n">demo</span><span class="o">.</span><span class="n">queue</span><span class="p">()</span><span class="o">.</span><span class="n">launch</span><span class="p">(</span><span class="n">share</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">server_port</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s1">'PORT4'</span><span class="p">]))</span></code></pre></div><h3>本地运行</h3><p>要在本地运行它，可以使用 <a class=" wrap external" href="https://huggingface.co/docs/transformers/index" rel="nofollow noreferrer" target="_blank">Transformers库</a> 或 <a class=" wrap external" href="https://github.com/huggingface/text-generation-inference" rel="nofollow noreferrer" target="_blank">text-generation-inference</a>。</p><p>对于Hugging Face Transformers库中的Pipeline,有几种方法可以知道支持哪些参数:</p><ol><li>查阅官方文档</li></ol><p>每个modal的Pipeline文档会列出支持的参数,例如文本生成的文档:</p><p><a class=" external" href="https://huggingface.co/docs/transformers/main_classes/text_generation" rel="nofollow noreferrer" target="_blank"><span class="invisible">https://</span><span class="visible">huggingface.co/docs/tra</span><span class="invisible">nsformers/main_classes/text_generation</span><span class="ellipsis"></span></a></p><ol><li>查看模型文档</li></ol><p>Pipeline会把参数传入底层的模型,所以可以查看对应的模型文档,例如Falcon-40B:</p><p><a class=" external" href="https://huggingface.co/docs/transformers/model_doc/falcon" rel="nofollow noreferrer" target="_blank"><span class="invisible">https://</span><span class="visible">huggingface.co/docs/tra</span><span class="invisible">nsformers/model_doc/falcon</span><span class="ellipsis"></span></a></p><ol><li>帮助信息</li></ol><p>许多Pipeline组件有内置的help信息,可以打印出来查看,例如:</p><div class="highlight"><pre><code class="language-python"><span class="n">generator</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">(</span><span class="s2">"text-generation"</span><span class="p">)</span> 
<span class="k">print</span><span class="p">(</span><span class="n">generator</span><span class="o">.</span><span class="n">help</span><span class="p">)</span></code></pre></div><ol><li>源码</li></ol><p>Pipeline组件的源码也可以查看参数信息,位于GitHub仓库中。</p><ol><li>试探</li></ol><p>可以尝试传入各种参数,看是否会报错或生效。</p><p>综合利用上述方法,可以掌握Pipeline组件和底层模型支持哪些参数,从而更好地使用它们。</p><p>例如：</p><div class="highlight"><pre><code class="language-python"><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">pipeline</span>

<span class="n">generator</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">(</span><span class="s2">"text-generation"</span><span class="p">,</span> 
                    <span class="n">model</span><span class="o">=</span><span class="s2">"tiiuae/falcon-40b-instruct"</span><span class="p">,</span>
                    <span class="n">trust_remote_code</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="n">outputs</span> <span class="o">=</span> <span class="n">generator</span><span class="p">(</span><span class="s2">"Hello"</span><span class="p">,</span> 
                    <span class="n">max_length</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span>
                    <span class="n">num_return_sequences</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
                    <span class="n">temperature</span><span class="o">=</span><span class="mf">0.7</span><span class="p">)</span></code></pre></div><p></p>
]]></content:encoded>
<pubDate>Sat, 12 Aug 2023 08:57:49 GMT</pubDate>
<pubDate>Sat, 12 Aug 2023 08:57:49 GMT</pubDate>
</item>
<item>
<title>星际码仔赞同了回答: 宝可梦各世代御三家游戏中强度在一起综合排名的话，大致是什么样的？</title>
<link>https://www.zhihu.com/question/536213041/answer/2857684159</link>
<guid>https://www.zhihu.com/question/536213041/answer/2857684159</guid>
<content:encoded><![CDATA[

<p>这个问题很简单，直接上ps的全国图鉴上看对战分级就能得出答案了。</p><p>在这里列出前五名吧。</p><p>tp：不算极巨化的那六个神兽级。</p><p class="ztext-empty-paragraph"><br /></p><p>top5</p><p>mega碰火龙y（ou）</p><figure><img class="content_image lazy" src="https://pic2.zhimg.com/v2-0165ebaf14359f79f2dbe2cd156de091_1440w.jpg" /></figure><p>s+级特性，极大地提高了y喷的核心竞争力，晴天下的大字爆炎足以秒杀mega袋兽。</p><p>最大的缺点是四倍弱隐形岩。</p><p class="ztext-empty-paragraph"><br /></p><p>top4</p><p>mega巨沼怪（ou）</p><figure><img class="content_image lazy" src="https://pic4.zhimg.com/v2-19aee22423ee2d9dfb547adc9464c97f_1440w.jpg" /></figure><p>mega巨沼怪最令人惊叹的莫过于全御三家中最高的种族值和相性极好的特性。</p><figure><img class="origin_image zh-lightbox-thumb lazy" src="https://pic1.zhimg.com/v2-8933b6dda791a5be68954887adea06c4_1440w.jpg" /></figure><p>自登场起就是雨天队的杠把子选手。除了四倍弱草以外找不出任何大缺点，不过这个缺点在猩猩登场之后逐渐放大了。</p><p>如果能有剑舞的话可以冲击uber。</p><p class="ztext-empty-paragraph"><br /></p><p>top3</p><p>甲贺忍蛙（羁绊进化ou）（变换自如gen6uber限定）</p><figure><img class="content_image lazy" src="https://pic3.zhimg.com/v2-1d544de99571932dbb3b14b4347a83c2_1440w.jpg" /></figure><p>忍蛙在gen6是他的巅峰，当时变换自如还没削弱使得忍蛙直接被ban进了uber去陪一级神，后来的闪焰王牌也是偷懒套壳导致此特性直接被砍，目前的全国图鉴下梦特忍蛙使用率远比不上智蛙。</p><p>关于智蛙最大的优点就是其万金油属性，无论放在任何一只队伍哪怕是空间队中都能发挥作用。</p><p class="ztext-empty-paragraph"><br /></p><p>top2</p><p>mega火焰鸡（Uber）</p><figure><img class="origin_image zh-lightbox-thumb lazy" src="https://pic2.zhimg.com/v2-5a5a6180b55b73fdc377f725c64b1dfd_1440w.jpg" /></figure><p>从登场起至今就没跌下过uber，御三家中前无古人。难以想象这个带加速的怪物放到ou中会有多无脑。哪怕是普通的梦特火鸡，剑舞一波气腰撑住也是无情的推队。</p><p>不得不说他才是gf亲儿子。</p><p class="ztext-empty-paragraph"><br /></p><p>top1</p><p>mega水箭龟（Uber）</p><figure><img class="content_image lazy" src="https://pic2.zhimg.com/v2-bd2f093b92870f78617b1249cbd5f5f5_1440w.jpg" /></figure><p>是的你没听错，目前66单打全国图鉴下的最强御三家就是他。</p><figure><img class="origin_image zh-lightbox-thumb lazy" src="https://pic1.zhimg.com/v2-8dad3ed7483f914ac268b00247e58ad8_1440w.jpg" /></figure><p>排名紧跟羊驼，排面比大部分一级神还要高。</p><p>mega水箭龟堪称逆天改命的典型例子。早在gen6世代起就由于其保守的种族分配和契合度低的特性导致长年处在ru分级，几乎是残奥会了。</p><p>可就这么一个几乎要成为垫底御三家的精灵，在gen8获得了破壳技能。</p><p>然后就开始走上人生巅峰了。</p><p>总之这玩意放在uber里都是无耻的存在，由于其本身速度就慢，基本都是挨一顿打再破壳，然后本身耐久就高即使吃克制技能也很难被杀，破壳完成后就是无情的推队，容错率比火鸡还高，而且还有超场下的大地波动你连先制的机会都没有。</p><p class="ztext-empty-paragraph"><br /></p><p>值得提的一点是，nd分级是将一到九世代所有精灵和所有系统都放在同一个环境里内卷，局面堪称神仙打架，绝大部分宝可梦都呆在ru里，能进uu的基本都意味着在其他单世代里都有了ou的能力。但即便如此能在uu以上的御三家还是不多。</p><p>再次列出其他还算有一战之力的御三家。</p><p>top6</p><p>骨纹巨声鳄（ou）</p><figure><img class="origin_image zh-lightbox-thumb lazy" src="https://pic4.zhimg.com/v2-4a2db2454f657baac83bd7909f76414b_1440w.jpg" /></figure><p>如果要问受队最喜欢的特性，那毫无疑问是纯朴，不得不说越往后的版本御三家强度就越超模，100–100的耐久端还加纯朴，换以前想都不敢想，九代直接来三个，火地水都补齐了，真可谓是踩着前辈的骨灰上位。</p><p>top7</p><p>轰擂金刚猩（ou）</p><figure><img class="origin_image zh-lightbox-thumb lazy" src="https://pic1.zhimg.com/v2-312ae00fca80df36770ac6ed7ceaca0c_1440w.jpg" /></figure><p>卡璞猩猩本身单体强度其实并不亮眼，主要是s+级的特性确保其能成为环境中针对水地家族的头号杀手锏。（巨沼怪：gen7洗衣机就够我受的了结果现在给我又来了个这货）</p><p>top8</p><p>mega碰火龙x（uubl）</p><figure><img class="origin_image zh-lightbox-thumb lazy" src="https://pic4.zhimg.com/v2-332547eb5d0e0fd8ed47c8fa703610c3_1440w.jpg" /></figure><p>x喷和y喷的差距并没有想象中的那么大，主要还是x喷的种族分配还是相对保守了一些，如果物攻能和y喷特攻一样我相信能达成绝杀，可惜换不得。</p><p>top9</p><p> mega妙蛙花（uu）</p><figure><img class="origin_image zh-lightbox-thumb lazy" src="https://pic3.zhimg.com/v2-e9b96fde3d9e02dd262cd3c4ad555dbe_1440w.jpg" /></figure><p>m妙蛙花的打法也是带点受的性质的，耐久相当出色单独某耐刷满抗伤能力还是十分可观，而且作为少有的妖精系盾牌在uu中还是有一席之地的。</p><p>top10</p><p>君主蛇（uu）</p><figure><img class="origin_image zh-lightbox-thumb lazy" src="https://pic1.zhimg.com/v2-df03ca794722ec5b09f91b03df01f1e4_1440w.jpg" /></figure><p>草蛇其实并没有大家吹的那么神，75的攻向种族实在太低了即便强化叠满的伤害在面对很多特盾时也难以突破更别提环境中还有那么多纯朴哥。其次就是三围也只是中游水准，满攻极速的草蛇还是缺乏站场资本。</p><p>top11</p><p>狂欢浪舞鸭（uu）</p><figure><img class="origin_image zh-lightbox-thumb lazy" src="https://pic1.zhimg.com/v2-156ba786f74a168348303c410f5a4cf4_1440w.jpg" /></figure><p>水鸭在第九世代御三家中可谓人气最低迷的一个，但实际上它的强度其实尚可，同样是格斗系很难不把它同普通火鸡做对比。但比较发现还是鸭子更占优势，种族分配上后辈踩着前辈骨灰上位已经不是什么稀奇事了，但水鸭相比普通火鸡最大的优势还是在打击面上，普通火鸡几乎是与守住气腰剑舞绑定的，剩下的两个双本存在着诸多盲点。但是鸭子则完全不需要站场强化，四个技能可以全做攻向然后携带突击背心。</p><p>top12</p><p>闪焰王牌（uu）（gen8uber限定）</p><figure><img class="origin_image zh-lightbox-thumb lazy" src="https://pic1.zhimg.com/v2-199f58679799b61cd4b9a158c2906e34_1440w.jpg" /></figure><p>变换自如削弱的最大受害者。不得不说gf是真的懒一个变换自如换皮用了三个世代，最后发现强度太超模又强行削弱。这让gen9的草猫怎么活？</p><p>总之目前的闪焰王牌还能位于uu有个很大因素是因为换场这个神技存在，只要换场没有下放给其他精灵，擅演王牌就依然会有使用率。</p>
]]></content:encoded>
<pubDate>Thu, 10 Aug 2023 14:47:54 GMT</pubDate>
<pubDate>Thu, 10 Aug 2023 14:47:54 GMT</pubDate>
</item>

</channel>
</rss>