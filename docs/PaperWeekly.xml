<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
<channel>
<title>知乎专栏-PaperWeekly</title>
<link>https://zhuanlan.zhihu.com/paperweekly</link>


<item>
<title>万字长文总结多模态大模型最新进展（Modality Bridging篇）</title>
<link>https://zhuanlan.zhihu.com/p/688215018</link>
<guid>https://zhuanlan.zhihu.com/p/688215018</guid>
<content:encoded><![CDATA[
<div> 多模态大型语言模型、视觉编码器、训练阶段、效率、分辨率<br /><br />总结:<br />文章探讨了多模态大型语言模型（MLLM）的开发和优化，特别关注于视觉信息的集成和处理。通过三个训练阶段，模型学习从低分辨率到高分辨率理解图像，同时保证了LLM的密切对齐。针对不同分辨率和纵横比的图像，提出了创新的模块化视觉编码方案，并采用压缩技术以及空间结构模式来保持高效的计算成本。此外，通过调整专门的视觉-文本对数据集，模型在多模态对话交互方面显示出强大的熟练度，能够有效整合和理解视觉与语言的输入。最终，这些研究成果促进了在处理高分辨率视觉内容方面的LMM的性能提升，为未来的多模态理解和生成任务奠定了基础。 <div>
<p>多模态大型语言模型（MLLM）最近已成为一个新兴的研究热点，它将强大的大型语言模型（LLMs）作为大脑来执行多模态任务。 MLLM的惊人新能力，如基于图像撰写故事和无OCR的数学推理，在传统方法中很少见，这表明了通向通用人工智能的潜在路径。</p><p>通常人们会在pair数据上进行大规模（相对于instruction tuning）的预训练，以促进不同模态之间的对齐。对齐数据集通常是图像文本对或自动语音识别（ASR）数据集，它们都包含文本。更具体地说，图像文本对以自然语言句子的形式描述图像，而ASR数据集包含语音的转录。对齐预训练的常见方法是保持预训练模块（例如视觉编码器和LLMs）冻结，并训练一个可学习的接口,本文调研了到近期位置不同的接口设计以及学习方法相关的文章。</p><ul><li><a class="internal" href="https://zhuanlan.zhihu.com/write">Flamingo: a Visual Language Model for Few-Shot Learning</a></li><li><a class="internal" href="https://zhuanlan.zhihu.com/write">BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models</a></li><li><a class="internal" href="https://zhuanlan.zhihu.com/write">InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning</a></li><li><a class="internal" href="https://zhuanlan.zhihu.com/write">Visual Instruction Tuning</a></li><li><a class="internal" href="https://zhuanlan.zhihu.com/write">Improved Baselines with Visual Instruction Tuning</a></li><li><a class="internal" href="https://zhuanlan.zhihu.com/write">LLaVA-NeXT: Improved reasoning, OCR, and world knowledge</a></li><li><a class="internal" href="https://zhuanlan.zhihu.com/write">Cheap and Quick: Efficient Vision-Language Instruction Tuning for Large Language Models</a></li><li><a class="internal" href="https://zhuanlan.zhihu.com/write">MIMIC-IT: Multi-Modal In-Context Instruction Tuning</a></li><li><a class="internal" href="https://zhuanlan.zhihu.com/write">LLaVAR: Enhanced Visual Instruction Tuning for Text-Rich Image Understanding</a></li><li><a class="internal" href="https://zhuanlan.zhihu.com/write">SVIT: Scaling up Visual Instruction Tuning</a></li><li><a class="internal" href="https://zhuanlan.zhihu.com/write">Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond</a></li><li><a class="internal" href="https://zhuanlan.zhihu.com/write">NExT-GPT: Any-to-Any Multimodal LLM</a></li><li><a class="internal" href="https://zhuanlan.zhihu.com/write">InternLM-XComposer: A Vision-Language Large Model for Advanced Text-image Comprehension and Composition</a></li><li><a class="internal" href="https://zhuanlan.zhihu.com/write">CogVLM: Visual Expert for Pretrained Language Models</a></li><li><a class="internal" href="https://zhuanlan.zhihu.com/write">OtterHD: A High-Resolution Multi-modality Model</a></li><li><a class="internal" href="https://zhuanlan.zhihu.com/write">Monkey : Image Resolution and Text Label Are Important Things for Large Multi-modal Models</a></li><li><a class="internal" href="https://zhuanlan.zhihu.com/write">LLaMA-VID: An Image is Worth 2 Tokens in Large Language Models</a></li><li><a class="internal" href="https://zhuanlan.zhihu.com/write">MoE-LLaVA: Mixture of Experts for Large Vision-Language Models</a></li><li><a class="internal" href="https://zhuanlan.zhihu.com/write">LLaVA-UHD: an LMM Perceiving Any Aspect Ratio and High-Resolution Images</a></li><li><a class="internal" href="https://zhuanlan.zhihu.com/write">Yi-VL</a></li></ul><h2>Flamingo: a Visual Language Model for Few-Shot Learning</h2><p class="ztext-empty-paragraph"><br /></p><figure><img class="origin_image zh-lightbox-thumb" src="https://pic2.zhimg.com/v2-d9dda02dfcef8135293e92d80e08e239_b.jpg" width="995" /></figure><p> 总的来说，首先，Perceiver Resampler接收来自视觉编码器的时空特征（从图像或视频获取），并输出固定数量的视觉标记。其次，这些视觉标记用于通过新初始化的交叉注意力层对冻结的语言模型进行条件化，这些层被插入到预训练的语言模型层之间。这些新层为语言模型提供了一种表达方式，以便将视觉信息纳入到下一个标记预测任务中</p><h3>Visual processing and the Perceiver Resampler</h3><p><b>视觉编码器</b>：是一个预训练并冻结的Normalizer-Free ResNet (NFNet)，使用Radford等人提出的 two-term contrastive loss，在图像和文本对数据集上对视觉编码器进行对比目标的预训练。使用最终阶段的输出，即一个二维空间网格的特征，将其压平为一个一维序列。对于视频输入，帧以1 FPS进行采样并独立编码，以获得一个三维时空特征网格，然后将学习到的时间嵌入添加到其中。特征然后被压平为一维，然后输入到Perceiver Resampler中。</p><figure><img class="origin_image zh-lightbox-thumb" src="https://pic2.zhimg.com/v2-ae76ce59b9f74cdf8b12004bf27b10ad_b.jpg" width="1546" /><figcaption>Perceiver Resampler模块将由Vision Encoder输出的可变大小的时空视觉特征网格映射到固定数量的输出标记（图中为五个），与输入图像分辨率或输入视频帧数无关。这个transformer具有一组学习到的潜在向量作为查询，而键和值则是由时空视觉特征与学习到的潜在向量的连接组成。</figcaption></figure><p><b>Perceiver Resampler</b>：从不同大小的大型特征图到少量视觉标记。这个模块将视觉编码器连接到冻结的语言模型，如上图所示。它以视觉编码器中的图像或视频特征的可变数量作为输入，并产生固定数量的视觉输出（64个），从而降低了视觉-文本交叉注意力的计算复杂度。类似于Perceiver和DETR，本文学习了预定义数量的潜在输入查询，这些查询被输入到一个Transformer中，并对视觉特征进行交叉关注。消融研究中展示了使用这样一个视觉-语言重采样模块优于一个普通的Transformer和一个MLP。</p><h3>GATED XATTN-DENSE details</h3><p class="ztext-empty-paragraph"><br /></p><figure><img class="origin_image zh-lightbox-thumb" src="https://pic4.zhimg.com/v2-e81aac64a8a40565ffff65e7a2bbc157_b.jpg" width="1567" /></figure><p> 上图提供了一个GATED XATTN-DENSE块的示意图，以及它与一个冻结的LM块的连接方式，同时附上了伪代码。下图绘制了Flamingo-3B模型的24个LM层在训练过程中（从0％到100％）不同层中tanh门控值的绝对值的演变。冻结的LM堆栈的所有层似乎都利用了视觉信息，因为tanh门控的绝对值从其0初始化中迅速增长。我们还注意到，绝对值似乎随着深度增加而增加。然而，从这个观察中很难得出强有力的结论：门控之前的激活的规模也可能随着深度变化。未来的工作需要更好地理解这些添加层对优化动态和模型本身的影响。</p><figure><img class="origin_image zh-lightbox-thumb" src="https://pic2.zhimg.com/v2-5c811461ae73b35f0561f062e585be71_b.jpg" width="1529" /></figure><h3>Multi-visual input support</h3><figure><img class="origin_image zh-lightbox-thumb" src="https://pic2.zhimg.com/v2-70d604d87d88a0485bccc35c7a5a63ed_b.jpg" width="1656" /><figcaption>首先通过在文本中的视觉数据位置插入image标签以及特殊标记BOS表示“序列开始”或EOC表示“块结束”）来处理文本。 图像由Vision Encoder和Perceiver Resampler独立处理，以提取视觉标记。 在给定的文本标记处，模型仅与最后一个前导图像/视频对应的视觉标记进行交叉关注。   指示文本标记可以关注的图像/视频，或者在没有前导图像/视频时为0</figcaption></figure><p>上图说明了本文使用的mask方法，以限制某个文本标记看到的视觉标记数量。我们还对图像/视频和文本的交错序列的符号化进行了规范化。 交错的视觉数据和文本序列。我们考虑交错的图像/视频和文本示例：每个示例包含一系列文本  ，一系列图像/视频  ，以及图像在文本中的位置序列。基于视觉数据的位置，我们定义一个函数   : [1,  ] ↦ → [0,   ]，它为每个文本位置分配最后一个出现在该位置之前的图像/视频的索引（或者如果该位置之前没有视觉数据，则为0）。函数   定义了我们考虑用于预测的标记 <img alt="l" src="https://www.zhihu.com/equation?tex=l" /> 的可用视觉输入：前面的标记 <img alt="y_{&lt; l}=(y_1,\dots,y_{l-1})" src="https://www.zhihu.com/equation?tex=y_%7B%3C+l%7D%3D%28y_1%2C%5Cdots%2Cy_%7Bl-1%7D%29" />.</p><h3>训练细节</h3><ol><li><b>训练数据集</b>由不同格式的训练数据集混合而成。去除交错的图像文本数据集M3W导致性能下降超过17％，而去除传统的配对图像文本对也会导致性能下降（下降9.8％），这表明需要不同类型的数据集。 </li></ol><figure><img class="origin_image zh-lightbox-thumb" src="https://pic2.zhimg.com/v2-75e7cf6153f4dc0230f3ef86e37ae1b9_b.jpg" width="1532" /></figure><p class="ztext-empty-paragraph"><br /></p><ol><li><b>冻结LM组件可以防止灾难性遗忘</b>。如果从头开始训练，我们观察到性能大幅下降了-12.9％。有趣的是，微调我们预训练的LM也导致了性能下降了-8.0％。</li><li><b>数据集加权</b>。M3W、ALIGN、LTIP和VTP，其权重分别为1.0、0.2、0.2和0.03。这些权重是在小模型规模下经验性地获得的，并且在之后保持不变。</li></ol><h2>BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models</h2><figure><img class="origin_image zh-lightbox-thumb" src="https://pic4.zhimg.com/v2-65240d0fb56f2efd0e6eb6704216e603_b.jpg" width="786" /><figcaption>BLIP-2框架概述。我们通过一个两阶段策略预训练轻量级的查询Transformer，以弥合模态差距。第一阶段从冻结的图像编码器中引导视觉-语言表示学习。第二阶段从冻结的LLM中引导视觉到语言的生成学习，这使得零样本指导的图像到文本生成成为可能。&amp;lt;br&amp;gt;</figcaption></figure><p>LLM本质上是个语言模型，自然无法直接接受其他模态的信息。所以如何把各个模态的信息，统一到LLM能理解的特征空间，就是第一步要解决的问题。为此，作者提出了Q-Former。</p><figure><img class="origin_image zh-lightbox-thumb" src="https://pic2.zhimg.com/v2-93f0dc063e4575bf5586d14ae3807e35_b.jpg" width="1654" /><figcaption>（左）Q-Former和BLIP-2的第一阶段视觉-语言表示学习目标的模型架构。我们共同优化三个目标，这些目标强制查询（一组可学习的嵌入）提取与文本最相关的视觉表示。（右）每个目标的自注意力屏蔽策略，以控制查询-文本交互。</figcaption></figure><p>Learned Query的引入在这里至关重要。可以看到这些Query通过Cross-Attention与图像的特征交互，通过Self-Attention与文本的特征交互。这样做的好处有两个：（1）这些Query是基于两种模态信息得到的；（2）无论多大的视觉Backbone，最后都是Query长度的特征输出，大大降低了计算量。比如在实际实验中，ViT-L/14的模型的输出的特征是257x1024的大小，最后也是32x768的Query特征。针对Q-Former的三个训练任务分别是 Image-Text Contrastive Learning (ITC)，Image-grounded Text Generation (ITG)，Image-Text Matching (ITM)。第一阶段，对于模型的训练，就是由以上三个任务组成，通过这几个任务，实现了对于特征的提取与融合。但现在模型还没见过LLM。我们现在用传感器完成了数据的提取与融合，下一步，我们得把数据转换成处理器能识别的格式。</p><figure><img class="origin_image zh-lightbox-thumb" src="https://pic3.zhimg.com/v2-5e59f4f4a156ae2a86f8f85c580596c2_b.jpg" width="1541" /><figcaption>BLIP-2的第二阶段视觉到语言生成预训练，从冻结的大型语言模型（LLM）中引导。 （顶部）引导基于解码器的LLM（例如OPT）。 （底部）引导基于编码器-解码器的LLM（例如FlanT5）。全连接层从Q-Former的输出维度调整到所选LLM的输入维度。</figcaption></figure><p>通过第一阶段的训练，Query已经浓缩了图片的精华，现在要做的，就是把Query变成LLM认识的样子。这里作者针对两类不同LLM设计了不同的任务：</p><p>Decoder类型的LLM（如OPT）：以Query做输入，文本做目标； Encoder-Decoder类型的LLM（如FlanT5）：以Query和一句话的前半段做输入，以后半段做目标；</p><p>为了适合各模型不同的Embedding维度，作者引入了一个FC层做维度变换。</p><h3>训练细节</h3><p>作为图文预训练的工作，工程问题往往是关键。BLIP2的训练过程主要由以下几个值得关注的点：</p><ol><li>训练数据方面：包含常见的 COCO，VG，SBU，CC3M，CC12M 以及 115M的LAION400M中的图片。采用了BLIP中的CapFilt方法来Bootstrapping训练数据。</li><li>CV模型：选择了CLIP的ViT-L/14和ViT-G/14，特别的是，作者采用倒数第二层的特征作为输出。</li><li>训练时，CV模型和LLM都是冻结的状态，并且参数都转为了FP16。这使得模型的计算量大幅度降低。主要训练的基于BERT-base初始化的Q-Former只有188M的参数量。</li></ol><h2>InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning</h2><figure><img class="origin_image zh-lightbox-thumb" src="https://pic3.zhimg.com/v2-2611061e42ef32d1ce9f1e40392717ae_b.jpg" width="976" /><figcaption>InstructBLIP的模型架构。Q-Former从冻结的图像编码器的输出嵌入中提取了指示感知的视觉特征，并将这些视觉特征作为软提示输入馈送给冻结的LLM。我们使用语言建模损失对模型进行指令调整，以生成响应。</figcaption></figure><p>视觉编码器提取输入图片的特征，并喂入 Q-Former 中。此外，Q-Former 的输入还包括可学习的 Queries (BLIP-2 的做法) 和 Instruction。Q-Former 的内部结构黄色部分所示，其中可学习的 Queries 通过 Self-Attention 和 Instruction 交互，可学习的 Queries 通过 Cross-Attention 和输入图片的特征交互，鼓励提取与任务相关的图像特征。</p><p>Q-Former 的输出通过一个 FC 层送入 LLM，Q-Former 的预训练过程遵循 BLIP-2 的两步：1) 不用 LLM，固定视觉编码器的参数预训练 Q-Former 的参数，训练目标是视觉语言建模。2) 固定 LLM 的参数，训练 Q-Former 的参数，训练目标是文本生成。</p><p>在推理的时候，对于大部分数据集，如 image captioning，open-ended VQA 等，InstructBLIP 可以直接使用 LLM 生成的文本作为输出；对于 classification 和 multi-choice VQA 这样的任务，InstructBLIP 遵循 ALBEF 的做法生成固定的几种答案，根据概率选择最后的结果作为输出。这种做法的数据集包括有 ScienceQA、IconQA、A-OKVQA (多项选择)、HatefulMemes、Visual Dialog、MSVD 和 MSRVTT 数据集。</p><h3>Tricks</h3><ol><li><b>数据重采样</b>由于训练数据集数量太大，而且每个数据集的大小存在显着差异，均匀混合它们可能会导致模型过拟合较小的数据集，并欠拟合更大的数据集。因此，作者改了一下采样数据的概率，从某个数据集里面采样数据的概率是<img alt="p_d=\frac{\sqrt{S_d}}{\sum_{i=1}^D\sqrt{S_i}}" src="https://www.zhihu.com/equation?tex=p_d%3D%5Cfrac%7B%5Csqrt%7BS_d%7D%7D%7B%5Csum_%7Bi%3D1%7D%5ED%5Csqrt%7BS_i%7D%7D" />，其中<img alt="S_i" src="https://www.zhihu.com/equation?tex=S_i" />是单个数据集的大小。</li></ol><h2>Visual Instruction Tuning</h2><h3>数据构造</h3><p>结合GPT-4优异的文字能力，将原始数据构造成结构化的文本信息作为Context，同时通过prompt template请求GPT-4得到一些结果，来生成原始的instruction data。在训练时，则可加入visual token，以得到align后的instruction-tuned model</p><figure><img class="origin_image zh-lightbox-thumb" src="https://pic1.zhimg.com/v2-6349286b44a2973fab52e78e695366d0_b.jpg" width="1054" /></figure><p>训练分两步，第一步做对齐，只训projection layer；第二步e2e finetune，vision encoder（clip vit-L）是freeze的。可以看到instruction tuning对任务效果影响巨大，另外每个任务本身的指令数据也对各个任务都有互补作用</p><figure><img class="origin_image zh-lightbox-thumb" src="https://pic1.zhimg.com/v2-f4271c2a888517a1abaf84611b5bb880_b.jpg" width="1303" /><figcaption>使用不同训练数据在LLaVA-Bench（COCO）上的消融实验。我们报告相对分数，相对于一个仅使用地面真实图像标题和边界框作为视觉输入的文本GPT-4模型。我们使用我们模型输出的答案和GPT-4（仅文本）的答案来提示GPT-4，并让它在两者之间进行比较并给出一个带有解释的评分。</figcaption></figure><h2>Improved Baselines with Visual Instruction Tuning</h2><p><b>Response formatting prompts</b>。我们发现，像InstructBLIP这样的方法无法很好地平衡短形式和长形式VQA的原因主要有以下几点。首先，是响应格式上的模糊提示。例如，Q: {问题} A: {答案}。这样的提示并不清楚地指示了期望的输出格式，甚至在自然的视觉对话中，<b>也可能使LLM在行为上过度拟合为短形式答案</b>。其次，没有对LLM进行微调。第一个问题由于InstructBLIP只对Qformer进行了指导调整而进一步恶化。它需要Qformer的视觉输出令牌来控制LLM的输出长度，使其为长形式或短形式，就像前缀调整一样，但是Qformer可能缺乏正确执行此操作的能力，因为与LLMa等LLM相比，其容量有限。为了解决这个问题，我们建议<b>使用一个单一的响应格式提示，清楚地指示输出格式</b>，在促进短答案时附加到VQA问题的末尾：用一个词或短语回答问题。我们经验证明，当LLM使用这样的提示进行微调时，LLaVA能够根据用户的指示正确调整输出格式，并且不需要对VQA数据进行额外处理，这进一步实现了对各种数据源的扩展。</p><p><b>Academic task oriented data</b> 我们进一步包括了额外的学术任务导向的VQA数据集，用于VQA、OCR和区域级感知，以各种方式增强模型的能力，如表1所示。我们首先包括了InstructBLIP中使用的四个额外数据集：开放知识VQA（OKVQA ，A-OKVQA ）和OCR（OCRVQA ，TextCaps）。A-OKVQA被转换为多项选择问题，并使用特定的响应格式提示：直接用给定选项的字母回答。仅使用InstructBLIP使用的数据集子集，LLaVA就在表1中的所有三个任务上都超过了它，表明LLaVA的有效设计。此外，我们发现进一步添加区域级VQA数据集（Visual Genome，RefCOCO）可以提高模型对细粒度视觉细节的定位能力。</p><p><b>Additional scaling.</b> 进一步增加了输入图像的分辨率，以使LLM能够清晰地“看到”图像的细节，并将GQA数据集作为额外的视觉知识源。我们还加入了ShareGPT 数据，并将LLM扩展到13B，在MM-Vet上的结果显示了将LLM扩展到13B时的最显著的改进，表明了基础LLM能力对视觉对话的重要性。</p><p><b>Limitations.。尽管LLaVA-1.5展示了令人期待的结果，但必须承认存在一些限制。首先，LLaVA利用完整的图像补丁，可能会延长每个训练迭代的时间。虽然视觉重采样器可以减少LLM中的视觉补丁数量，但它们目前不能像LLaVA那样有效地收敛，可能是由于重采样器中的可训练参数更多</b>。一个高效的样本重采样器的开发可以为未来扩展指导跟随多模态模型铺平道路。第二，由于缺乏这种指导跟随数据和上下文长度的限制，<b>LLaVA-1.5目前还不能处理多个图像</b>。第三，尽管LLaVA-1.5在遵循复杂指令方面表现出了熟练，但其问题解决能力在某些领域仍然可能受到限制，这可以通过更有能力的语言模型和高质量、针对性的视觉指导调整数据来改善。最后，尽管LLaVA的产生幻觉的倾向显著降低，但它<b>仍然可能产生幻觉并偶尔传播错误信息</b>，在关键应用（例如医学）中应谨慎使用。</p><h2>LLaVA-NeXT: Improved reasoning, OCR, and world knowledge</h2><p>LLaVA-NeXT，它在推理、OCR和世界知识方面有所改进。LLaVA-NeXT甚至在几个基准测试中超越了Gemini Pro。</p><p>与LLaVA-1.5相比，LLaVA-NeXT有几个改进：</p><ol><li>将输入图像分辨率提高了4倍像素。这使得它能够捕捉更多的视觉细节。它支持三种宽高比，分辨率可达672x672、336x1344、1344x336。</li><li>通过改进的视觉指导调整数据混合，提供更好的视觉推理和OCR能力。针对更多场景提供更好的视觉对话，涵盖不同的应用。具有更好的世界知识和逻辑推理能力。</li><li>除了性能提升外，LLaVA-NeXT还保持了LLaVA-1.5的简约设计和数据效率。它重用了LLaVA-1.5的预训练连接器，并且仍然使用不到100万个视觉指导调整样本。最大的34B变种在约1天内使用32个A100完成训练。</li></ol><h3>Detailed Technical Improvement</h3><figure><img class="origin_image zh-lightbox-thumb" src="https://pic4.zhimg.com/v2-71f9cb077319722e3a51cbcd1ace63db_b.jpg" width="1344" /><figcaption>通过将图像分割成网格并独立对其进行编码，将LLaVA-1.5扩展到更高分辨率。这使得模型能够适应任何分辨率，而无需为ViTs执行位置嵌入插值。我们还将下采样图像的特征连接起来，以为LLM提供全局上下文。</figcaption></figure><p><b>Scaling to Higher Resolutions</b> 我们通过将图像分成原始训练视觉编码器的分辨率的较小图像块，并独立对其进行编码来克服这一问题。在获取单个块的特征图后，我们将它们合并成目标分辨率的单个大特征图，并将其馈送到LLM中。为了为LLM提供全局上下文并减少分割-编码-合并操作的人为因素，我们还将一个降采样图像的特征连接到合并后的特征图中。这使我们能够将输入扩展到任意分辨率并保持LLaVA-1.5的数据效率。我们将这个结果模型称为LLaVA-1.5-HD。</p><p><b>高质量的用户指导数据</b>。我们对高质量的视觉指导跟随数据的定义主要有两个标准：首先，<b>任务指令的多样性</b>，确保充分代表了在真实世界场景中可能遇到的广泛用户意图，特别是在模型部署阶段。其次，响应的优越性至关重要，目标是获得良好的用户反馈。为实现这一目标，我们考虑了两个数据来源：(1) 现有的GPT-V数据，包括LAION-GPT-V和ShareGPT-4V。(2) 为了进一步促进更多场景下更好的视觉对话，我们收集了一个包含不同应用的小型15K视觉指导调整数据集。这些指令和图像来自LLaVA演示，是真实用户的请求。我们仔细过滤可能涉及隐私问题或潜在有害的样本，并使用GPT-4V生成响应。</p><p><b>多模态文档/图表数据</b>。 (1) 我们从训练数据中删除了TextCaps，因为我们意识到TextCaps使用与TextVQA相同的训练图像集。这使我们能够更好地了解在开发过程中评估TextVQA时我们模型的零-shot OCR能力。为了维持和进一步提高我们模型的OCR能力，我们用DocVQA和SynDog-EN替换了TextCaps。(2) 受到Qwen-VL-7B-Chat的启发，我们进一步添加了ChartQA、DVQA和AI2D，以便更好地理解图表和图表的内容。</p><h3>Open Problems in LMMs</h3><p><b>数据效率</b> 在本节中，我们进行了进一步提高数据效率的实验，通过随机子采样LLaVA-1.5的训练数据混合，采样比例范围从0.1到0.5不等。我们在图4中可视化了不同采样变体的相对性能。首先，完整的数据混合提供了最佳的知识覆盖，并允许模型实现最佳的整体性能。令我们惊讶的是，<b>仅使用50%的样本，模型仍然保持了超过98%的完整数据集性能</b>。这表明在数据效率方面还有进一步改进的空间。其次，当将数据集缩减到50%时，模型在MMBench、ScienceQA和POPE上的性能完全不降低，甚至在MMBench上略有改善。同样，当进一步将数据从50%降至30%时，模型的性能保持稳定。这些结果显示了多模态模型也具有“少即是多”的潜在好处。</p><p><b>重新思考LMM中的幻觉</b> 将模型的输入分辨率提高到448时，这种幻觉显著减少。这一发现很有意思，因为它表明LMMs可能对训练数据中的一些错误具有鲁棒性。然而，当输入分辨率不足以使模型辨别训练数据中的所有细节，并且超出模型能力的数据量足够大时，模型会学会产生幻觉。这进一步表明，需要在提高数据注释的同时保持良好的模型处理信息的能力之间取得平衡。不平衡的扩展可能导致模型产生更多的幻觉或对视觉细节的理解能力降低。</p><h2>Cheap and Quick: Efficient Vision-Language Instruction Tuning for Large Language Models</h2><figure><img class="origin_image zh-lightbox-thumb" src="https://pic3.zhimg.com/v2-0437705810c0b3004d98d48610586c7a_b.jpg" width="973" /><figcaption>Mixture-of-Modality Adaptation (MMA)概述及LaVIN的架构。在LaVIN中，采用了新颖的混合模态适配器来处理不同模态的指令。在指导调优过程中，LaVIN通过端到端的模态混合训练（Mixture of Modality Training，MMT）进行优化。</figcaption></figure><p>本文提出了混合模态适应（Mixture-of-Modality Adaptation，MMA）：一种端到端的优化方案，通过轻量级适配器连接图像编码器和LLM。与此同时，我们还提出了MMA中的一种新颖路由算法，可以帮助模型自动调整单模态和多模态指令的推理路径。基于MMA，我们开发了一个名为LaVIN的大型视觉语言指导模型，它在各种遵循指令的任务中展现出了比现有多模态LLM更优异的训练效率和更好的推理能力。</p><p>LaVIN在效率上具有优越性，并且与现有的多模态LLM相比具有竞争力的性能，同时也确认了它作为通用聊天机器人的巨大潜力。实验结果显示，LaVIN可以达到与先进的多模态LLM（如LLaVA）相当的性能，同时减少了高达71.4%的训练时间和99.9%的存储成本。值得注意的是，将LaVIN在ScienceQA上进行微调仅需1.4小时，使用8个A100 GPU，更新的参数仅为3.8M。</p><h2>MIMIC-IT: Multi-Modal In-Context Instruction Tuning</h2><figure><img class="origin_image zh-lightbox-thumb" src="https://pic4.zhimg.com/v2-6f59245d58ce6b447d6ffdf48ab9a62f_b.jpg" width="965" /><figcaption>MIMIC-IT数据集包括280万个多模态指令-回复对，涵盖了基本能力：感知、推理和规划。每个指令都伴随着多模态的对话背景，使得在MIMIC-IT上训练的VLM能够展现出在交互式指令遵循方面的强大熟练度，实现零-shot泛化。</figcaption></figure><p>数据格式比较：LLaVA-Instruct-150K vs. MIMIC-IT。 (a) LLaVA-Instruct-150K由一张图片及其对应的仅包含语言的上下文信息（黄色框）组成。 (b) MIMIC-IT包含<b>多个图片或视频的输入数据</b>，并支持<b>多模态上下文信息</b>，即考虑图片/视频和语言输入作为上下文信息。</p><figure><img class="origin_image zh-lightbox-thumb" src="https://pic2.zhimg.com/v2-ccde8c99b09ee4344dc6c57b71df6069_b.jpg" width="966" /></figure><h2>LLaVAR: Enhanced Visual Instruction Tuning for Text-Rich Image Understanding</h2><p>本工作通过文本丰富的图像（例如电影海报、书籍封面等）增强了当前的视觉指令调整流程。具体而言，我们首先<b>使用公开可用的OCR工具在LAION数据集的422K个文本丰富的图像上收集结果</b>。此外，我们使用识别出的文本和图像标题提示纯文本GPT-4生成16K个对话，每个对话包含针对文本丰富图像的问答对。通过将我们收集的数据与先前的多模态指令遵循数据相结合，我们的模型LLaVAR大大提高了LLaVA模型在基于文本的VQA数据集上的能力（最多提高20%的准确率）。</p><figure><img class="origin_image zh-lightbox-thumb" src="https://pic1.zhimg.com/v2-d25705a7feacf4b3359d3327defe89f4_b.jpg" width="1006" /></figure><h2>SVIT: Scaling up Visual Instruction Tuning</h2><p>为了推动多模态能力的边界，我们提出了规模化视觉指导调整（SVIT）方法。</p><p>SVIT涉及构建一个包含420万个视觉指导调整数据点的数据集，包括160万个对话问答（QA）对，160万个复杂推理QA对，100万个引用QA对和10.6万个详细的图像描述。除了数量之外，所提出的数据集还具有高质量和丰富多样性。它是通过提示GPT-4与丰富的图像手动注释一起生成的。</p><p>此外，我们提出了一种新的数据处理方法，选择具有更好多样性和平衡性的子集，从而激发模型的优越能力。</p><h3>数据集选择算法</h3><figure><img class="origin_image zh-lightbox-thumb" src="https://pic1.zhimg.com/v2-e8ff97f47a8af096beeaa988ca51eddc_b.jpg" width="671" /></figure><p>流行的基准测试评估多模态大型语言模型（MLLM）的不同能力，这需要特定的训练数据配方来激发预训练模型。因此，我们设计了一种新的数据配方，即核心集选择算法，以更好地适应这些基准测试，并在性能和训练效率之间取得平衡。</p><p><b>多样性</b>。我们构建了一组与流行基准测试相匹配的关键概念，即MME和MMBench。具体来说，我们设计了几个高级概念，然后使用GPT-4生成每个概念的数十个关键词。然后，我们过滤掉在SVIT数据集中频率较低的那些关键词。概念集在上表中。我们通过与概念集的重叠来衡量每个训练样本的信息量，并选择最具信息量的样本。</p><p><b>平衡</b>。在MME基准测试中，使用“是”或“否”问题来评估模型。然而，在由GPT-4生成的数据中，这两个选择的比例极不平衡，这使得调整后的模型有倾向性地回答“是”。我们通过重新采样来调整比例。</p><p>通过以上两个操作，我们获得了157,712个样本的核心集SVIT-core-150K，其大小与LLaVA-Instruct-150K相同。我们还用SVIT-core-150K替换了LLaVA-v1.5-mix-665K中的LLaVA-Instruct-150K，从而生成了SVIT-mix-665K。</p><h2>Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond</h2><p class="ztext-empty-paragraph"><br /></p><figure><img class="origin_image zh-lightbox-thumb" src="https://pic1.zhimg.com/v2-8657c104fc246033ae6b776bc5700b6c_b.jpg" width="815" /></figure><p><b>预训练的第一阶段</b>，我们主要利用<b>大规模的、弱标记的、网络爬取</b>的图文对数据集。我们的预训练数据集由几个公开可访问的来源和一些内部数据组成。我们努力清理了数据集中的某些模式。原始数据集包含总共50亿个图文对，在清理后，仅剩14亿数据，其中77.3%是英文（文本）数据，22.7%是中文（文本）数据。<b>我们在这个阶段冻结了大型语言模型</b>，只优化了视觉编码器和VL适配器。输入图像被调整为224×224。训练目标是最小化文本标记的交叉熵。最大学习率为2e−4，训练过程使用了30720的图文对批量大小，整个预训练的第一阶段持续了50000个步骤，消耗了大约15亿个图文样本。更多的超参数详见附录C，该阶段的收敛曲线如图所示。</p><p><b>在多任务预训练的第二阶段</b>，我们引入了<b>高质量、细粒度的VL标注数据</b>，并使用<b>更大的输入分辨率和交替的图文数据</b>。同时训练了Qwen-VL的7个任务。对于文本生成，我们使用内部收集的语料库来维持LLM的能力。我们将视觉编码器的输入<b>分辨率从224×224增加到448×448</b>，减少了图像降采样造成的信息损失。我们解锁了大型语言模型并训练了整个模型。训练目标与预训练阶段相同。</p><p><b>在监督微调阶段</b>，我们通过指令微调来对Qwen-VL预训练模型进行微调，以增强其<b>指令跟随和对话能力</b>，从而得到交互式Qwen-VL-Chat模型。多模态指令调整数据主要来自通过LLM自我指导生成的字幕数据或对话数据，这些数据通常只涉及单图对话和推理，并且仅限于图像内容理解。我们通过手动注释、模型生成和策略串联构建了一个额外的对话数据集，以将定位和多图理解能力引入Qwen-VL模型。我们确认模型有效地将这些能力转移到更广泛的语言和问题类型上。此外，我们在训练过程中混合了多模态和纯文本对话数据，以确保模型在对话能力上的普遍性。指令调整数据量为35万。在这个阶段，<b>我们冻结了视觉编码器，并优化了语言模型和适配器模块</b>。我们在下面展示了该阶段的数据格式。</p><h2>NExT-GPT: Any-to-Any Multimodal LLM</h2><figure><img class="origin_image zh-lightbox-thumb" src="https://pic4.zhimg.com/v2-b7125d59f78d301182e6d5ece140e35b_b.jpg" width="1045" /></figure><p>作者提出了一个端到端通用的任意对任意MM-LLM（Multimodal-Large Language Model）系统。NExT-GPT将LLM与多模态适配器和不同的扩散解码器连接起来，使NExT-GPT能够感知输入并以文本、图像、视频和音频的任意组合生成输出。</p><p>NExT-GPT基本思想是利用编码器对各种模态的输入进行编码，将其投影为LLM可理解的类语言表示。ExT-GPT利用现有的开源LLM作为核心，处理输入信息，进行语义理解和推理。LLM不仅直接生成文本标记，而且还<b>产生独特的“模态信号”标记，这些标记作为指令来指示解码层是否要相应地输出什么模态内容</b>。然后，生成带有特定指令的多模态信号，经过投影后传输到不同的编码器，最终生成相应模态的内容。</p><p><b>Multimodal Encoding Stage</b></p><p>首先，NExT-GPT利用现有的完善模型对各种模式的输入进行编码。 对于不同的模态，有一组替代编码器，例如 Q-Former、ViT、CLIP。 在本文中，NExT-GPT采用了ImageBind，它是跨六种模式的统一高性能编码器。 然后，通过线性投影层，不同的输入表示被映射为LLM可以理解的类似语言的表示。</p><p><b>LLM Understanding and Reasoning Stage</b></p><p>在LLM方面，NExT-GPT采用的是 Vicuna2，它是一种基于开源文本的 LLM，广泛用于现有的 MM-LLM中。 LLM 将不同模态的表示作为输入，并对输入进行语义理解和推理。 它输出两项内容： 1) 直接文本响应； 2) 每种模态的信号标记，用作指示解码层是否生成多模态内容以及如果生成则生成什么内容的指令。</p><p><b>Multimodal Generation Stage</b></p><p>从 LLM接收到多模态信号之后，基于 Transformer 的输出投影层会将信号标记表示映射为后续多模态解码器可以理解的信号表示。 具体来说，NExT-GPT采用当前现成的潜在条件扩散模型（conditioned diffusion models）用于生成不同模态结果，包括用于图像合成的Stable Diffusion模型、用于视频合成的 Zeroscope4模型和用于音频合成的 AudioLDM5 模型。</p><p><b>Lightweight Multimodal Alignment Learning（轻量级多模态对齐学习）</b></p><figure><img class="origin_image zh-lightbox-thumb" src="https://pic4.zhimg.com/v2-6dac669948e35a4ffcf8f916d29f61cb_b.jpg" width="1000" /></figure><p>为了完成编码器对齐，作者从现有语料库和基准中准备了“X-caption”对（“X”代表图像、音频或视频，caption代表文字）数据，然后强制 LLM 根据标注caption生成每个输入模态的caption，学习过程如上图所示。</p><p class="ztext-empty-paragraph"><br /></p><figure><img class="origin_image zh-lightbox-thumb" src="https://pic2.zhimg.com/v2-deb265cc609ea29cb9babefae0960e61_b.jpg" width="986" /></figure><p> 在解码端，NExT-GPT集成了来自外部资源的预训练条件扩散模型，对齐的主要目的是将<b>扩散模型与LLM的输出指令保持一致</b>。 然而，在每个扩散模型和LLM之间执行全面的对齐过程将带来巨大的计算负担。 因此，我们在这里探索一种更有效的方法，即解码端指令跟随对齐，如上图所示。 具体来说，由于各种模态的扩散模型仅以文本标记输入为条件， 这种调节与NExT-GPT系统中 LLM 的模态信号标记不同，这导致扩散模型对 LLM 指令的准确解释存在差距。 因此，作者考虑<b>最小化 LLM 的模态信号标记表示与扩散模型的条件文本表示之间的距离</b>。 由于仅使用文本条件编码器（扩散模型的Text Encoder冻结），因此学习仅基于纯粹的字幕文本，即没有任何视觉或音频资源，这也确保了高度轻量级的训练。</p><p><b>2.3 Modality-switching Instruction Tuning（模态转化指令调优）</b></p><p>尽管编码和解码端能够与 LLM 保持一致，但距离使整个系统能够忠实地遵循和理解用户的指令并生成所需的多模态输出的目标仍然存在差距。 为了增强LLM的能力和可控性，进一步的指令调整（Instruction Tuning，IT）被认为有必要的。 IT 使用“（输入，输出）”对整体 MM-LLM 进行额外训练，其中“输入”代表用户的指令，“输出”表示符合给定指令的所需模型输出。</p><figure><img class="origin_image zh-lightbox-thumb" src="https://pic3.zhimg.com/v2-3fd3f7b2ef3dfe498b2f75d693d36c32_b.jpg" width="974" /></figure><p>具体来说，作者利用 LoRA使 NExT-GPT中的一小部分参数能够在 IT 阶段与两层投影同时更新。 如上图所示，当 IT 对话样本输入系统时，LLM 会重建并生成输入的文本内容（并使用多模态信号标记表示多模态内容），优化的目标是根据金标注和LLM的输出进行的。 除了LLM调优之外，作者还对NExT-GPT的解码端进行了微调，将输出投影编码的模态信号标记表示与扩散条件编码器编码的金多模态caption标注表示对齐。 至此，全面的调优过程更加接近与用户忠实有效交互的目标。</p><p>为了更好地进行指令调优，作者还收集了几组数据集，其中的“X”可以是图像、视频、音频或其他模态的数据：</p><ol><li>Text+X →Text Data：此类成熟的数据包括 LLaVA、miniGPT-4、VideoChat等；</li><li>Text →Text+X Data：基于现有语料库中丰富的“X-caption”对，通过一些模板，作者借用 GPT-4 来生成各种文本指令来产生数据。</li><li>modality-switching instruction tuning（MosIT） Data：作者设计了一些“人”角色和“机器”角色之间的模板对话示例，在此基础上促使 GPT-4 在各种场景下生成更多具有 100 多个主题或关键词的对话。</li></ol><h2>InternLM-XComposer: A Vision-Language Large Model for Advanced Text-image Comprehension and Composition</h2><figure><img class="origin_image zh-lightbox-thumb" src="https://pic3.zhimg.com/v2-6d80c18cd502e8ae7ca92e7ea270578e_b.jpg" width="808" /><figcaption>InternLM-XComposer的架构和训练方案。预训练阶段对齐了视觉和语言知识，SFT阶段激发了不同的模型能力。</figcaption></figure><p>模型由三个组件构成</p><ol><li>视觉编码器：EVA-CLIP （CLIP的一个改进变种，通过掩码图像建模能力增强，以有效捕捉输入图像的视觉细微差异）。输入 224x224，以 stride 14 分为小 patch 后输入 transformer</li><li>感知采样器（Perceive Sampler）：InternLM-XComposer 中的感知采样器作为一种专注的池化机制，旨在将初始的 257个 图像嵌入压缩为 64 个经过优化的嵌入。这些优化的嵌入随后会与大型语言模型理解的知识结构相匹配。与 BLIP2 类似，使用带有交叉注意力层的 BERTbase 作为感知采样器。</li><li>LLM：InternLM-XComposer 以 InternLM 作为其基础的大型语言模型。值得注意的是，InternLM 是一款强大的语言模型，具备多语言能力，在英语和中文方面表现出色。使用公开可用的 InternLM-Chat-7B 作为大型语言模型。</li></ol><h2>CogVLM: Visual Expert for Pretrained Language Models</h2><p>tl;nr: 使用已经训练好的LLM，然后给它添加图像的功能。方法上，引入vit 做图像的encoder和MLP adapter, 来将图像编码到和text一样的embedding空间中，然后是在LLM的各层添加visual expert，它具有独立的QKV和FFN相关的参数， 并使用LLM中的层来做初始化，训练的时候冻结已经训练好的LLM部分，训练图像相关的部分。这就是作者探讨的deep fusion方法。最后的效果提升很大。 除了很少的任务没有超过Pali-x之外，其他全部sota。</p><p>浅层对齐的方法： blip-2中，把已经训练好的image encoder冻结，然后加一个Q-former或者linear layer，把image feature映射到语言模型的input embedding space中, BLIP-2 NoCaps CIDEr 121.6。收敛很快，但是结果没有联合训练的模型效果好，e.g., PaLI-X. 用浅层对齐的方法训练的chat-style模型, e.g., MiniGPT-4, LLAVA, and VisualGLM , 视觉理解能力弱表现为幻觉。</p><p>作者认为核心问题是，浅层对齐缺少不同模态信息的deep fusion，这个灵感来自p-tuning和LoRA的对比，p-tuning learns a task prefix embedding in the input while <b>LoRA adapts the model weights in each layer via a low-rank matrix. LoRA效果更好且更稳定</b>。in the shallow alignment methods, the image features act like the prefix embedding in p-tuning. 其他细节： - 语言模型权重冻结，这些权重是为文本训练的，文本的输入空间，图像的embedding在这个空间里没有很好的对应关系，每一层的输入的分布也是不断变化的，当经过几层变换之后，图像的特征分布已经和比较深的层的权重所需要的输入特征的分布不再匹配了。 - 在预训练过程中，图像字幕任务的先验，例如文字风格和字幕长度，只能在浅对齐方法中编码到视觉特征中。它削弱了视觉特征与内容之间的一致性。</p><h3>CogVLM-17B 包含：</h3><ol><li>LLM：Frozen Vicuna-7B-v1.5，此模型在所有的注意力操作中都应用了因果掩码(causal mask)，包括图像特征之间的注意力。</li><li>ViT encoder：EVA2-CLIP-E ，负责将图像转化为特征表示。在CogVLM-17B中，<b>移除了ViT编码器的最后一层</b>，因为该层专注于整合 [CLS] 特征以用于对比学习。</li><li>MLP adapter：a two-layer SwiGLU MLP，用于将ViT的输出映射到与文本特征相同的空间。所有的图像特征在语言模型中共享相同的「位置编码id」。</li><li>Visual expert module：在 LLM 的每一层中引入可训练的 visual expert，其包含专门处理 image feature 的「QKV矩阵」和「MLP层」，以实现深度的视觉-语言特征对齐。QKV矩阵和MLP的形状与预训练语言模型中的相同，并从中进行初始化。trainable visual expert 专门用于转换图像特征，功能上和 LLM QKV/MLP 一致，但是只针对 image feature，从而实现模态间的深度融合。</li></ol><figure><img class="origin_image zh-lightbox-thumb" src="https://pic2.zhimg.com/v2-9ec9b63ccff457fb1ded52fd4b45ba85_b.jpg" width="1221" /><figcaption>CogVLM的架构。(a) 关于输入的说明，其中图像由预训练的ViT处理，并映射到与文本特征相同的空间中。(b) 语言模型中的Transformer块。图像特征具有不同的QKV矩阵和FFN。只有紫色部分是可训练的。&amp;lt;br&amp;gt;</figcaption></figure><p><b>PRETRAINING</b>: 用了公开可用的图像文本对进行训练，为 LAION-2B 和 COYO-700M</p><p><b>The first stage</b>：Image captioning loss, next token prediction task on 1.5B image-text pairs</p><p><b>The second stage</b>：a mixture of image captioning and Referring Expression Comprehension (REC) 。在答案的部分，只考虑了下一个标记的预测损失。REC 任务是根据 text description of an object 来预测图像中的 bounding box ，比如 “Question: Where is the [object]?” and “Answer: [x0, y0, x1, y1]” 。其中，x和y坐标的取值范围从000到999，表示在图像中的归一化位置。</p><h2>OtterHD: A High-Resolution Multi-modality Model</h2><p>在本文中，我们提出了OtterHD-8B，这是一种创新的多模态模型，是从Fuyu-8B演变而来，<b>专门设计用于以细粒度精度解释高分辨率视觉输入</b>。与传统模型不同，传统模型受固定大小的视觉编码器限制，OtterHD-8B具有<b>处理灵活输入尺寸的能力</b>，确保其在各种推理需求下的多功能性。除了这个模型，我们还引入了MagnifierBench，这是一个评估框架，旨在审查模型对微小物体的细节和空间关系的辨别能力。我们的比较分析显示，虽然目前领先的模型在这个基准测试中表现不佳，但特别是在直接处理高分辨率输入时，OtterHD-8B的表现优于其竞争对手很大程度上。这些发现揭示了不同模型在视觉信息处理中的结构差异，以及<b>视觉编码器的预训练分辨率差异对模型在这些基准测试中有效性的影响</b>。我们的研究突显了大型多模态模型中灵活性和高分辨率输入能力的关键作用，并且展示了Fuyu架构的简洁性在处理复杂视觉数据方面所具有的潜力。</p><h2>Monkey : Image Resolution and Text Label Are Important Things for Large Multi-modal Models</h2><p>Monkey模型提出了一种有效地<b>提高输入分辨率的方法</b>，最高可达 896 x 1344 像素，而<b>无需从零开始进行预训练</b>。针对复杂场景描述、问答和叙述，Monkey模型采用了一种无需预训练即可提高输入分辨率的架构和一种多层级详细描述生成方法。这两个设计确保了模型能够从生成的数据中进行更有效的学习，更高的分辨率可以更详尽地捕捉视觉特征，这反过来又提高了详细描述的有效性。</p><h3>1、提高输入分辨率</h3><figure><img class="origin_image zh-lightbox-thumb" src="https://pic2.zhimg.com/v2-2242209a29354f9b85a91c72d66766b9_b.jpg" width="1728" /><figcaption>Monkey的整体架构允许通过从原始图像中捕获全局特征和从分割补丁中获取局部特征来实现高分辨率。所有补丁都通过共享的静态ViT编码器进行处理，例如具有2b参数的ViT-BigG。</figcaption></figure><ol><li>给定一个H x W的图像，使用 x （和LMM分辨率一致）大小的滑动窗口将图像划分为更小的局部区域。Monkey对于每个图片块的编码器都增加了独属它的Lora[10]来有效地识别和吸收每个图像区域的细节敏感特征，从而增强对空间和上下文关系的理解。训练时只训练Lora部分，因此无需大幅增加参数量和计算需求。</li><li>原始图像大小也被调整为 x ，用于全局信息的提取。</li><li>最后，通过视觉编码器和重采样器处理所有局部图像和全局图像，并将局部特征和全局特征送入LLM。这种方法能够在不显着增加计算负载的情况下提高模型分辨率和性能。</li></ol><h3>2、多级特征整合详细描述生成</h3><p>之前的工作如LLaVA[3]、Qwen-VL[4]等依赖于互联网上爬取的大规模图文数据及进行模型的预训练。但这类数据标注比较简单，缺乏更丰富的图像细节。即使使用高分辨率图像进行训练， LMM 也无法在图像视觉特征和其中各个物体之间建立准确的关联，从而可能损害了视觉处理和语言理解之间的协同作用。Monkey使用了一种多级特征融合的详细描述生成方法（利用 BLIP-2[5]、PP-OCR[6]、GRIT[7]、SAM[8]和 ChatGPT[9]等预训练系统），为CC3M中的400k图像提供更加细致的描述，来更好地将高分辨率的视觉模型和语言模型对齐。</p><h3>关键发现</h3><ol><li>提高分辨率能提高模型性能（r3-r9），四个LoRA能够帮助模型获得图像中不同部分的独特特征(r7 vs. r9)，并帮助模型建立对空间和上下文关系的理解。进一步提高输入分辨率能够提高模型在文档等更高分辨率的图像上的性能(r5,r6)。同时，相比与直接插值扩大模型输入分辨率的方法相比(r1,r2 vs. r9)，本文的方法在时间和性能上更具优势。表六中当把llava1.5的输入分辨率从224扩大为448，性能得到显著提升，进一步展现了本文方法的有效性。</li></ol><figure><img class="origin_image zh-lightbox-thumb" src="https://pic2.zhimg.com/v2-9dd64535a2ebf14a8b4422b6e1f9ca7d_b.jpg" width="1221" /></figure><h2>LLaMA-VID: An Image is Worth 2 Tokens in Large Language Models</h2><p>当前的VLMs在诸如图像字幕和视觉问答等任务中表现出色，但在处理长视频时面临着计算负担，因为存在过多的视觉标记。LLaMA-VID通过用两个不同的标记表示每个帧来解决这个问题，即上下文标记和内容标记。上下文标记基于用户输入编码整体图像背景，而内容标记则封装了每个帧中的视觉线索。这种双标记策略显著减少了长视频的负担，同时又保留了关键信息。总的来说，LLaMA-VID赋予现有框架支持长达一小时的视频，并通过额外的上下文标记推动了它们的上限。在大多数基于视频或图像的基准测试中，它被证明超越了先前的方法。</p><figure><img class="origin_image zh-lightbox-thumb" src="https://pic4.zhimg.com/v2-9d163e917f310479ab7bf069a7e9a827_b.jpg" width="1205" /><figcaption>LLaMA-VID的框架。在用户指令下，LLaMA-VID通过接受单个图像或视频帧作为输入，并从LLM生成响应。该过程始于一个视觉编码器，将输入帧转换为视觉嵌入。然后，文本解码器根据用户输入生成文本查询。在上下文注意力中，文本查询从视觉嵌入中聚合与文本相关的视觉线索。为了提高效率，提供了将视觉嵌入降采样到各种令牌大小甚至单个令牌的选项。然后，使用线性投影器制定文本引导的上下文令牌和视觉丰富的内容令牌来表示每个时间t的每个帧。最后，LLM接受用户指令和所有视觉令牌作为输入并给出响应。&amp;lt;br&amp;gt;</figcaption></figure><h2>MoE-LLaVA: Mixture of Experts for Large Vision-Language Models</h2><p>最近的进展表明，扩展大型视觉语言模型（LVLMs）有效地提高了下游任务的性能。然而，现有的扩展方法使得所有模型参数在计算中对每个标记都是活跃的，这带来了巨大的训练和推理成本。在这项工作中，我们提出了一种简单而有效的训练策略MoE-Tuning用于LVLMs。这一策略创新地解决了多模态稀疏学习中的性能下降问题，从而构建了一个具有惊人参数数量但计算成本恒定的稀疏模型。此外，我们提出了基于MoE的稀疏LVLM体系结构MoE-LLaVA，它在部署过程中通过路由器唯一激活了仅排名靠前的k个专家，使其余的专家保持不活跃状态。大量实验证明了MoE-LLaVA在各种视觉理解和物体幻觉基准测试中的显著性能。值得注意的是，仅有约3B个稀疏激活参数，MoE-LLaVA在各种视觉理解数据集上表现出与LLaVA-1.5-7B相当的性能，甚至在物体幻觉基准测试中超过了LLaVA-1.5-13B。通过MoE-LLaVA，我们旨在建立稀疏LVLMs的基准，并为未来研究开发更高效、更有效的多模态学习系统提供宝贵的见解。</p><figure><img class="origin_image zh-lightbox-thumb" src="https://pic3.zhimg.com/v2-85189e0f8d3bd83219736302df49880e_b.jpg" width="801" /><figcaption>MoE-Tuning的示意图。MoE-Tuning包括三个阶段。在第一阶段，只有MLP被训练。在第二阶段，除了视觉编码器（VE）之外，所有参数都被训练。在第三阶段，FFN被用来初始化MoE中的专家，只有MoE层被训练。对于每个MoE层，每个标记只激活两个专家，而其他专家保持沉默。</figcaption></figure><p><b>阶段一</b>：在这个阶段，我们的目标是使图像标记适应LLM，使LLM能够理解图像中的实例。为了实现这一目标，我们使用MLP将图像标记投影到LLM的输入域中，将图像块视为伪文本标记。<b>在这个阶段，LLM被训练来描述图像</b>。MoE层在这个阶段不应用于LLM。</p><p><b>阶段二</b>：使用多模态指令数据进行调整是<b>增强大型模型能力和可控性</b>的关键技术。在这个阶段，LLM被调整为具有多模态理解能力的LVLM。我们使用更复杂的指令，包括<b>图像逻辑推理和文本识别等任务</b>，这些任务要求模型具有更强的多模态理解能力。通常情况下，对于密集型模型，LVLM训练在这个阶段被认为是完成的。然而，我们在同时将LLM转变为LVLM并稀疏化LVLM方面遇到了挑战。因此，MoE-LLaVA利用第二阶段的权重作为第三阶段的初始化，以缓解稀疏模型的学习困难。</p><p><b>阶段三</b>：作为初始化，<b>我们多次复制FFN以初始化专家</b>。当图像标记和文本标记被输入到MoE层时，路由器计算每个标记与专家之间的匹配权重。然后，每个标记都由前k个专家处理，并且根据路由器的权重进行加权求和。当激活前k个专家时，其余的专家保持沉默。这种建模方法形成了MoE-LLaVA，具有无限可能的稀疏路径，提供了广泛的能力。</p><figure><img class="origin_image zh-lightbox-thumb" src="https://pic1.zhimg.com/v2-a147e4b0e24d77acaf5a47757952ae6c_b.jpg" width="1181" /></figure><h2>LLaVA-UHD: an LMM Perceiving Any Aspect Ratio and High-Resolution Images</h2><p>该文讨论了视觉编码在大型多模态模型（LMMs）中对理解视觉世界的基础作用。它突出了现有LMMs的局限性，如固定的图像大小和分辨率，以及最近对这一方向的探索在适应性、效率甚至正确性方面存在的不足。</p><p>为了解决这些挑战，该论文介绍了LLaVA-UHD，一种大型多模态模型，旨在高效处理任何纵横比和高分辨率的图像。LLaVA-UHD包括三个主要组成部分：</p><figure><img class="origin_image zh-lightbox-thumb" src="https://pic3.zhimg.com/v2-112d8c1871aacda183546c18c931eea2_b.jpg" width="1023" /><figcaption>LLaVA-UHD框架。左图：给定一个高分辨率图像，LLaVA-UHD首先计算理想的切片数量，然后从可能的因式分解中选择最佳分区，将高分辨率图像分割成不同大小的切片。右图：切片通过在位置嵌入上进行2D插值以保持原始纵横比进行编码，然后压缩并按空间结构排列以供LLM处理。</figcaption></figure><ol><li>图像模块化策略：该策略将原始分辨率的图像划分为较小的可变大小切片，以便进行高效和可扩展的编码。</li><li>压缩模块：该模块进一步压缩由视觉编码器生成的图像标记，增强了效率。</li><li>空间结构：一种用于组织切片标记以供LLMs理解空间关系的模式。</li></ol><h3>模块化视觉编码</h3><p>针对具有不同纵横比的高分辨率图像，一个朴素的方法是将ViT的位置嵌入插值到目标形状，以整体编码。然而，这种方法由于二次计算成本和由于分布外问题导致的性能降低而不是最佳的。为了解决这个挑战，我们提出了一种模块化的视觉编码策略。基本思想是将原始分辨率图像划分为<b>较小的可变大小切片</b>，其中每个切片的形状与ViT的标准预训练设置不会偏离太远。通过可变大小的切片，LLaVA-UHD可以在不需要填充或形状扭曲的情况下实现对原始分辨率图像的完全适应性。</p><p>接下来，我们对P进行二维插值，以适应由分区策略给出的切片分辨率，用于视觉编码。在我们的实验中，我们表明，在预训练期间可以保持ViT和位置嵌入参数不变，并且在instruction tuning阶段更新这些参数就足以实现良好的性能。除了切片之外，我们还提供了一个以本机纵横比的低分辨率概览图像。概览图像可以提供图像的粗略信息和全局语义连接。</p><h3>压缩层</h3><p>高分辨率图像需要LLMs处理更多的视觉标记，这占据了大部分计算量。例如，一个672×1008的分辨率图像将为LLaVA-1.5生成3456个视觉标记。为了解决这个问题，我们使用一个<b>共享的感知器重新采样器层来压缩每个图像切片的视觉标记</b>。具体来说，由视觉编码器输出的图像标记<b>通过一组查询向量通过交叉注意力被重新采样</b>为较少的数量（在我们的实验中从576个到64个）。与流行的基于MLP的视觉投影方法相比，感知器重新采样器不受图像分辨率的限制，始终保持固定且可负担得起的视觉标记数量，因此更适用于理解高分辨率图像。因此，LLaVA-UHD可以使用比LLaVA-1.5在编码336×336分辨率图像时更低的计算成本来编码672×1008分辨率图像。</p><h3>图像切片的空间结构</h3><p>由于图像分区在不同图像之间是动态的，因此有必要向LLM提供图像切片的空间组织信息。受FuYu模型的启发，我们设计了一个空间模式来使用两个特殊标记指示图像切片的相对位置。具体地，<b>我们使用“,”来分隔一行中的切片表示，并使用“\n”来分隔不同的行</b>。在我们的实验中，我们发现这种简单的模式可以有效地向动态分区提供信息，从而产生良好的性能。</p><p>全面的实验证明，即使建立在分辨率为336×336的LLaVA-1.5架构上，LLaVA-UHD支持高达672×1088的图像，并且在仅使用94％的推断计算量的情况下，在TextVQA上取得了6.4％的准确率提高。此外，该模型在学术环境中可以高效训练，在8个A100 GPU上仅需23小时，而LLaVA-1.5则需要26小时。</p><h2>Yi-VL</h2><figure><img class="origin_image zh-lightbox-thumb" src="https://pic1.zhimg.com/v2-b65afa1658259064f9bab257cbfe1308_b.jpg" width="711" /></figure><p>Yi-VL采用了LLaVA架构，经过全面的三阶段训练过程，以将视觉信息与Yi LLM的语义空间良好对齐：</p><p>第1阶段：<b>ViT和投影模块的参数使用224×224的图像分辨率进行训练</b>。LLM的权重被冻结。训练利用包含来自LAION-400M的1亿个图像-文本对的图像标题数据集。主要目标是增强ViT在指定架构内的知识获取能力，并实现ViT和LLM之间更好的对齐。</p><p>第2阶段：<b>ViT的图像分辨率扩展到448×448</b>，并训练ViT和投影模块的参数。它旨在进一步提升模型对复杂视觉细节的识别能力。此阶段使用的数据集包括约2500万个图像-文本对，例如LAION-400M、CLLaVA、LLaVAR、Flickr、VQAv2、RefCOCO、Visual7w等。</p><p>第3阶段：训练整个模型的参数（即ViT、投影模块和LLM）。主要目标是增强模型在多模态对话交互中的熟练程度，从而赋予其无缝整合和解释视觉和语言输入的能力。为此，训练数据集涵盖了各种来源，总计约100万个图像-文本对，包括GQA、VizWiz VQA、TextCaps、OCR-VQA、Visual Genome、LAION GPT4V等。为确保数据平衡，我们对<b>任何单个来源的最大数据贡献设定了上限，限制为不超过5万对</b>。</p><p class="ztext-empty-paragraph"><br /></p><p>最后，欢迎大家关注github，聚合了Multimodal Large Language Models, Large Language Models, and Diffusion Models以及一些前沿研究方向的一些阅读笔记，非常欢迎大家补充完善</p><a class=" external" href="https://link.zhihu.com/?target=https%3A//github.com/yfzhang114/Awesome-Multimodal-Large-Language-Models" rel="nofollow noreferrer" target="_blank"><span class="invisible">https://</span><span class="visible">github.com/yfzhang114/A</span><span class="invisible">wesome-Multimodal-Large-Language-Models</span><span class="ellipsis"></span></a><p></p>
]]></content:encoded>
<pubDate>Sat, 30 Mar 2024 07:28:02 GMT</pubDate>
</item>
<item>
<title>中科院，CMU，Meta等联合出品：去除LLM先验对多模态大模型的负面影响</title>
<link>https://zhuanlan.zhihu.com/p/686461442</link>
<guid>https://zhuanlan.zhihu.com/p/686461442</guid>
<content:encoded><![CDATA[
<div> LVLMs，偏见，模态差异，后处理去偏，解码配置
<br />
总结: 本文研究了大型视觉语言模型（LVLMs）存在的偏见问题，发现其内容主要受底层大型语言模型（LLMs）的影响而非输入图像。作者提出后处理去偏和去偏抽样方法来解决偏见问题，有效减轻幻觉并提高LVLM的推理能力。通过优化解码配置，触发现有LVLM的潜力释放，超越默认配置带来显著的性能提升。总体而言，针对LVLM的偏见问题探索了有效的解决方案，同时揭示了评估方法的不足之处，为未来研究和应用提供了有益的启示。 <div>
<p>大型视觉语言模型在近期发展迅速（LVLMs），他们使用图像-文本对进行预训练或使用专门的视觉指导调整数据集进行微调，将大语言模型（LLM）拓展为了处理文本和图像的多模态模型。尽管这些模型擅长理解复杂的视觉模式并将其转化为语言，来自中科院，CMU，阿里巴巴，squirrel AI以及Meta AI的研究人员揭示了一个显著问题</p><p><i>由LVLMs生成的内容在很大程度上偏向于在预训练期间使用的底层LLMs，而不是受到输入图像的影响。即使在图像完全噪声或不存在的情况下，LVLMs也会生成置信度很高的答案，表明了LVLM存在一种对LLM先验的偏见</i></p><p>针对这个问题，作者提出了两种通过training-free的思路进行debias。</p><figure><img class="origin_image zh-lightbox-thumb" src="https://pic1.zhimg.com/v2-f8322c1904539b8b3448d7658a737afa_720w.jpg?source=d16d100b" width="1475" /></figure><p><br /></p><p>Debiasing Large Visual Language Models / Debiasing Multimodal Large Language Models</p><p>论文：<a class=" external" href="http://link.zhihu.com/?target=https%3A//arxiv.org/abs/2403.05262" rel="nofollow noreferrer" target="_blank"><span class="invisible">https://</span><span class="visible">arxiv.org/abs/2403.0526</span><span class="invisible">2</span><span class="ellipsis"></span></a></p><p>Code: <a class=" external" href="http://link.zhihu.com/?target=https%3A//github.com/yfzhang114/LLaVA-Align" rel="nofollow noreferrer" target="_blank"><span class="invisible">https://</span><span class="visible">github.com/yfzhang114/L</span><span class="invisible">LaVA-Align</span><span class="ellipsis"></span></a></p><h2>Exploring the Impact of LLM Biases on LVLMs</h2><figure><img class="origin_image zh-lightbox-thumb" src="https://pic1.zhimg.com/v2-e70c2072c22a0f1f14ab2ae2fa598717_720w.jpg?source=d16d100b" width="718" /></figure><p>为了评估LLM偏见对LVLMs的影响，本文使用上表中的五个目标问题进行了实验。对于MSCOCO数据集的每个类别，本文设计了相应的提示，并将其与不同类型的视觉输入配对，包括与问题完全不相关的视觉输入，如完全采样于高斯噪声的图像（Noise）或完全黑色或白色的图像（Zero/One），或者用纯文本替换视觉输入，其中要么删除所有视觉标记（None），要么用类似于的无意义占位符替换它们（Unk）。</p><figure><img class="origin_image zh-lightbox-thumb" src="https://picx.zhimg.com/v2-2b6c7678d3720f56d951c8f688634a4d_720w.jpg?source=d16d100b" width="1509" /><figcaption>LLaVA-1.5-7B</figcaption></figure><p>如上图(a)所示，LVLMs表现出对特定答案的偏见，比如作者询问LVLM关于图中并不存在的狐狸颜色的问题，结果产生自信但不切实际的答案，如棕色、灰色或蓝色。图(b)显示了在这些情况下LVLM输出的前15个选择及其对应的概率。令人惊讶的是，即使没有可用的图像或问题涉及不存在或无意义的图像，当前的LVLMs往往会产生具体的答案。这表明LVLM存在一种实质性的偏见源自对LLMs的预训练。这样的偏见带来了持久的挑战，尤其是幻觉问题，构成了对LVLMs可靠性和适用性的显著威胁。</p><h3>现象分析</h3><figure><img class="origin_image zh-lightbox-thumb" src="https://picx.zhimg.com/v2-98bae0b6ea9ecce6c3c1b1a91a7b0f12_720w.jpg?source=d16d100b" width="1417" /></figure><p>作者进一步分析注意机制，以解释为什么模型严重依赖LLM偏见。如上图所示，LLM输出倾向于为文本标记分配更多的注意力。即使在原始图像-文本对（naive），其中图像token的数量超过文本token的数量的情况下，文本的注意力分数之和也超过了90%。有趣的是，当作者输入原始文本-图像对（Naive），纯文本（None）和完全嘈杂的图像（Noise）时，模型展现出相似的注意力模式。大多数注意力集中在特殊但无信息的token上，而不是视觉标记或文本序列，比如问题本身。</p><figure><img class="origin_image zh-lightbox-thumb" src="https://pica.zhimg.com/v2-8338f25eb8e214deb3967910868eb88e_720w.jpg?source=d16d100b" width="1456" /></figure><p>在<b>开放式生成任务</b>中，作者随机选择了LLaVA-Bench中的两个问题，并检查在生成更多标记时的注意力分数。如上图所示，随着在开放式任务中生成文本长度的增加，分配给图像的注意力相应减少，加剧了这一问题。因此，模型越来越容易独立于输入图像生成内容，这从注意力的角度揭示了幻觉内容产生的起因。此外，结果还表明，浅层对视觉标记分配更高的注意力；然而，随着特征深入到更深的层次，对视觉标记的关注会减少。</p><p><b>根本原因：</b> 有几个因素导致这一现象的存在，其中<b>模态差异</b>[1]为一个关键影响因素。这种差异表现为图像-文本嵌入始终存在于不同的空间，并且距离相对较大，即使在经过大量图像-文本对训练的模型(CLIP)中也观察到这一特征。不幸的是，LVLMs始终在受限的视觉-语言数据集上进行训练，导致特征对齐受到削弱。相比之下，参与该过程的LLMs在更广泛的训练语料库中经过了训练，在基于文本的任务方面表现出了更高的熟练度。显著的模态差距使LLMs主要专注于熟悉的标记或表示，忽略关键的视觉标记。此外，根据从<b><i>shallow-to-deep</i></b>[2]，浅层网络可以灵活适应数据流的变化或在有限数据下更高效地学习。这解释了为什么浅层相较于深层更好地分配注意力分数。浅层可能调整得更好，但深层保留了大部分原始LLM模式，并倾向于忽略陌生的视觉标记。此外，LVLMs可能继承LLMs的缺点，后者经常<b>忽略上下文信息，严重依赖其先验的参数知识</b>[3]，进一步降低了视觉标记的影响。这些发现强调了当前训练策略和数据集的固有局限性，强调了两种模态之间增强对齐或更高程度认识</p><h2>Model Calibration</h2><figure><img class="origin_image zh-lightbox-thumb" src="https://picx.zhimg.com/v2-80230fd397ca00f2db960d6b54636095_720w.jpg?source=d16d100b" width="1387" /><figcaption>本文提出的后处理去偏和去偏采样方法的示意图, 分别关注于调整预测结果和生成分布，从而提高模型对视觉信息的关注度，增强生成的图像描述的质量。</figcaption></figure><h3>Post-hoc Debiasing of Prediction Results (Fig. 3 (left))</h3><p><b>解决偏见问题</b>： 为了解决模型输出中的偏见问题，我们引入了一个“校准”步骤，通过仿射变换调整模型的输出概率：<img alt="y=softmax(Wp_\theta+b)" src="https://www.zhihu.com/equation?tex=y%3Dsoftmax%28Wp_%5Ctheta%2Bb%29" />。在这个方程中，一个权重矩阵<img alt="W" src="https://www.zhihu.com/equation?tex=W" />和一个偏置向量<img alt="b" src="https://www.zhihu.com/equation?tex=b" />被应用于原始概率，产生调整后的概率。对于分类任务，<img alt="p" src="https://www.zhihu.com/equation?tex=p" />表示与每个标签名称相关的概率，被归一化为总和为一。为了提高效率，我们将矩阵<img alt="W" src="https://www.zhihu.com/equation?tex=W" />限制为对角矩阵。</p><p><b>学习<img alt="W" src="https://www.zhihu.com/equation?tex=W" />的直观方法</b>： 学习<img alt="W" src="https://www.zhihu.com/equation?tex=W" />的直观方法是通过输入一个无意义的视觉输入<img alt="v'" src="https://www.zhihu.com/equation?tex=v%27" />（如None或Noise）来初始估计LLM对某些答案的偏见。由此产生的输出预测被表示为<img alt="p':=p'_\theta(y|x,v')" src="https://www.zhihu.com/equation?tex=p%27%3A%3Dp%27_%5Ctheta%28y%7Cx%2Cv%27%29" />。理想情况下，LLMs应该对这个测试输入分配一个均匀的分布得分。例如，当提出问题<b>图片中的狐狸颜色是棕色吗？</b>时，提供一个无意义的图像或没有图像应该导致对'是'和'否'的概率相等。这是因为模型无法做出决定。然而，由于模型的偏见，它倾向于为'是'分配更高的分数。通过将<img alt="W=\text{diag}(p'_\theta)^{-1}" src="https://www.zhihu.com/equation?tex=W%3D%5Ctext%7Bdiag%7D%28p%27_%5Ctheta%29%5E%7B-1%7D" />和<img alt="b" src="https://www.zhihu.com/equation?tex=b" />设置为全零向量，可以纠正这个错误。随后，去偏的结果得到为<img alt="y=softmax(Wp_\theta+b)" src="https://www.zhihu.com/equation?tex=y%3Dsoftmax%28Wp_%5Ctheta%2Bb%29" />。</p><p>实验结果发现： 我们的实验发现，使用不相关的视觉输入（Noise、Zero、One）并不一致地提高模型性能，并对问题类型表现出敏感性。因此，作为默认的去偏方法，我们选择无图像的方法，具体而言是<b>None</b>和<b>Unk</b>。此外，我们还尝试了一种使用<b>Both</b>无图像输入的去偏方法。在这种配置中，我们将<img alt="v'" src="https://www.zhihu.com/equation?tex=v%27" />设置为None和$$，得到相应的概率分布<img alt="p'_n" src="https://www.zhihu.com/equation?tex=p%27_n" />和<img alt="p'_u" src="https://www.zhihu.com/equation?tex=p%27_u" />。然后，我们计算概率分布<img alt="p' = (p'_n + p'_u)/2" src="https://www.zhihu.com/equation?tex=p%27+%3D+%28p%27_n+%2B+p%27_u%29%2F2" />。</p><h3>Visual Debias Decoding (VDD) (Fig. 3 (Right))</h3><p>挑战与引入对比解码： 尽管后处理去偏方法可以自然地扩展到以整个词汇表作为标签空间的开放式生成环境中，但由于标签空间庞大，其实施面临挑战。同时，保持输出文本标记的相关性或共现性至关重要，仅强制执行均匀的语言模型输出分布可能会显著损害生成质量。为此，我们采用对比解码的概念，引入VDD策略。</p><p><b>VDD策略概述</b>： 与后处理去偏类似，我们将原始图像和文本输入到LVLM中，获取logits <img alt="l:= l_\theta(y|x,v)" src="https://www.zhihu.com/equation?tex=l%3A%3D+l_%5Ctheta%28y%7Cx%2Cv%29" />，同时使用无图像输入获取对数 <img alt="l':=l_\theta(y|x,v')" src="https://www.zhihu.com/equation?tex=l%27%3A%3Dl_%5Ctheta%28y%7Cx%2Cv%27%29" />，其中 <img alt="l'" src="https://www.zhihu.com/equation?tex=l%27" /> 只包含文本先验。为了强调视觉信息的贡献，我们旨在减轻无图像logits突出的不良行为，并基于LVLM在提供图像输入时的剩余正面行为生成文本。为了实现这一概念，我们提出了对比目标：</p><p><img alt="p_{vdd}(y|v,v',x)=\text{softmax}\left[(1+\alpha) l_\theta(y|x,v)-\alpha l_\theta(y|x,v')\right] \\" src="https://www.zhihu.com/equation?tex=p_%7Bvdd%7D%28y%7Cv%2Cv%27%2Cx%29%3D%5Ctext%7Bsoftmax%7D%5Cleft%5B%281%2B%5Calpha%29+l_%5Ctheta%28y%7Cx%2Cv%29-%5Calpha+l_%5Ctheta%28y%7Cx%2Cv%27%29%5Cright%5D+%5C%5C" /></p><p><b>自适应合理性约束</b>： 从纯文本模型中得到的对数并非总有害的。它们可以灵活地捕捉英语语法和常识的各个基本方面。因此，在每种情况下都对所有标记采用通用的惩罚可能不合适。为了解决这一挑战，我们引入了跨LVLMs输出词汇 <img alt="\mathcal{V}" src="https://www.zhihu.com/equation?tex=%5Cmathcal%7BV%7D" /> 的自适应合理性约束。该约束与输出分布与原始视觉输入相关的置信水平密切相关：</p><p><img alt="\mathcal{V}_{head}(y_{t})=\{y_i\in\mathcal{V}:p_\theta(y_i|v,t,y_{:t})\geq \beta\max_{w\in\mathcal{V}}p_\theta(w|v,t,y_{:t})\} \\" src="https://www.zhihu.com/equation?tex=%5Cmathcal%7BV%7D_%7Bhead%7D%28y_%7Bt%7D%29%3D%5C%7By_i%5Cin%5Cmathcal%7BV%7D%3Ap_%5Ctheta%28y_i%7Cv%2Ct%2Cy_%7B%3At%7D%29%5Cgeq+%5Cbeta%5Cmax_%7Bw%5Cin%5Cmathcal%7BV%7D%7Dp_%5Ctheta%28w%7Cv%2Ct%2Cy_%7B%3At%7D%29%5C%7D+%5C%5C" /></p><p>在这里，<img alt="\beta\in[0,1]" src="https://www.zhihu.com/equation?tex=%5Cbeta%5Cin%5B0%2C1%5D" /> 是一个截断<img alt="p_\theta" src="https://www.zhihu.com/equation?tex=p_%5Ctheta" />下一个标记分布的超参数。较大的 <img alt="\beta" src="https://www.zhihu.com/equation?tex=%5Cbeta" /> 表示更激进的截断，仅保留高概率标记。在生成过程中，我们将所有不在 <img alt="\mathcal{V}_{head}" src="https://www.zhihu.com/equation?tex=%5Cmathcal%7BV%7D_%7Bhead%7D" /> 中的标记的对数设置为<img alt="-\infty" src="https://www.zhihu.com/equation?tex=-%5Cinfty" />：</p><p>VDD方法的优势： 自适应合理性约束灵活地保留了考虑视觉信息时的高概率标记，通过利用纯文本对数减轻偏见。该约束在专家对特定标记表现出高置信度时最多保留候选池中的一个标记，有效地减轻了对比目标的影响。重要的是，我们的VDD方法只需要将纯文本输入到LVLMs中，相对于视觉对比解码提高了效率。这避免了使用扭曲图像生成参考对数时涉及到的复杂性，该过程需要对注入到图像中的噪声水平进行精细调整。类似于后处理去偏方法，这里将这些方法表示为VDD-None，VDD-Unk和VDD-Both，分别对应于使用无图像输入None，Unk和Both来获取参考对数。</p><h2>Impact of Decoding Configuration on Model Performance</h2><figure><img class="origin_image zh-lightbox-thumb" src="https://pic1.zhimg.com/v2-dfe6bbe20c822869c6978fc8292b2ea4_720w.jpg?source=d16d100b" width="1341" /><figcaption>利用不同的生成策略改善了整体真实性评分。上标1和2引用了不同论文报告的结果，其中“rlhf”表示使用RLHF技术进行微调的模型。在这个例子中，通过将温度从默认的1.0调整为0.2，成功生成了对齐的输出。&lt;br&gt;</figcaption></figure><p>此外，我们发现目前的LVLM在各种生成配置下表现出显著的不稳定性。我们的主要假设是，现有的评估主要基于默认的解码设置，限制了对模型全面能力的探索。如上图所示，生成配置的多样选择会导致显著不同的性能结果。值得注意的是，调整生成温度可以显著提高POPE-MSCOCO的平均F1分数，从76提高到约84.04。这一提高远远超过了13B模型和RLHF调优模型以前取得的结果，表明目前对LVLM能力的估计可能被低估了。此外，我们的调查发现不同的模型对不同的生成配置有偏好，突显了当前LVLM评估的不稳定性。这引发了对评估公平性的担忧，特别是因为它们经常依赖于默认的生成配置或有选择地选择为提出的模型选择最佳配置。为了最大程度发挥现有LVLMs的潜力，我们系统地研究了四个基准测试中六个LVLM模型，精心搜索它们的生成配置以找到最佳设置。我们的结果在很大程度上显著优于先前报告的结果，强调了为每个模型选择最佳解码配置的重要性。</p><h2>Experiments</h2><p>我们进行了一系列深入的实验，以证明： (i) 提出的去偏方法显著减轻了幻觉并提高了推理能力； (ii) 探索最佳生成配置释放了现有LVLMs的全部潜力，相比默认配置取得了显著的性能提升； (iii) 去偏方法纠正了模型的预测，特别是在模型缺乏信心且容易出错的情况下； (iv) 我们详细检查了我们方法的失败案例，揭示了当前评估基准中的一些缺陷。</p><h3>后处理去偏和去偏抽样方法对LVLM性能的影响:</h3><p>我们通过结合各种去偏策略，包括后处理去偏方法和去偏抽样策略，进行了全面的分析，以研究它们对LVLM的真实性和推理能力的影响。我们在MME数据集上获得的结果如下所示。</p><figure><img class="origin_image zh-lightbox-thumb" src="https://picx.zhimg.com/v2-a14e82dd7bb62536d9cd472f8c81f6d0_720w.jpg?source=d16d100b" width="1057" /></figure><p>我们的主要观察结果可以总结如下：</p><ol><li>后处理去偏方法通过减轻幻觉显著提高了模型的真实性，其中None和Unk策略在减轻对象和属性幻觉方面表现优越，尤其是在减轻对象和属性幻觉方面；</li><li>去偏抽样策略取得了更好的平衡，VDD-None和VDD-Unk在竞争性幻觉得分的同时，在推理得分上始终表现优越；</li><li>结合不同的后处理去偏策略（Both）并没有产生显著的好处，而结合去偏抽样策略则显示出有希望的结果；</li><li>后处理去偏方法独立于采样方法，如VCD和我们提出的去偏抽样方法。例如，结合VCD采样和Unk后处理去偏的VCD (u) 的性能优于仅使用VCD。值得注意的是，VCD在LLaVA-13B模型上表现不佳，可能是因为它对图像噪声水平敏感。相反，我们的模型依赖于文本而不是图像，从而提高了效率和鲁棒性。其他数据集的结果参考原文。</li></ol><h3>解码策略对LVLM性能的影响:</h3><figure><img class="origin_image zh-lightbox-thumb" src="https://picx.zhimg.com/v2-c1f6129485a158ccd179fd29c44d99b4_720w.jpg?source=d16d100b" width="1518" /></figure><p>在左侧，我们通过采用不同的解码策略和设置，对LLaVA-v1.5模型在三个不同数据集（POPE-MSCOCO，MME和MMMU）上的性能进行了全面评估，包括温度Temp <img alt="\tau" src="https://www.zhihu.com/equation?tex=%5Ctau" />}、Top-<img alt="k" src="https://www.zhihu.com/equation?tex=k" />和Top-<img alt="p" src="https://www.zhihu.com/equation?tex=p" />。其中，“Default”列表示使用默认采样参数的基线性能，“Sampling”列展示了通过不同解码策略获得的结果，“Overall”列突显了在所有子集上通过所有采样策略实现的最佳性能。值得注意的是，应用最佳采样策略极大地提高了整体模型性能，相较于默认配置有显著改进。为了强调这一现象超越LLaVA模型的普适性，我们扩展了对POPE-MSCOCO基准中Qwen-VL和InstructBLIP的评估。结果一致表明，通过精细调整的采样策略可以提高现有LVLM的性能。在的右侧，我们强调了我们提出的方法在与替代策略的比较中的卓越性能，标为“Ours”。我们的方法在特定指导数据集上超过了微调模型，突显了我们精细调整的采样策略的有效性。此外，采用这些精细调整的采样策略的7B模型在不同的微调策略下（包括监督微调和RLHF）胜过了13B模型。</p><h2>总体性能:</h2><p><br /></p><figure><img class="origin_image zh-lightbox-thumb" src="https://pica.zhimg.com/v2-066b853f5acb3483dada10eab510e649_720w.jpg?source=d16d100b" width="1521" /></figure><p> 我们首先确定了所有骨干网络的最佳采样配置。随后，我们应用去偏采样方法。同时，对于分类任务，我们同时使用None和Unk输入实施我们的后处理去偏方法。如上图所示，很明显VCD未能在所有基线模型中一致提高性能，特别是在推理任务中。相比之下，我们提出的方法在所有方面和骨干模型上都展示出卓越的结果。</p><figure><img class="origin_image zh-lightbox-thumb" src="https://pic1.zhimg.com/v2-193d2a161ebbb6abeb9b80535fe5a743_720w.jpg?source=d16d100b" width="1510" /></figure><p>最后，我们在LLava-Bench上评估了所提出的方法，对于开放式生成任务，VCD在一致超越默认解码配置方面并不成功，尽管它偶尔在复杂子集中表现优异。相反，我们提出的VDD 在改善默认解码策略方面一直表现出色。VDD 部分的右侧列说明了，对于生成任务，模型性能一直受到各种解码配置的影响。与偏爱低温度(<img alt="\tau" src="https://www.zhihu.com/equation?tex=%5Ctau" />)的多选QA任务不同，我们观察到生成任务受益于较高的<img alt="\tau" src="https://www.zhihu.com/equation?tex=%5Ctau" />或更大的<img alt="p" src="https://www.zhihu.com/equation?tex=p" />值。详细结果和更大骨干网络参照原文。</p><figure><img class="origin_image zh-lightbox-thumb" src="https://pic1.zhimg.com/v2-bf75a820b99e0f123c1571e65d9a667e_720w.jpg?source=d16d100b" width="975" /><figcaption>以下是展示VDD对LLaVA-v1.5-13B影响的另一个定性示例。VDD被证明更有帮助，幻觉更少。生成内容更清晰，已明确突出显示了幻觉内容。&lt;br&gt;</figcaption></figure><p>原文也有很多定性实验，总的来说，VDD生成的结果更有帮助且幻觉较少。</p><h3>校准在模型预测缺乏信心时特别有优势:</h3><figure><img class="origin_image zh-lightbox-thumb" src="https://pic1.zhimg.com/v2-3265f7aac2cb80ca0321875709f8d893_720w.jpg?source=d16d100b" width="1300" /><figcaption>比较不同置信区间下后处理去偏方法的有效性:</figcaption></figure><p>上图，我们分析了各种后处理去偏方法。很明显，在模型展现低信心的情况下，“Naive”方法在性能上表现不佳，这符合直觉。相反，我们提出的后处理去偏方法在这些情况下取得了显著的性能提升。随着预测置信度的提高，模型的预测变得更加可靠和准确，导致性能增益减小。即使在高置信度水平（约为0.9-1.0）下，我们提出的方法也能够取得可比较的结果而没有不利影响。因此，我们提出的方法的总体性能超过了Naive方法。值得注意的是，两种基于图像的去偏方法在低置信度得分的情况下表现出色，表明对预测结果进行了激进的调整。然而，这种激进的调整可能导致对高置信度样本的较差预测。因此，我们默认使用基于文本的去偏方法，因为它们始终能够带来改进。</p><h3>目前某些benchmark存在的问题</h3><figure><img class="origin_image zh-lightbox-thumb" src="https://pic1.zhimg.com/v2-1123a6858edfb95bba7f7a8fa9a68b55_720w.jpg?source=d16d100b" width="1380" /></figure><p>如上图所示，我们揭示了去偏方法与受到LLM（大型语言模型）偏倚影响的基准之间错综复杂的相互作用，其中Naive方法的性能主要围绕文本信息。我们提出的后处理去偏方法在MMMU基准上表现出不一致的改进。经过仔细检查，当我们通过将输入图像替换为空字符串（None<img alt="^*" src="https://www.zhihu.com/equation?tex=%5E%2A" />）或标识符（Unk<img alt="^*" src="https://www.zhihu.com/equation?tex=%5E%2A" />）来完全删除视觉信息时，模型在特定子集中保持相当或更优越的性能。在这些情况下，我们设计用于缓解LVLM偏倚的后处理方法未能产生最佳结果。特别是在纯粹的LLMs表现卓越的情况下，应用后处理方法可能会产生不利影响。重要的是要强调，为LVLMs定制的基准应优先依赖输入图像，而不仅仅是文本内容。因此，我们的方法可以作为有价值的指标；当所提出的去偏方法表现不佳时，这表明基准可能更偏向LLMs，可能不适合有效评估LVLMs。在这种情况下，所提出的采样方法，VDD，与Naive相比也表现出劣势，平均准确率为<img alt="34.3%" src="https://www.zhihu.com/equation?tex=34.3%25" />对比<img alt="35.7%" src="https://www.zhihu.com/equation?tex=35.7%25" />。</p><h2>结论与展望</h2><p>我们进行了对LVLMs（大型视觉语言模型）的偏倚和挑战进行了全面的探索，特别关注它们与底层LLMs（大型语言模型）的相互作用。我们的调查揭示了LVLM生成内容中明显的偏倚，主要受到LLMs植根于语言先验而非视觉输入的影响。为了解决这些偏倚，我们引入了去偏策略，包括后处理去偏方法和去偏采样技术。我们的实验证明了这些策略在减轻幻觉和提高LVLM的推理能力方面的有效性。后处理去偏方法，如None和Unk，显著提高了模型的真实性，特别是当模型对其预测缺乏信心时。此外，去偏采样策略，标记为VDD，通过在幻觉评分上表现出竞争力的同时在推理任务中持续优于其他方法，实现了平衡。提出的策略有助于提高LVLMs的可靠性和适用性，解决与语言先验相关的偏倚。此外，我们对解码配置对LVLM性能的影响进行的探讨揭示了通过优化采样策略实现的实质性改进。优化的解码配置释放了现有LVLMs的全部潜力，超越了默认配置，并引发对现有评估公平性的担忧。随着该领域的发展，解决偏倚并完善评估方法对于在现实应用中充分发挥LVLMs的潜力至关重要。局限性和未来工作详见附录。</p><p>[1] Liang, Victor Weixin, et al. "Mind the gap: Understanding the modality gap in multi-modal contrastive representation learning." Advances in Neural Information Processing Systems 35 (2022): 17612-17625.</p><p>[2] Phuong, Mary, and Christoph H. Lampert. "Distillation-based training for multi-exit architectures." Proceedings of the IEEE/CVF international conference on computer vision. 2019.</p><p>[3] Chen, Hung-Ting, Michael JQ Zhang, and Eunsol Choi. "Rich knowledge sources bring complex knowledge conflicts: Recalibrating models to reflect conflicting evidence." arXiv preprint arXiv:2210.13701 (2022).</p><p></p>
]]></content:encoded>
<pubDate>Sun, 24 Mar 2024 13:13:48 GMT</pubDate>
</item>
<item>
<title>NeurIPS'23:从分布鲁棒优化角度来理解对比学习的鲁棒性和温度系数的意义</title>
<link>https://zhuanlan.zhihu.com/p/670251964</link>
<guid>https://zhuanlan.zhihu.com/p/670251964</guid>
<content:encoded><![CDATA[
<p></p><figure><img class="origin_image zh-lightbox-thumb" src="https://pic3.zhimg.com/v2-88c5f7ddd6edb904a45797fcb1423d66_b.jpg" width="2416" /></figure><p>TL;DR: 本文从分布式鲁棒优化（DRO）的角度分析了对比学习损失函数（InfoNCE），揭示了InfoNCE对负样本分布的鲁棒性，并指出温度系数的本质是DRO中控制鲁棒半径的拉格朗日系数，同时也建立了DRO 和互信息(MI)之间的理论联系。此外，我们从DRO的角度指出了InfoNCE的潜在缺点，例如过度保守和对异常值的敏感。最后提出了一种新的损失函数——ADNCE，并验证了其在各个场景的有效性。</p><blockquote> 论文链接：(<a class=" wrap external" href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2310.11048" rel="nofollow noreferrer" target="_blank">Understanding Contrastive Learning via Distributionally Robust Optimization</a>)<br /> Github地址：<b><a class=" wrap external" href="https://link.zhihu.com/?target=https%3A//github.com/junkangwu/ADNCE" rel="nofollow noreferrer" target="_blank">ADNCE</a></b><br /> 个人主页：<b><a class=" wrap external" href="https://link.zhihu.com/?target=https%3A//junkangwu.github.io/" rel="nofollow noreferrer" target="_blank">Junkang Wu's HomePage</a></b><br /> </blockquote><h2><b>1. 摘要</b></h2><p>近年来，对比学习因其在自监督领域卓越的表现，受到越来越多的关注。核心思想是学习“拉近”证样本（例如来自同一图像的增强数据），同时“推开”负样本（例如来自不同图像的增强数据）的表示。通过利用这种直观的概念，无监督学习甚至开始挑战监督学习。</p><p>然而，对比学习饱受困扰的一个问题就是--<b>负采样偏差</b>。由于无监督场景下的对比学习无法提前获得物品标签，负样本采样就成为了一个显著的问题。为了解决这个问题，近年来有一些工作缓解这个问题，比如，【1，2】通过估计最优负样本的分布以缓解负采样中出现的偏差，【3】则添加了一个检测模块用于识别并且修正存在的假负样本。</p><p>本工作则刷新了以往对比学习领域相关工作的认知，通过引入分布式鲁棒优化（DRO）这一理论工具，我们发现对比学习损失函数（InfoNCE）本质上是KL散度范围内，作用在负样本分布上的鲁棒优化目标（CL-DRO）。这一发现首先揭示了InfoNCE中的温度系数<img alt="\tau" src="https://www.zhihu.com/equation?tex=%5Ctau" />并非是一种启发式设计，而是控制负样本鲁棒半径的一个拉格朗日系数。同时拥有DRO这一理论框架，我们还可以对其难负样本挖掘、方差控制等性质一一提供理论上的解释。进一步，我们不局限于KL散度，分析了一般情况下（<img alt="\phi" src="https://www.zhihu.com/equation?tex=%5Cphi" />-divergence)下的DRO目标，有趣的是我们验证了任何<img alt="\phi" src="https://www.zhihu.com/equation?tex=%5Cphi" />-divergence下的CL-DRO和该<img alt="\phi" src="https://www.zhihu.com/equation?tex=%5Cphi" />-divergence对应的变分表示的等价性。这一发现严格上证明出“InfoNCE是更紧的互信息的估计”。同时，这也为任意<img alt="\phi" src="https://www.zhihu.com/equation?tex=%5Cphi" />-divergence下的互信息估计提供了新的途径。</p><p>最后，DRO的提出也揭示出InfoNCE存在的缺陷--过于保守，盲目地赋予难负样本最高的权重；以及忽略了outlier数据的影响。为改善这一现状，我们提出可调节的InfoNCE（ADNCE）用于重塑worst-case分布，通过在多个领域的尝试（CV，NLP和Graph）验证了本方法的有效性。</p><h2><b>2. 从DRO的视角理解对比学习</b></h2><h2><b>2.1 动机</b></h2><p>在对比学习（CL）的实际应用中，负样本<img alt="(x,y)" src="https://www.zhihu.com/equation?tex=%28x%2Cy%29" />通常是从训练数据中统一抽取的，它们可能具有相似的语义（例如标签）。 正如 Chuang【1】 等人所提出的，这引入了负采样偏差的潜在问题。</p><p>在本项研究中，我们观察到一个有趣的现象，即InfoNCE本身表现出对负采样偏差的抗噪性。 我们在两个基准数据集CIFAR10和STL10上对CL进行了测试，如下表所示，我们发现： 1) 通过微调温度 <img alt="\tau" src="https://www.zhihu.com/equation?tex=%5Ctau" />，传统的SimCLR 表现出显著的提升，达到了与专门设计用于解决负采样偏差的方法相当的性能水平（即 SimCLR(<img alt="\tau^*" src="https://www.zhihu.com/equation?tex=%5Ctau%5E%2A" /> ) 与 SimCLR(<img alt="\tau_0" src="https://www.zhihu.com/equation?tex=%5Ctau_0" />)、DCL【1】 和 HCL【2】)； 2）通过适当选择 <img alt="\tau" src="https://www.zhihu.com/equation?tex=%5Ctau" />，DCL和HCL实现的相对改进是微乎其微的。这些发现引导我们提出两个关键的问题：</p><ul><li>为什么 InfoNCE 表现出对负采样偏差的容忍度？</li><li><img alt="\tau" src="https://www.zhihu.com/equation?tex=%5Ctau" /> 扮演什么角色，为什么如此重要？</li></ul><figure><img class="origin_image zh-lightbox-thumb" src="https://pic2.zhimg.com/v2-d33b6c0b583b11b18e6880a9d3a1e7c1_b.jpg" width="1542" /></figure><h2><b>2.2 分布式鲁棒优化（DRO）</b></h2><p>在现有的机器学习任务中，我们默认其遵循iid假设（训练集样本分布和测试集样本分布均独立同分布）。为解决不满足iid假设的情形，DRO旨在定义一个潜在的分布<img alt="Q" src="https://www.zhihu.com/equation?tex=Q" />的集合，优化其中最糟糕的（worst-case）分布下的目标函数。</p><p><img alt="\mathcal{L}_{\text{DRO}}=\operatorname*{max}_{{Q} }  \mathbb{E}_Q [\mathcal{L}(x;\theta)] \qquad s.t. D_{\phi}({Q}||{Q}_0) \leq \eta. \\" src="https://www.zhihu.com/equation?tex=%5Cmathcal%7BL%7D_%7B%5Ctext%7BDRO%7D%7D%3D%5Coperatorname%2A%7Bmax%7D_%7B%7BQ%7D+%7D++%5Cmathbb%7BE%7D_Q+%5B%5Cmathcal%7BL%7D%28x%3B%5Ctheta%29%5D+%5Cqquad+s.t.+D_%7B%5Cphi%7D%28%7BQ%7D%7C%7C%7BQ%7D_0%29+%5Cleq+%5Ceta.+%5C%5C" /></p><p>其中潜在分布<img alt="Q" src="https://www.zhihu.com/equation?tex=Q" />是围绕在初始训练分布<img alt="Q_0" src="https://www.zhihu.com/equation?tex=Q_0" />附近，并且两者的距离<img alt="\mathcal{D}{\phi}" src="https://www.zhihu.com/equation?tex=%5Cmathcal%7BD%7D%7B%5Cphi%7D" /> 在鲁棒半径 <img alt="\eta" src="https://www.zhihu.com/equation?tex=%5Ceta" /> 内。</p><h2>2.3 CL-DRO</h2><figure><img class="origin_image zh-lightbox-thumb" src="https://pic1.zhimg.com/v2-f92c20e7241e7046402906eaa2ff1784_b.jpg" width="1964" /></figure><p>首先，我们定义一个新的训练目标--CL-DRO。CL-DRO可以理解为基本目标<img alt="L_{\text{basic}}= - \mathbb{E}_{P_X} \big[ \mathbb{E}_{P_0}[f_\theta(x,y^+)] - \mathbb{E}_{Q_0}[f_\theta(x,y)] \big ]" src="https://www.zhihu.com/equation?tex=L_%7B%5Ctext%7Bbasic%7D%7D%3D+-+%5Cmathbb%7BE%7D_%7BP_X%7D+%5Cbig%5B+%5Cmathbb%7BE%7D_%7BP_0%7D%5Bf_%5Ctheta%28x%2Cy%5E%2B%29%5D+-+%5Cmathbb%7BE%7D_%7BQ_0%7D%5Bf_%5Ctheta%28x%2Cy%29%5D+%5Cbig+%5D" />的增强版，其目的是增加正样本之间的表征相似度并减少负样本之间的相似度。 CL-DRO 通过在负样本分布上结合 DRO 来改进 <img alt="L_{\text{basic}}" src="https://www.zhihu.com/equation?tex=L_%7B%5Ctext%7Bbasic%7D%7D" />，其中 <img alt="f_\theta" src="https://www.zhihu.com/equation?tex=f_%5Ctheta" /> 优化了一系列潜在分布。 因此，CL-DRO 使模型能够适应负样本分布变化。</p><p class="ztext-empty-paragraph"><br /></p><p><b>有趣的是：当我们选取KL散度作为<img alt="\mathcal{D}{\phi}" src="https://www.zhihu.com/equation?tex=%5Cmathcal%7BD%7D%7B%5Cphi%7D" />的距离度量。CL-DRO本质上和InfoNCE目标函数等价：</b></p><figure><img class="origin_image zh-lightbox-thumb" src="https://pic2.zhimg.com/v2-fb5262f2968118b1cd80edf86a5c9825_b.jpg" width="2002" /></figure><p>定理3.2指出了InfoNCE成功的关键原因：<b>对负样本分布采取了鲁棒优化，而非固定分布下的优化</b>。其优势在于，面临负样本噪音或者真实负样本难以获取的情形，其提供了一个更加鲁棒的优化目标，从而直接降低了采样误差的影响，同时温度系数<img alt="\tau" src="https://www.zhihu.com/equation?tex=%5Ctau" />也并非一种启发式设计，而是一个拉格朗日系数，控制了鲁棒性能的关键。</p><figure><img class="origin_image zh-lightbox-thumb" src="https://pic3.zhimg.com/v2-bba552cef45718b0e07f76982ae47e82_b.jpg" width="1994" /></figure><p>定理3.3则表示出InfoNCE是理想负样本分布下损失函数的上界，其中<img alt="\mathcal{B}(\rho, N, \tau) \to 0" src="https://www.zhihu.com/equation?tex=%5Cmathcal%7BB%7D%28%5Crho%2C+N%2C+%5Ctau%29+%5Cto+0" />展现出采样样本<img alt="N" src="https://www.zhihu.com/equation?tex=N" />越多，两者差距逐渐减少（也间接指出了为何对比学习需要大量负样本的原因）；同时也为温度系数如何影响性能提供了直观解释。</p><h2><b>2.4 从DRO的视角理解温度系数<img alt="\tau" src="https://www.zhihu.com/equation?tex=%5Ctau" /></b></h2><h3><b>A. <img alt="\tau" src="https://www.zhihu.com/equation?tex=%5Ctau" />调节鲁棒半径</b></h3><figure><img class="origin_image zh-lightbox-thumb" src="https://pic3.zhimg.com/v2-58a8edfaa6d2dbd9ed540d4e54292c6e_b.jpg" width="1962" /></figure><p>推论3.4是我们通过对CL-DRO的表达式进行近似求解得到，该表达式展现了温度系数<img alt="\tau" src="https://www.zhihu.com/equation?tex=%5Ctau" />、鲁棒半径<img alt="\eta" src="https://www.zhihu.com/equation?tex=%5Ceta" />以及负样本打分方差<img alt="\mathbb{V}_{Q_0}" src="https://www.zhihu.com/equation?tex=%5Cmathbb%7BV%7D_%7BQ_0%7D" />三者之间的关系。<img alt="\tau" src="https://www.zhihu.com/equation?tex=%5Ctau" />值过大意味着鲁棒半径 <img alt="\eta" src="https://www.zhihu.com/equation?tex=%5Ceta" /> 较小，这将不满足<img alt="D_{KL}(Q^{\text{ideal}}||Q_0)\leq \eta" src="https://www.zhihu.com/equation?tex=D_%7BKL%7D%28Q%5E%7B%5Ctext%7Bideal%7D%7D%7C%7CQ_0%29%5Cleq+%5Ceta" />中的约束条件。 直观上，较大的 <img alt="\tau" src="https://www.zhihu.com/equation?tex=%5Ctau" /> 描绘了 DRO 中的受限分布集合较小，该分布集可能无法包含理想的负分布，从而导致鲁棒性下降。反之，随着 <img alt="\tau" src="https://www.zhihu.com/equation?tex=%5Ctau" /> 减小，条件 <img alt="D_{KL}(Q^{\text{ideal}}||Q_0)\leq \eta" src="https://www.zhihu.com/equation?tex=D_%7BKL%7D%28Q%5E%7B%5Ctext%7Bideal%7D%7D%7C%7CQ_0%29%5Cleq+%5Ceta" /> 可能得到满足，但它扩展了项 <img alt="\mathcal{B} (\rho, N, \tau)" src="https://www.zhihu.com/equation?tex=%5Cmathcal%7BB%7D+%28%5Crho%2C+N%2C+%5Ctau%29" /> 并放松泛化界限。 这两个因素在 <img alt="\tau" src="https://www.zhihu.com/equation?tex=%5Ctau" /> 的选择上建立了权衡。</p><h3><b>B. <img alt="\tau" src="https://www.zhihu.com/equation?tex=%5Ctau" />控制负样本的方差</b></h3><figure><img class="origin_image zh-lightbox-thumb" src="https://pic2.zhimg.com/v2-0dc626c0b9aae2a3e3dc7fa3262f9f2d_b.jpg" width="1972" /></figure><p>定理3.5同样是一个很有意思的发现：<b>给定任何<img alt="\phi" src="https://www.zhihu.com/equation?tex=%5Cphi" />-divergence，其对应的CL-DRO目标函数可以近似表达为一个mean-variance表达式，其中温度系数是控制方差项惩罚权重的超参数。</b>variance regularization常常用于损失函数上，是一种降低损失方差直接有效的策略。然而在公式8中所示，仅仅控制在负样本损失的方差是一种较优的选择，我们认为其是InfoNCE成功的关键之一。</p><h3><b>C. <img alt="\tau" src="https://www.zhihu.com/equation?tex=%5Ctau" />体现难负样本挖掘</b></h3><p>值得注意的是，我们将KL散度下CL-DRO目标函数对应的worst-case分布进行求解，可得<img alt="Q^* = Q_0  \frac{\exp[{f_\theta/\tau}]}{\mathbb{E}_{Q_0} \exp[{f_\theta/\tau}]}" src="https://www.zhihu.com/equation?tex=Q%5E%2A+%3D+Q_0++%5Cfrac%7B%5Cexp%5B%7Bf_%5Ctheta%2F%5Ctau%7D%5D%7D%7B%5Cmathbb%7BE%7D_%7BQ_0%7D+%5Cexp%5B%7Bf_%5Ctheta%2F%5Ctau%7D%5D%7D" />。<img alt="Q^*" src="https://www.zhihu.com/equation?tex=Q%5E%2A" />的表达式体现出每个负样本被赋予的权重是由<img alt="\frac{\exp[{f_\theta/\tau}]}{\mathbb{E}_{Q_0} \exp[{f_\theta/\tau}]}" src="https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cexp%5B%7Bf_%5Ctheta%2F%5Ctau%7D%5D%7D%7B%5Cmathbb%7BE%7D_%7BQ_0%7D+%5Cexp%5B%7Bf_%5Ctheta%2F%5Ctau%7D%5D%7D" />决定，即打分越高，负样本权重越高，并且<img alt="\tau" src="https://www.zhihu.com/equation?tex=%5Ctau" />越小会加剧这一权重差距。这一发现尽管和近年来很多结论一致，但我们从DRO提供了一个新的证明思路。</p><h2><b>2.5 实验验证</b></h2><p>为验证上述发现，我们一一通过实验进行验证。 首先，为验证<img alt="\tau" src="https://www.zhihu.com/equation?tex=%5Ctau" />与采样偏差之间的关联，我们利用CIFAR10上标签，调整负样本中假负样本的比例（1.0代表均匀采样，保留所有假负样本；0.0代表无任何假负样本存在，即有监督负采样）。如图1所示，随着噪音比例增加，最优温度系数也逐渐降低，即鲁棒半径逐渐增大。这一发现符合推论3.4的结论。 </p><figure><img class="origin_image zh-lightbox-thumb" src="https://pic1.zhimg.com/v2-d68872c81c1086c42d02de3d5eb42c68_b.jpg" width="2238" /></figure><p> 进一步，我们测试不同温度系数下，负样本打分的方差与正样本预测分数的均值的变化。如图2所示，温度系数越小，负样本的方差也逐渐降低，即对应公式8中对负样本方差的惩罚逐渐增大。与之对应的正样本均值也逐渐降低。这一发现仅凭现有对比学习中的理论研究很难将其解释清楚，但有了DRO这一理论工具，上述现象都变得直观且可解释了。</p><figure><img class="origin_image zh-lightbox-thumb" src="https://pic1.zhimg.com/v2-3c49047993491db1e8cb11dd5d288bfc_b.jpg" width="1276" /></figure><p>最后，我们利用公式8的目标函数在CIFAR10与STL10上进行测试，如表2所示，简单的mean-variance损失函数形式即可实现和InfoNCE比肩的性能，这一实验现象同样验证了定理3.5的正确性。</p><h2><b>3. DRO、InfoNCE与互信息MI之间的关联</b></h2><p>在对比学习（InfoNCE）理论研究中，往往会从互信息的角度进行思考，即InfoNCE的本质是通过最大化正样本之间的互信息从而实现获得较好的表征。而DRO又拓宽了InfoNCE的理论解释途径，这不禁让我们思考DRO，InfoNCE以及互信息三者之间的关联。</p><figure><img class="origin_image zh-lightbox-thumb" src="https://pic2.zhimg.com/v2-f9f6818d110718b88dd40e51598308fd_b.jpg" width="2244" /></figure><p>公式9是<img alt="\phi" src="https://www.zhihu.com/equation?tex=%5Cphi" />-divergence下的互信息表示形式。结合CL-DRO表达式，我们发现：</p><figure><img class="origin_image zh-lightbox-thumb" src="https://pic4.zhimg.com/v2-cb1ab602636b4295c7b5322000acb0e3_b.jpg" width="2260" /></figure><p> 定理4.2指出了最大化CL-DRO训练目标本质上是对互信息的的估计。这给出我们以下发现：</p><h2><b>3.1 InfoNCE是一种更紧的互信息估计</b></h2><p><img alt="\phi" src="https://www.zhihu.com/equation?tex=%5Cphi" />-散度的现有常见变分近似是 Donsker-Varadhan 目标 (<img alt="I_{DV}" src="https://www.zhihu.com/equation?tex=I_%7BDV%7D" />) : <img alt="D_{\phi}(P||Q) := \operatorname*{sup}_{ {f}\in \mathcal{F}} \{ \mathbb{E}_P [f ] - \mathbb{E}_Q [\phi^*(f)] \}" src="https://www.zhihu.com/equation?tex=D_%7B%5Cphi%7D%28P%7C%7CQ%29+%3A%3D+%5Coperatorname%2A%7Bsup%7D_%7B+%7Bf%7D%5Cin+%5Cmathcal%7BF%7D%7D+%5C%7B+%5Cmathbb%7BE%7D_P+%5Bf+%5D+-+%5Cmathbb%7BE%7D_Q+%5B%5Cphi%5E%2A%28f%29%5D+%5C%7D" />它适用于 <img alt="\mathcal{F}" src="https://www.zhihu.com/equation?tex=%5Cmathcal%7BF%7D" /> 中的任意有限测度。但有研究【4】指出，该表达式没有考虑到<img alt="\mathbb{E}_P[1]=1" src="https://www.zhihu.com/equation?tex=%5Cmathbb%7BE%7D_P%5B1%5D%3D1" />，即定义在概率分布上这一事实。 因此，我们就有更严格的表示： <img alt="D_{\phi}(P| |Q) := \operatorname*{sup}_{{f}\in \mathcal{F}} \mathbb{E}_{P} [f] - \operatorname*{min}_{\lambda \in \mathbb{R}} \{ \lambda + \mathbb{E}_{Q} [\phi^*(f-\lambda)] \}" src="https://www.zhihu.com/equation?tex=D_%7B%5Cphi%7D%28P%7C+%7CQ%29+%3A%3D+%5Coperatorname%2A%7Bsup%7D_%7B%7Bf%7D%5Cin+%5Cmathcal%7BF%7D%7D+%5Cmathbb%7BE%7D_%7BP%7D+%5Bf%5D+-+%5Coperatorname%2A%7Bmin%7D_%7B%5Clambda+%5Cin+%5Cmathbb%7BR%7D%7D+%5C%7B+%5Clambda+%2B+%5Cmathbb%7BE%7D_%7BQ%7D+%5B%5Cphi%5E%2A%28f-%5Clambda%29%5D+%5C%7D" />。这一更紧的表示形式在定理4.2中恰好作为DRO和MI等价性的关键桥梁。</p><h2><b>3.2 DRO bridges the gap between MI and InfoNCE.</b></h2><p>现有工作已经证明InfoNCE是互信息估计的下界，但是他们的证明仍然存在缺陷。如MINE【5】使用<img alt="\mathcal{I}_{DV}" src="https://www.zhihu.com/equation?tex=%5Cmathcal%7BI%7D_%7BDV%7D" />推导出的结果既不是MI的上界也不是MI的下界。而CPC【6】的推导过程又存在多次冗余的估计。上述两点问题在【7】中也有具体说明。而本文提出的DRO则回避了上述问题，直接建立起两者之间的理论桥梁。</p><h2><b>3.3 DRO 提供一般情形下MI的估计。</b></h2><p>现有互信息的估计往往是在KL散度下的，而定理4.2则给出了一种<img alt="\phi" src="https://www.zhihu.com/equation?tex=%5Cphi" />-MI的估计形式，例如，如果我们考虑 <img alt="\chi^2" src="https://www.zhihu.com/equation?tex=%5Cchi%5E2" /> 散度的情况，由 <img alt="\phi(x)=\frac{1}{2\sqrt{2}}(x-1)^2" src="https://www.zhihu.com/equation?tex=%5Cphi%28x%29%3D%5Cfrac%7B1%7D%7B2%5Csqrt%7B2%7D%7D%28x-1%29%5E2" /> 给出，我们可以 得到凸共轭<img alt="\phi^*(x)=x+x^2" src="https://www.zhihu.com/equation?tex=%5Cphi%5E%2A%28x%29%3Dx%2Bx%5E2" />。 变分表示变为 <img alt="I_{\tt{\chi^2}}(X;Y)=D_{\tt{\chi^2}}(P_0\,\|\,Q_0) = \max_{f}~ \{\mathbb{E}_{P_0}[f] - \mathbb{E}_{Q_0}[f]-\mathbb{V}_{Q_0}[f] \}" src="https://www.zhihu.com/equation?tex=I_%7B%5Ctt%7B%5Cchi%5E2%7D%7D%28X%3BY%29%3DD_%7B%5Ctt%7B%5Cchi%5E2%7D%7D%28P_0%5C%2C%5C%7C%5C%2CQ_0%29+%3D+%5Cmax_%7Bf%7D~+%5C%7B%5Cmathbb%7BE%7D_%7BP_0%7D%5Bf%5D+-+%5Cmathbb%7BE%7D_%7BQ_0%7D%5Bf%5D-%5Cmathbb%7BV%7D_%7BQ_0%7D%5Bf%5D+%5C%7D" />，其中 <img alt="\mathbb{V }_{Q_0}[f]" src="https://www.zhihu.com/equation?tex=%5Cmathbb%7BV+%7D_%7BQ_0%7D%5Bf%5D" /> 表示 <img alt="f" src="https://www.zhihu.com/equation?tex=f" /> 在分布 <img alt="Q" src="https://www.zhihu.com/equation?tex=Q" /> 上的方差。 我们的理论框架提供了估计灵活的 <img alt="\phi" src="https://www.zhihu.com/equation?tex=%5Cphi" />-MI 的机会，它可以适应特定的场景。</p><h2><b>4. 方法</b></h2><h2><b>4.1 InfoNCE仍有不足</b></h2><ul><li><b>过于保守</b>：DRO的最糟糕分布情形下，负样本权重是按照 <img alt="\exp[{f_\theta/\tau}]" src="https://www.zhihu.com/equation?tex=%5Cexp%5B%7Bf_%5Ctheta%2F%5Ctau%7D%5D" />分配，这表示最高相似度的负样本将获得最高权重。然而有研究表示，最具信息量的负样本往往是头部区域并非是头部尖端样本。因此我们认为盲目的按照相似度打分赋权不是一个最优选择。</li><li><b>对异常点非常敏感</b>：DRO本身存在的一个问题就是对异常点非常敏感【8】。由于其仅关注最头部样本，这将导致头部样本中的噪音将无限度地被放大，从而严重影响模型收敛。</li></ul><h2><b>4.2 ADNCE</b></h2><figure><img class="origin_image zh-lightbox-thumb" src="https://pic2.zhimg.com/v2-69f6bf6280f30a192fa91fe6c2b0fc55_b.jpg" width="1728" /></figure><p>我们的目标就是改善4.1中提出的问题，即负样本权重分配至“更合理的区域”，而不是固定的仅关注最头部的样本。为此，我们引入<img alt="w(f,\mu,\sigma)" src="https://www.zhihu.com/equation?tex=w%28f%2C%5Cmu%2C%5Csigma%29" />提出一个简单的re-weight策略：</p><p><img alt=" w(f_\theta(x,y),\mu,\sigma)\propto \frac{1}{\sigma\sqrt{2\pi}}\exp[{-\frac{1}{2}(\frac{f_\theta(x,y)- \mu}{\sigma})^2}] \\" src="https://www.zhihu.com/equation?tex=+w%28f_%5Ctheta%28x%2Cy%29%2C%5Cmu%2C%5Csigma%29%5Cpropto+%5Cfrac%7B1%7D%7B%5Csigma%5Csqrt%7B2%5Cpi%7D%7D%5Cexp%5B%7B-%5Cfrac%7B1%7D%7B2%7D%28%5Cfrac%7Bf_%5Ctheta%28x%2Cy%29-+%5Cmu%7D%7B%5Csigma%7D%29%5E2%7D%5D+%5C%5C" /></p><p>其中 <img alt="\mu" src="https://www.zhihu.com/equation?tex=%5Cmu" /> 和 <img alt="\sigma" src="https://www.zhihu.com/equation?tex=%5Csigma" /> 是我们可以控制的两个超参数。如图3所示，<img alt="\mu" src="https://www.zhihu.com/equation?tex=%5Cmu" />控制权重分配的中心区域，越接近<img alt="\mu" src="https://www.zhihu.com/equation?tex=%5Cmu" />的样本权重越大，而<img alt="\sigma" src="https://www.zhihu.com/equation?tex=%5Csigma" />控制权重分配的差异化（高度）。 直观上，<img alt="\sigma" src="https://www.zhihu.com/equation?tex=%5Csigma" /> 越小，样本之间的权重差异就越明显。将<img alt="w(f,\mu,\sigma)" src="https://www.zhihu.com/equation?tex=w%28f%2C%5Cmu%2C%5Csigma%29" />纳入训练目标可得：</p><p><img alt=" \mathcal{L}_{\text{ADNCE}} = -\mathbb{E}_{P} [f_\theta(x,y^+)/\tau] + \log \mathbb{E}_{Q_0} [ w(f_\theta(x,y),\mu,\sigma) e^{f_\theta(x,y)/\tau}/ Z_{\mu, \sigma}] \\" src="https://www.zhihu.com/equation?tex=+%5Cmathcal%7BL%7D_%7B%5Ctext%7BADNCE%7D%7D+%3D+-%5Cmathbb%7BE%7D_%7BP%7D+%5Bf_%5Ctheta%28x%2Cy%5E%2B%29%2F%5Ctau%5D+%2B+%5Clog+%5Cmathbb%7BE%7D_%7BQ_0%7D+%5B+w%28f_%5Ctheta%28x%2Cy%29%2C%5Cmu%2C%5Csigma%29+e%5E%7Bf_%5Ctheta%28x%2Cy%29%2F%5Ctau%7D%2F+Z_%7B%5Cmu%2C+%5Csigma%7D%5D+%5C%5C" /></p><h2><b>5 实验</b></h2><p>为验证ADNCE的有效性，我们在三个最为常见对比学习场景进行实验，分布是image、sentence以及graph。具体实验设定请参见原文。</p><h3><b>5.1 Image</b></h3><p class="ztext-empty-paragraph"><br /></p><figure><img class="origin_image zh-lightbox-thumb" src="https://pic1.zhimg.com/v2-b4a71dd7b334683f348666c66bd524e8_b.jpg" width="2262" /></figure><p> 如 Tab.3 所示，<img alt="\tau" src="https://www.zhihu.com/equation?tex=%5Ctau" /> 的网格搜索对模型性能具有至关重要的影响。 从早期阶段（100个epoch）到后期（400个epoch）,这种影响在整个训练过程中都是显著的。此外，ADNCE 表现出持续优于其他方案，特别提高了训练早期阶段的表现。 相比之下，虽然 <img alt="\alpha" src="https://www.zhihu.com/equation?tex=%5Calpha" />-CL-direct 引入了一种新的方法来设置 <img alt="\tau" src="https://www.zhihu.com/equation?tex=%5Ctau" /> 的值，但其本质仍然偏重于最困难的负样本，因此与微调 <img alt="\tau" src="https://www.zhihu.com/equation?tex=%5Ctau" /> 相比，产生了与使用网格搜索相似的性能。同时在Fig.4中，我们绘制了训练曲线以进一步说明ADNCE的稳定优越性。</p><h3><b>5.2 Sentence</b></h3><figure><img class="origin_image zh-lightbox-thumb" src="https://pic1.zhimg.com/v2-6aad16d56276ea56157bdf7942c542d0_b.jpg" width="2262" /></figure><p> 正如表4中所观察到的,ADNCE 始终优于 InfoNCE，平均 Spearman 相关性达到 77%。其中，用 ADNCE 替换 InfoNCE 的简便性以及在 BERT 和 RoBERTa 中观察到的显着性能改进证明了 ADNCE 的有效性和广泛适用性。此外，<img alt="\tau^*" src="https://www.zhihu.com/equation?tex=%5Ctau%5E%2A" /> 相对于 <img alt="\tau_0" src="https://www.zhihu.com/equation?tex=%5Ctau_0" /> 的改进强调了选择适当鲁棒性半径的重要性。</p><h3><b>5.3 Graph</b></h3><figure><img class="origin_image zh-lightbox-thumb" src="https://pic3.zhimg.com/v2-6428cb36c2647ab0a5cd94cada6363f6_b.jpg" width="2234" /></figure><p> 表5显示，ADNCE 在四个数据集上均优于所有基线，尤其是与三种最先进的基于 InfoNCE 的对比方法（GraphCL、JOAO 和 JOAOv2）相比，从而在四个数据集上都创下了新记录。而在 GraphCL(<img alt="\tau^{*}" src="https://www.zhihu.com/equation?tex=%5Ctau%5E%7B%2A%7D" />) 中观察到的相对于 GraphCL(<img alt="\tau_0" src="https://www.zhihu.com/equation?tex=%5Ctau_0" />) 的改进与我们对 DRO 的理解一致。</p><h2><b>总结</b></h2><p>我们通过分布鲁棒优化 (DRO) 的视角提供了对比学习 (CL) 的新颖视角，并揭示了关于采样偏差容忍度、<img alt="\tau" src="https://www.zhihu.com/equation?tex=%5Ctau" /> 的作用以及 DRO 和 MI 之间的理论联系的几个关键见解。理论分析和实证实验都证实了上述发现。 此外，我们从DRO的角度指出了CL的潜在缺点，例如过度保守和对异常值的敏感。为了解决这些问题，我们提出了一种新颖的 CL 损失——ADNCE，并验证了其在各个领域的有效性。</p><p>这项工作的局限性主要源于两个方面：1）我们的DRO框架只为基于InfoNCE的方法提供了理论解释，对于没有负样本的CL的仍然存在理论上的差距； 2）ADNCE需要通过参数调整权重分配，无法自适应学习最佳的重加权方案。</p><h2><b>参考文献</b></h2><p>【1】Ching-Yao Chuang, Joshua Robinson, Yen-Chen Lin, Antonio Torralba, Stefanie Jegelka. Debiased Contrastive Learning. NeurIPS 2020</p><p>【2】Joshua David Robinson, Ching-Yao Chuang, Suvrit Sra, Stefanie Jegelka. Contrastive Learning with Hard Negative Samples. ICLR 2021</p><p>【3】Tsai-Shien Chen, Wei-Chih Hung, Hung-Yu Tseng, Shao-Yi Chien, Ming-Hsuan Yang. Incremental False Negative Detection for Contrastive Learning. ICLR 2022</p><p>【4】Avraham Ruderman, Mark D. Reid, Dario García-García, James Petterson. Tighter Variational Representations of f-Divergences via Restriction to Probability Measures. ICML 2012</p><p>【5】Mohamed Ishmael Belghazi, Aristide Baratin, Sai Rajeswar, Sherjil Ozair, Yoshua Bengio, R. Devon Hjelm, Aaron C. Courville. Mutual Information Neural Estimation. ICML 2018: 530-539</p><p>【6】Aäron van den Oord, Yazhe Li, Oriol Vinyals. Representation Learning with Contrastive Predictive Coding. CoRR abs/1807.03748 (2018)</p><p>【7】Ben Poole, Sherjil Ozair, Aäron van den Oord, Alexander A. Alemi, George Tucker. On Variational Bounds of Mutual Information. ICML 2019: 5171-5180</p><p>【8】Runtian Zhai, Chen Dan, J. Zico Kolter, Pradeep Ravikumar. DORO: Distributional and Outlier Robust Optimization. ICML 2021: 12345-12355</p>
]]></content:encoded>
<pubDate>Mon, 04 Dec 2023 03:34:43 GMT</pubDate>
</item>
<item>
<title>无需偏好数据也能RLHF？腾讯AI Lab提出对抗式偏好对齐方法</title>
<link>https://zhuanlan.zhihu.com/p/669939433</link>
<guid>https://zhuanlan.zhihu.com/p/669939433</guid>
<content:encoded><![CDATA[
<p>人类偏好对齐是大模型训练的重要阶段，通过偏好对齐可以进一步提升大模型的人机交互体验。目前被广泛使用的偏好对齐方法是 RLHF（reinforcement learning from human feedback）。然而 RLHF 有着计算复杂度高、实现复杂、训练不稳定等问题，为此最近一些方法（RRHF, DPO, Rejection Sampling 等）也在试图回避 RL 的训练范式进行偏好对齐。但即使对 RLHF 进行了化简，目前的偏好对齐方法仍然有一个麻烦的问题无法回避，就是生成样本的<b>分布偏移</b>问题：当大模型经过一段时间迭代后，其生成回复的文本分布会产生偏移，然而新分布下的样本并未进行过偏好标注，导致对齐算法的效率大幅下降。</p><figure><img class="origin_image zh-lightbox-thumb" src="https://pic1.zhimg.com/v2-2035836b52f12816b0fa08fae4fc64d4_b.jpg" width="1080" /><figcaption>▲ 样本分布偏移：偏好标注（红色虚线）原本可以覆盖模型的输出分布（左图蓝色曲线），但当模型更新后，样本输出分布（右图绿色曲线）与偏好标注范围产生不一致。</figcaption></figure><p>对于分布偏移问题，目前普遍采用的解决方法是在模型迭代一定步数后，让模型重新生成回复样本，并在新样本上重新进行人工标注。这样的方式耗时耗力，严重影响了偏好对齐算法的效率。为了更加高效地解决样本分布偏移问题，本篇文章提出了 Adversarial Preference Optimization（APO）方法，巧妙地让偏好奖励模型（Reward Model, RM）和大模型（LLM）进行对抗训练，使得 RM 可以自动适应 LLM 的分布变化，以此达到减少偏好标注数据量并提高对齐算法效率的效果。文章在 Helpful&amp;Harmless 数据集上进行了效果验证。实验结果表明，通过对抗方式进行大模型对齐，可以与现有对齐方法相结合，并在不增加偏好数据量的条件下，进一步提升 RM 和 LLM 的效果。</p><figure><img class="origin_image zh-lightbox-thumb" src="https://pic2.zhimg.com/v2-8aa5ad15d6fe9a630f2c70391db3c85d_b.jpg" width="1080" /></figure><p><b>论文题目：</b></p><p>Adversarial Preference Optimization</p><p><b>论文链接：</b></p><p><a class=" external" href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2311.08045" rel="nofollow noreferrer" target="_blank"><span class="invisible">https://</span><span class="visible">arxiv.org/abs/2311.0804</span><span class="invisible">5</span><span class="ellipsis"></span></a></p><p><b>代码链接：</b></p><p><a class=" external" href="https://link.zhihu.com/?target=https%3A//github.com/Linear95/APO" rel="nofollow noreferrer" target="_blank"><span class="invisible">https://</span><span class="visible">github.com/Linear95/APO</span><span class="invisible"></span></a></p><h2><b>1. 对抗式对齐方法</b></h2><p>人类偏好对齐方法目标是去优化LLM输出策略 <img alt="\pi\theta(y|x)" src="https://www.zhihu.com/equation?tex=%5Cpi%5Ctheta%28y%7Cx%29" /><i> 在RM模型 </i><img alt="r\phi(x,y)" src="https://www.zhihu.com/equation?tex=r%5Cphi%28x%2Cy%29" /> 评价下的期望得分： </p><figure><img class="origin_image zh-lightbox-thumb" src="https://pic2.zhimg.com/v2-a2887c2b6264d47368015e976cbf5c91_b.jpg" width="900" /></figure><p>本文借助少量人工金标准数据（golden responses），将偏好对齐的目标转变成一个min-max博弈： </p><figure><img class="origin_image zh-lightbox-thumb" src="https://pic4.zhimg.com/v2-4874df79ec2cb7137bf44aa24c59b043_b.jpg" width="1000" /></figure><p>以上的博弈过程中，LLM模型 <img alt="\pi\theta(y|x)" src="https://www.zhihu.com/equation?tex=%5Cpi%5Ctheta%28y%7Cx%29" /><i> 需要不断提高回复质量以减少其得分和金标数据得分之间的差距，而RM模型 </i><img alt="r\phi(x,y)" src="https://www.zhihu.com/equation?tex=r%5Cphi%28x%2Cy%29" /> 需要不断将模型生产的结果和金标准结果区分开。通过这种对抗的训练方式，RM会时刻跟随LLM的变化而迭代，分布偏移的问题就此得到缓解。</p><figure><img class="origin_image zh-lightbox-thumb" src="https://pic1.zhimg.com/v2-4c8369f6f444d14c122fbcf9702733e0_b.jpg" width="1080" /><figcaption>▲ APO 的 RM 和 LLM 交替训练流程具体的对抗算法实现如上图所示。</figcaption></figure><p class="ztext-empty-paragraph"><br /></p><p>RM 和 LLM 将交替进行迭代：</p><ul><li>在 LLM 迭代步骤中，RM 参数固定，博弈目标转换为正常的偏好对齐问题，可以使用 RLHF、RRHF、DPO、Rejection Sampling 等方法求解。流程上，作者将 LLM 训练用的 queries 经过 LLM 推理出对应的回复样本，再用 RM 模型进行打分，最后用打分反馈来更新 LLM 的回复策略。</li><li>在 RM 迭代步骤中，LLM 参数固定，作者将 RM 训练的 queries 经过 LLM 推理得到对应的样本回复。然后将生成的样本回复和金标准回复进行组合，得到新的 APO 偏好数据，用来更新 RM 模型。</li></ul><p>此外，文章还在博弈过程中引入了 KL 散度作为正则项，以缓解对抗训练过拟合和收敛困难的问题。同时，文章还讨论了 APO 方法与 GAN 等经典对抗训练方法之间的联系和区别。</p><h2><b>2. 实验结果</b></h2><p>为验证 APO 训练框架有效性，作者在 Helpful&amp;Harmless 偏好数据集上进行了偏好对齐实验，并对该数据集中的问题调用 GPT-4 获得回答作为金标准回复。文章选择了 rejection sampling 方法作为基线进行比较，RM 模型选择 LLaMA-7B，LLM 的 SFT 模型选择 Alpaca-7B，并进行了三轮对抗迭代。在对抗迭代中，作者尝试了两种 RM 的更新方式：</p><ul><li>From Base：也就是每次 RM 都是基于 base model 利用新的 APO 对抗数据进行更新，</li><li>Sequential：每一轮 RM 都是在上一轮 RM 的 checkpoint 上继续用新的 APO 对抗数据进行训练。</li></ul><p>关于 RM 和 LLM 表现的实验结果总结在下面的图表当中：</p><figure><img class="origin_image zh-lightbox-thumb" src="https://pic1.zhimg.com/v2-745275b1cff7038e1dc92a6b8c650568_b.jpg" width="1080" /></figure><ul><li>左侧的图表展示了 APO 训练后 RM 的表现变化，纵轴为 RM 在测试集上的准确率，横轴为模型的校准误差（Calibration Error）。可以看到，通过加入 APO训练数据，RM 模型的偏好准确率可以一致地获得提升，尤其是通过 Sequential 的方式更新 RM（APO-v1  APO-v2seq  APO-v3seq）可以将 RM 的准确率持续提升。但于此同时 RM 校准表现会有一定的损失。</li><li>右侧图表展示了 APO 训练对 LLM 对齐效果的增益，纵轴为测试 RM 对 LLM 回复样本的平均打分，代表 LLM 的对齐质量，横轴为对抗迭代的轮数。可以看到，用 From Base 训练的 RM 进行 APO 对抗的结果（红色虚线）可以持续地跟 rejection sampling（RJS）基线（蓝色虚线，仅用 Base RM 进行对齐）拉开差距。而 Sequential 训练的 RM 可以在第二轮获得更好的效果，但是在第三轮时效果反而下降，作者分析原因可能是因为 overfit 导致 RM 的校准误差过大。</li></ul><p>此外作者还利用 GPT-4 对 LLM 在测试集上的回复进行了评价。如下图所示，通过 APO 训练的模型可以相比于 RJS 基线获得显著提升。</p><figure><img class="origin_image zh-lightbox-thumb" src="https://pic1.zhimg.com/v2-dc5bcb4d2175d92aea6c31cb2d24cc5c_b.jpg" width="1080" /></figure><h2><b>3. 总结</b></h2><p>本文提出了一种新的人类偏好对齐训练范式 APO，通过 RM 和 LLM 进行对抗的方式，可以在不增加标注数据量的前提下进一步增强 RM 和 LLM 的表现，同时缓解 LLM 的样本分布偏移问题。作为一种通用的训练框架，APO 可以跟现有的对齐方法（RLHF，RRHF，DPO等）做到兼容，可以进一步提升这些对齐方法的效果。通过对抗的方式，RM 可以对 LLM 的迭代做到自适应，从而降低了重新对 LLM 样本进行偏好标注的需求，可以降低标注成本，提升大模型对齐的效率。作者也提到，目前对 APO 的探索还在相对初步的阶段，后续还将持续扩充实验，探索如何降低 RM 在对抗过程中的校准误差，如何保证对抗过程不会 overfit，以及将 APO 与其他对齐方法结合的效果表现。</p><hr /><p><b>#投 稿 通 道#</b></p><p>如何才能让更多的优质内容以更短路径到达读者群体，缩短读者寻找优质内容的成本呢？ <b>答案就是：你不认识的人。</b></p><p>总有一些你不认识的人，知道你想知道的东西。PaperWeekly 或许可以成为一座桥梁，促使不同背景、不同方向的学者和学术灵感相互碰撞，迸发出更多的可能性。</p><p>PaperWeekly 鼓励高校实验室或个人，在我们的平台上分享各类优质内容，可以是<b>最新论文解读</b>，也可以是<b>学术热点剖析</b>、<b>科研心得</b>或<b>竞赛经验讲解</b>等。我们的目的只有一个，让知识真正流动起来。</p><p><b>来稿标准：</b></p><p>• 文章确系个人<b>原创作品</b>，未曾在公开渠道发表，如为其他平台已发表或待发表的文章，请明确标注</p><p>• 稿件建议以 <b>markdown</b> 格式撰写，文中配图以附件形式发送，要求图片清晰，无版权问题</p><p>• PaperWeekly 尊重原作者署名权，并将为每篇被采纳的原创首发稿件，提供<b>业内具有竞争力稿酬</b>，具体依据文章阅读量和文章质量阶梯制结算</p><p><b>投稿方式：</b></p><p>• 方法一：在PaperWeekly知乎专栏页面点击“投稿”，即可递交文章</p><p>• 方法二：发送邮件至：hr@paperweekly.site ，所有文章配图，请单独在附件中发送</p><p>• 来稿请备注即时联系方式（微信），以便我们在稿件选用的第一时间联系作者</p><p>• 您也可以直接添加小编微信（<b>pwbot02</b>）快速投稿，备注：姓名-投稿</p><h2><b>关于PaperWeekly</b></h2><p>PaperWeekly 是一个推荐、解读、讨论、报道人工智能前沿论文成果的学术平台。如果你研究或从事 AI 领域，欢迎在公众号后台点击<b>「交流群」</b>，小助手将把你带入 PaperWeekly 的交流群里。</p><p><b>加入社区：</b><a class=" wrap external" href="https://link.zhihu.com/?target=http%3A//paperweek.ly/" rel="nofollow noreferrer" target="_blank">http://paperweek.ly</a></p><p><b>微信公众号：PaperWeekly</b></p><p><b>新浪微博：@PaperWeekly</b></p>
]]></content:encoded>
<pubDate>Fri, 01 Dec 2023 12:46:52 GMT</pubDate>
</item>
<item>
<title>LLaMA2+RLHF=脆皮大模型？ICLR 2024 高分投稿：多样性驱动的红蓝对抗</title>
<link>https://zhuanlan.zhihu.com/p/669675072</link>
<guid>https://zhuanlan.zhihu.com/p/669675072</guid>
<content:encoded><![CDATA[
<h2>导言：速览8888高分的红蓝对抗投稿</h2><p>红蓝对抗（Red Teaming，即设计让大模型输出不适当回答的 test cases，也可以叫对抗样本）是检测大模型安全性的重要手段，之前利用RL目标训练的语言模型（Red Teaming LM，红队模型）自动生成对抗样本忽视了生成的多样性目标，往往只会生成重复的几个有效对抗样本。本文针对这一问题，在红队模型的训练中引入了熵回报（entropy bonus）和新颖性奖励（novelty reward）两个正则项，显式地鼓励红队模型生成多样的对抗样本，来诱导大模型犯错。实验显示新设计的训练目标显著提升了针对一系列大模型的对抗样本的多样性和有效性，提高了自动红蓝对抗工程的效率。</p><figure><img class="origin_image zh-lightbox-thumb" src="https://pic4.zhimg.com/v2-67664c47c0aa1ce99ede533b90e7087f_b.jpg" width="2084" /><figcaption>本文方法生成的red teaming prompts和 LLaMa-2-7b-chat-hf对其产生的不当回复。右侧的百分比分数为有害文本检测模型给出的有害概率。</figcaption></figure><p>该论文<a class=" wrap external" href="https://link.zhihu.com/?target=https%3A//openreview.net/forum%3Fid%3D4KqkizXgXU" rel="nofollow noreferrer" target="_blank">Curiosity-driven Red-teaming for Large Language Models</a>正在 ICLR 2024审稿，获得了四个审稿人一致的 8888 好评。感觉此文没有太多的理论推导，胜在清晰的motivation 和扎实的实验，其中对经过RLHF对齐的 LLaMa-2-7b-chat-hf 也能产生多样的对抗样本（如上图所示），诱导模型回答出“你不配当医生”、”中餐厨师没有个人生活“这样的不当内容。</p><h2>症结：训练目标导致Red Teaming样本缺乏多样性</h2><p>假设目标 LLM（红队试图攻击的模型）的语言模型概率分布为 <img alt="p" src="https://www.zhihu.com/equation?tex=p" />，输入prompt为 <img alt="x" src="https://www.zhihu.com/equation?tex=x" /> ， <img alt="y" src="https://www.zhihu.com/equation?tex=y" /> 为其生成的回复 <img alt="y \sim p(. \mid x)" src="https://www.zhihu.com/equation?tex=y+%5Csim+p%28.+%5Cmid+x%29" /> 。红队的目标是生成让目标 LLM 生成有害的 <img alt="y" src="https://www.zhihu.com/equation?tex=y" /> 的 <img alt="x" src="https://www.zhihu.com/equation?tex=x" /> ，有害程度的定量指标为 <img alt="R(y)" src="https://www.zhihu.com/equation?tex=R%28y%29" /> （越高越有害），本文实现中为有害文本分类数据集上训练的 roberta 分类器输出的概率。设用于生成 <img alt="x" src="https://www.zhihu.com/equation?tex=x" /> 的红队模型参数为 <img alt="\pi" src="https://www.zhihu.com/equation?tex=%5Cpi" /> ，则红蓝对抗的最终目标为最大化 <img alt="\mathbb{E}_{x \sim \pi, y \sim p(. \mid x)}[R(y)]" src="https://www.zhihu.com/equation?tex=%5Cmathbb%7BE%7D_%7Bx+%5Csim+%5Cpi%2C+y+%5Csim+p%28.+%5Cmid+x%29%7D%5BR%28y%29%5D" /> 。设输入给红队模型的 prompt 为 <img alt="z" src="https://www.zhihu.com/equation?tex=z" /> ， <img alt="z" src="https://www.zhihu.com/equation?tex=z" /> 从数据集 <img alt="\mathcal{D}" src="https://www.zhihu.com/equation?tex=%5Cmathcal%7BD%7D" /> 中采样，与PPO-ptx加上一个对原模型输出分布的 KL 散度约束防止训练崩坏）类似，之前训练 read teaming LM 的目标[1]通常为：</p><p><img alt="\max _\pi \mathbb{E}\left[R(y)-\beta D_{K L}\left(\pi(. \mid z) \| \pi_{\text {ref }}(. \mid z)\right)\right], \text { where } z \sim \mathcal{D}, x \sim \pi(. \mid z), y \sim p(. \mid x)" src="https://www.zhihu.com/equation?tex=%5Cmax+_%5Cpi+%5Cmathbb%7BE%7D%5Cleft%5BR%28y%29-%5Cbeta+D_%7BK+L%7D%5Cleft%28%5Cpi%28.+%5Cmid+z%29+%5C%7C+%5Cpi_%7B%5Ctext+%7Bref+%7D%7D%28.+%5Cmid+z%29%5Cright%29%5Cright%5D%2C+%5Ctext+%7B+where+%7D+z+%5Csim+%5Cmathcal%7BD%7D%2C+x+%5Csim+%5Cpi%28.+%5Cmid+z%29%2C+y+%5Csim+p%28.+%5Cmid+x%29" /> </p><p>其中 <img alt="\beta" src="https://www.zhihu.com/equation?tex=%5Cbeta" /> 为 KL 散毒约束项的权重超参。</p><p>这样训练出来red teaming LM 确实可以产生使 <img alt="R(y)" src="https://www.zhihu.com/equation?tex=R%28y%29" /> 值很大、大模型回复有害的对抗 prompt <img alt="x" src="https://www.zhihu.com/equation?tex=x" /> ，但会趋向于生成几个类似的重复样本，存在两个缺陷：</p><ol><li><b>只注重有效性，不鼓励多样性：</b>目标中未考虑多样性，只能生成极为有限的对抗 prompts，无法起到红队应有的全面评估大模型安全风险的作用；</li><li><b>重利用，缺乏探索：</b>从强化学习的exploration-utiliztion trade-off 这一视角来看，这一训练策略一旦达到某个可以产生一个有效的 <img alt="x" src="https://www.zhihu.com/equation?tex=x" /> 使得奖励 <img alt="R(y)" src="https://www.zhihu.com/equation?tex=R%28y%29" /> 较大的step，就会不断加强该 <img alt="x" src="https://www.zhihu.com/equation?tex=x" /> 的生成概率，使得策略参数一直在这附近优化，偏重利用而不是探索，无法发现新的、可能更有效的策略。</li></ol><h2>解方：多样性驱动的综合训练目标</h2><p>相应地，为了解决以上这两个导致自动生成的对抗样本非常单调的症结，作者给出了两个优化目标中的正则项作为解方：</p><ol><li><b>熵回报entropy bonus：</b>鼓励red teaming输出分布的熵增大，增强多样性（采用生成时的随机性），避免陷入只能生成一种对抗 prompt的状态；</li><li><b>新颖性奖励 novelty reward：</b>显式地鼓励当前生成的对抗prompt 和历史生成结果的差异（如self-BLEU 分数或者句子embedding距离）</li></ol><p>最后red teaming LM（参数为 <img alt="\pi" src="https://www.zhihu.com/equation?tex=%5Cpi" /> ）总的训练目标为：</p><p><img alt="\max _\pi \mathbb{E}[R(y)-\beta D_{K L}\left(\pi(. \mid z) \| \pi_{\text {ref }}(. \mid z)\right)-\underbrace{\lambda_E \log (\pi(x \mid z))}_{\text{Entropy bonus }}+\sum_i \underbrace{\lambda_i B_i(x)}_{\text {Novelty reward }}]," src="https://www.zhihu.com/equation?tex=%5Cmax+_%5Cpi+%5Cmathbb%7BE%7D%5BR%28y%29-%5Cbeta+D_%7BK+L%7D%5Cleft%28%5Cpi%28.+%5Cmid+z%29+%5C%7C+%5Cpi_%7B%5Ctext+%7Bref+%7D%7D%28.+%5Cmid+z%29%5Cright%29-%5Cunderbrace%7B%5Clambda_E+%5Clog+%28%5Cpi%28x+%5Cmid+z%29%29%7D_%7B%5Ctext%7BEntropy+bonus+%7D%7D%2B%5Csum_i+%5Cunderbrace%7B%5Clambda_i+B_i%28x%29%7D_%7B%5Ctext+%7BNovelty+reward+%7D%7D%5D%2C" /> </p><p>其中 <img alt="z \sim \mathcal{D}, x \sim \pi(. \mid z), y \sim p(. \mid x)" src="https://www.zhihu.com/equation?tex=z+%5Csim+%5Cmathcal%7BD%7D%2C+x+%5Csim+%5Cpi%28.+%5Cmid+z%29%2C+y+%5Csim+p%28.+%5Cmid+x%29" /> 。</p><h2><b>疗效：面对新红队，RLHF 对齐过的大模型也很脆皮</b></h2><h3><b>基线方法</b></h3><p>作者采用四种现有的优化 red teaming LM 的方法作为基线：</p><ol><li>RL [1]: 即上一节列出的带 KL散度约束项的 RL 目标；</li><li>RL+TDiv [2]: 在 RL 的基础上，添加一个鼓励多样性的正则项（多样性以目标 LLM 的 embedding 相似度衡量）；</li><li>Zero-shot (ZS) 和 few-shot (FS)：固定 red teaming LM 的参数，通过 prompt engineering 优化（few-shot即提供几个实例进行上下文学习）</li></ol><h3><b>评测指标</b></h3><p><b>有效性</b>的评测指标为对抗样本的有害比例（取有害文本分类模型的概率输出卡阈值），<b>多样性</b>的评测指标为self-bleu 和 bert embedding 距离。</p><h3><b>实验效果</b></h3><p>作者在续写和指令遵循（instruction following）两个任务上做了评测，红队模型为 137M 的 GPT-2，目标模型主要是GPT-2-alpaca、Dolly-7B 。结果显示，在本文提出的训练策略得到的红队模型生成的对抗prompts 显著更优，有效性和多样性都领先于基线方法，例如以下在 Dolly-7B 上的结果：</p><figure><img class="origin_image zh-lightbox-thumb" src="https://pic1.zhimg.com/v2-d9d68b2e80045462e50527469e44a4b4_b.jpg" width="2190" /></figure><p>值得注意的是，即使是经过仔细的 red teaming和 RLHF 对齐的 LLaMa-2-chat-7b 模型（据LLaMa2 原文和之前的 red teaming工作宣称很难攻破），本文的方法也发现了一批共 196 个有效的对抗 prompts，举例如下：</p><figure><img class="origin_image zh-lightbox-thumb" src="https://pic4.zhimg.com/v2-47e0b0e8b320237c6d5e88c42c889753_b.jpg" width="1814" /></figure><p>可以看出，对于人看起来很自然的对抗prompts，LLaMa-2 会产生恶意满满的”厨师不配有个人生活“、”小孩什么事都做不了“这样的恶意回复。<b>当红蓝对抗的矛再次升级，LLM safety 的盾牌加固看来任重道远。</b></p><p><b>参考文献</b></p><p>[1] Perez, Ethan, et al. "Red Teaming Language Models with Language Models."<i>Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing</i>. 2022.</p><p>[2] Casper, Stephen, et al. "Explore, Establish, Exploit: Red Teaming Language Models from Scratch."<i>arXiv preprint arXiv:2306.09442</i>(2023).</p>
]]></content:encoded>
<pubDate>Thu, 30 Nov 2023 12:37:27 GMT</pubDate>
</item>
<item>
<title>EMNLP 2023 | 用于开放域多跳推理的大语言模型的自我提示思想链</title>
<link>https://zhuanlan.zhihu.com/p/667935793</link>
<guid>https://zhuanlan.zhihu.com/p/667935793</guid>
<content:encoded><![CDATA[
<p><b>©PaperWeekly 原创 · 作者 | </b>王金元</p><p><b>单位 | </b>上海交通大学</p><p><b>研究方向 | </b>大模型微调及应用</p><p class="ztext-empty-paragraph"><br /></p><figure><img class="origin_image zh-lightbox-thumb" src="https://pic1.zhimg.com/v2-f3c8410c6e7905e1860ac747b9cdd67c_b.jpg" width="1080" /></figure><p><b>论文标题：</b></p><p>Self-prompted Chain-of-Thought on Large Language Models for Open-domain Multi-hop Reasoning</p><p><b>模型&amp;代码地址：</b></p><p><a class=" external" href="https://link.zhihu.com/?target=https%3A//github.com/noewangjy/SP-CoT" rel="nofollow noreferrer" target="_blank"><span class="invisible">https://</span><span class="visible">github.com/noewangjy/SP</span><span class="invisible">-CoT</span><span class="ellipsis"></span></a></p><p>在开放域问答（ODQA）中，大多数现有问题仅要求基于常识的单跳推理。为了进一步扩展这项任务，我们正式引入了开放域多跳推理（ODMR），通过在开放域设置中使用显式推理步骤回答多跳问题。最近，大型语言模型（LLM）在无需外部语料库的情况下促进 ODQA 方面发现了显着的效用。此外，思想链（CoT）提示通过手动或自动范式更大程度地提高了大模型的推理能力。然而，现有的自动化方法缺乏质量保证，而手动方法的可扩展性有限且多样性差，阻碍了大模型的零样本能力。</p><p>在本文中，我们提出了自我提示的思想链（SP-CoT），这是一种自动化框架，可以由大模型通过自我提示方法大规模生产高质量的 CoT。SP-CoT 引入了高质量 ODMR 数据集的自动生成管道、用于上下文 CoT 选择的自适应采样器以及通过上下文学习进行自我提示推理。</p><p>在四个多跳问答基准的大量实验表明，我们提出的 SP-CoT 不仅在大规模（175B）LLM 上显着超越了之前的 SOTA 方法，而且将小规模（13B）LLM 的零样本性能提高了近一倍。 进一步的分析揭示，SP-CoT能够召回 MuSiQue-Ans 数据集上约 50% 的中间答案，具备产生直接和简洁的中间推理步骤的卓越能力。</p><h2><b>1.介绍</b></h2><p>开放领域问题回答（ODQA）是一个长久以来的挑战性任务，主要解决在没有特定上下文提供的情况下的事实性常识问题。尽管现有的 ODQA 工作主要集中在解决大多需要单跳推理的问题上，但学界对多跳问题回答（MHQA）的兴趣日益浓厚，这项任务的目标是通过在一系列候选文章上进行多步推理来得出正确答案。然而，这样的情境与真实世界的应用之间存在着显著的差异，因为后者往往缺乏用户提供的明确的候选文章集。</p><p>鉴于此，我们正式引入开放领域多跳推理（ODMR）作为 ODQA 的进阶任务，要求在开放领域环境中（不借助任何外部知识）使用明确的推理依据进行多跳问题回答。对于 ODMR 任务，一个新兴的方法是利用大型语言模型（LLMs），因为它们的众多参数中存储了大量的知识。</p><p>近年来，LLMs 已经展现出了强大的推理和指令执行能力，如 GPT-3、PaLM 和 InstructGPT。经过在大量文本数据上的广泛训练后，LLMs 证明了它们在复杂的推理任务上的零样本推理者身份，它们可以将多步骤的问题分解为中间的问题，然后进行逐步推理，最后产生最终答案。这种中间推理步骤被称为思维链（CoTs）。CoTs 通常作为上下文学习（ICL）的上下文演示，使 LLMs 能够通过一些作为提示的参考例子生成与目标任务形式一致的输出。Manual-CoT 采用人工设计的 CoTs 作为上下文演示来提高 LLMs 的推理性能。但是，它需要人类精细和细致的设计，而且对于每个问题，演示都是相同的，这可能是次优的。</p><p>Zero-shot-CoT 被提议用于触发自动化的 CoTs，例如使用 "\texttt{Let's think step by step:}" 这样的特定提示技术。前者提出了 Auto-CoT，一个自动化框架，用于大规模生产 CoTs 和建立上下文演示。然而，之前的工作并没有充分利用 LLMs 强大的指令执行和零样本推理能力。在本文中，我们提出了自提示思维链（SP-CoT），这是一个仅用于 LLM 的框架，用于大规模生产 ODMR 的高质量 CoTs。总体上，SP-CoT 引入了一个自动化生成 ODMR 数据集的管道，一个适应性的用于 CoT 选择的采样器，以及通过情景学习的自提示推理。这些自动化的 ODMR 数据集是没有候选上下文的 MHQA 数据集，但包括了六种类型的复杂推理链和逐步分解的多跳问题。每个中间 QA 步骤都配备了一个简短的解释来证明答案的正确性。通过利用 LLMs 的 ICL 能力，我们的方法对不同规模的 LLMs 都通常有效。</p><p>我们在开放领域环境的四个 MHQA 数据集上评估了我们的方法：ComplexWebQuestions（CWebQ）、HotpotQA、2WikiMultiHopQA （2Wiki）和 MuSiQue-Ans （MSQ）。广泛的实验表明，我们提出的 SP-CoT 不仅在大规模（175B）LLMs 上显著超过了之前的 SOTA 方法，而且在 ODMR 中几乎使小规模（13B）LLMs 的零样本性能翻倍。进一步的分析揭示了 SP-CoT 在 MSQ 数据集上回调约 50% 的中间答案，从而显著地激发了直接和简洁的中间推理步骤的出色能力。我们的贡献可以总结如下：</p><ul><li>我们引入了一个自动化流程，利用 LLMs 生成高质量的 ODMR 数据集，其中包括 2-4 跳问题和六种复杂的推理链。</li><li>我们提出了 SP-CoT，一个自动化框架，用于大规模生产 CoTs，作为演示池来进行采样，同时确保质量和多样性。</li><li>我们进行了广泛的实验，确认了 SP-CoT 在四个 ODMR 基准上的有效性。在 ODMR 设置中，我们的方法通过引出高质量的中间推理步骤显著提高了性能。</li></ul><h2><b>2. 方法</b></h2><p>我们提出的 SP-CoT 分为三个阶段（如图 1 所示）：在第一阶段，我们提示 LLM 逐步生成包括上下文、问题、答案和解释的 2 跳常识 QA 四元组。在第二阶段，我们通过连接 2 跳 QA 四元组来构建多跳推理链，并通过组合构建 ODMR 数据集。在最后一个阶段，我们采用基于聚类的采样方法，动态地选择并构造上下文示范用于推断。</p><figure><img class="origin_image zh-lightbox-thumb" src="https://pic1.zhimg.com/v2-153027a4e54942d1b16931f6595887e4_b.jpg" width="1080" /></figure><h3><b>2.1 阶段 1：通过自我生成的2跳QA</b></h3><p>在第一阶段，我们提示 LLM 逐步生成包括上下文、问题、答案和解释的 2 跳 QA 四元组，如图 2 所示。受之前工作的启发，我们设计了一个 2 跳常识 QA 生成流程，包括以下 4 个步骤：</p><figure><img class="origin_image zh-lightbox-thumb" src="https://pic4.zhimg.com/v2-523bfd83615b65b89c4771ef9bf99d2f_b.jpg" width="1080" /></figure><h3><b>步骤1：第一跳段落生成</b></h3><p>为了确保常识知识的全面覆盖，我们根据TriviaQA的统计数据手工设计了29个不同的主题。对于每个主题，我们要求LLM命名一定数量的关键词。对于每个收集到的关键词 <img alt="k_1" src="https://www.zhihu.com/equation?tex=k_1" />，我们要求LLM生成一个类似维基的段落 <img alt="p_1" src="https://www.zhihu.com/equation?tex=p_1" />。尽管存在一些事实错误，这些生成的段落包含了足够的事实信息，可用作QA生成的上下文。</p><h3><b>步骤2：第一跳QA生成</b></h3><p>考虑到常识问题的答案很可能是命名实体，我们使用Spacy和 NLTK库从段落 <img alt="p_1" src="https://www.zhihu.com/equation?tex=p_1" /> 中提取命名实体作为候选答案。对于每一个候选答案 <img alt="a_1" src="https://www.zhihu.com/equation?tex=a_1" />，我们要求LLM根据段落 <img alt="p_1" src="https://www.zhihu.com/equation?tex=p_1" /> 提出一个问题 <img alt="q_1" src="https://www.zhihu.com/equation?tex=q_1" />，其答案是 <img alt="a_1" src="https://www.zhihu.com/equation?tex=a_1" />。为了确保 <img alt="q_1" src="https://www.zhihu.com/equation?tex=q_1" /> 的质量，我们采用了双重检查的过程，其中我们要求LLM给定上下文 <img alt="p_1" src="https://www.zhihu.com/equation?tex=p_1" /> 来回答生成的问题 <img alt="q_1" src="https://www.zhihu.com/equation?tex=q_1" />，以检查生成的答案 <img alt="a_1'" src="https://www.zhihu.com/equation?tex=a_1%27" /> 是否与 <img alt="a_1" src="https://www.zhihu.com/equation?tex=a_1" /> 一致。一旦生成的QA对通过了双重检查，我们就提示LLM为其写一个简短的解释 <img alt="e_1" src="https://www.zhihu.com/equation?tex=e_1" />。需要注意的是，候选答案必须排除关键词（<img alt="a_1\neq k_1" src="https://www.zhihu.com/equation?tex=a_1%5Cneq+k_1" />），因为第一跳中的答案会成为第二跳的关键词（<img alt="k_2 = a_1" src="https://www.zhihu.com/equation?tex=k_2+%3D+a_1" />, <img alt="k_2 \neq k_1" src="https://www.zhihu.com/equation?tex=k_2+%5Cneq+k_1" />）。除此之外，有效的解释必须包含答案（<img alt="a_1\in e_1" src="https://www.zhihu.com/equation?tex=a_1%5Cin+e_1" />）。</p><h3><b>步骤3：第二跳段落生成</b></h3><p>在第一跳的答案被用作第二跳段落生成的关键词之前，我们使用Spacy过滤出带有某些标签的答案（QUANTITY、ORDINAL、CARDINAL、PERCENT、MONEY、DATE、TIME），这些答案不适合生成类似维基的段落。给定一个关键词 <img alt="k_2" src="https://www.zhihu.com/equation?tex=k_2" />，我们重复在步骤1中描述的相同提示来生成段落 <img alt="p_2" src="https://www.zhihu.com/equation?tex=p_2" />。</p><h3><b>步骤4：第二跳QA生成</b></h3><p>我们首先从生成的段落 <img alt="p_2" src="https://www.zhihu.com/equation?tex=p_2" /> 中提取候选答案，同时屏蔽第一跳QA中的关键词 <img alt="k_1" src="https://www.zhihu.com/equation?tex=k_1" /> 和答案 <img alt="a_1" src="https://www.zhihu.com/equation?tex=a_1" />（也称为 <img alt="k_2" src="https://www.zhihu.com/equation?tex=k_2" />）以避免循环图。对于每个候选答案 <img alt="a_2" src="https://www.zhihu.com/equation?tex=a_2" />，我们要求LLM生成一个包含第一跳答案 <img alt="a_1" src="https://www.zhihu.com/equation?tex=a_1" /> 的问题 <img alt="q_2" src="https://www.zhihu.com/equation?tex=q_2" />，该问题可以由候选答案 <img alt="a_2" src="https://www.zhihu.com/equation?tex=a_2" /> 来回答。我们使用步骤2中的相同双重检查来检查 <img alt="q_2" src="https://www.zhihu.com/equation?tex=q_2" /> 的质量，并确保第二跳问题 <img alt="q_2" src="https://www.zhihu.com/equation?tex=q_2" /> 包含第一跳答案 <img alt="a_1" src="https://www.zhihu.com/equation?tex=a_1" />（<img alt="a_1 \in q_2" src="https://www.zhihu.com/equation?tex=a_1+%5Cin+q_2" />）以进行连续推理。然后我们重复步骤2中的相同提示来生成解释 <img alt="e_2" src="https://www.zhihu.com/equation?tex=e_2" />。</p><p>到目前为止，我们已经指导了LLM生成一个2跳常识QA四元组对，即（<img alt="p_1" src="https://www.zhihu.com/equation?tex=p_1" />, <img alt="q_1" src="https://www.zhihu.com/equation?tex=q_1" />, <img alt="a_1" src="https://www.zhihu.com/equation?tex=a_1" />, <img alt="e_1" src="https://www.zhihu.com/equation?tex=e_1" />） <img alt="\to" src="https://www.zhihu.com/equation?tex=%5Cto" /> （<img alt="p_2" src="https://www.zhihu.com/equation?tex=p_2" />, <img alt="q_2" src="https://www.zhihu.com/equation?tex=q_2" />, <img alt="a_2" src="https://www.zhihu.com/equation?tex=a_2" />, <img alt="e_2" src="https://www.zhihu.com/equation?tex=e_2" />），其中 <img alt="a_1 \in q_2" src="https://www.zhihu.com/equation?tex=a_1+%5Cin+q_2" />。</p><h3><b>2.2 阶段 2：通过组合实现多跳问答</b></h3><p>在第二阶段，我们通过连接的 2 跳 QA 四元组构建多跳推理链，如图 3 所示。我们提出了一个自动化的数据集构建流程，用 2-4 跳来构建 ODMR 数据集，流程如下：</p><figure><img class="origin_image zh-lightbox-thumb" src="https://pic2.zhimg.com/v2-adbd7f88dd871452e5ad7177a2a14385_b.jpg" width="1080" /></figure><h3><b>步骤1：推理链组合</b></h3><p>为了连接更多的问题，我们遵循Trivedi等人提出的可组合性标准，即两个单跳QA对 <img alt="(q_1, a_1)" src="https://www.zhihu.com/equation?tex=%28q_1%2C+a_1%29" /> 和 <img alt="(q_2, a_2)" src="https://www.zhihu.com/equation?tex=%28q_2%2C+a_2%29" /> 可以组成一个多跳问题 <img alt="Q" src="https://www.zhihu.com/equation?tex=Q" /> ，如果 <img alt="a_1" src="https://www.zhihu.com/equation?tex=a_1" /> 是一个命名实体并且它在 <img alt="q_2" src="https://www.zhihu.com/equation?tex=q_2" /> 中被提及。由于我们的2跳QA对已经满足这一条件，所以我们使用这一标准来连接更多的问题。我们采用6个具有2-4跳的推理图来构建6种类型的多跳推理链 ，并确保在每个推理链中：1） 中间问题 <img alt="q_i" src="https://www.zhihu.com/equation?tex=q_i" /> 的答案 <img alt="a_i" src="https://www.zhihu.com/equation?tex=a_i" /> 将出现并且仅出现在其下一跳问题 <img alt="q_{i+1}" src="https://www.zhihu.com/equation?tex=q_%7Bi%2B1%7D" /> 中，以避免快捷方式；2） 最后一个问题的答案不会出现在任何中间问题中。</p><h3><b>步骤2：重复控制</b></h3><p>由于基于规则的组合构建，我们的新数据集有相当相似的推理链，其中有重复的中间问题。为了确保我们数据集的多样性和简单性，我们通过预设的重复度来过滤推理链，该重复度由在同一推理类型中与其他链共存的问题的数量来定义。</p><h3><b>步骤3：二进制问题生成</b></h3><p>我们注意到MHQA数据集还包括应该由“Yes”或“No”，而不是命名实体来回答的普通疑问句。因此，我们利用LLM重新构造一些推理链的最后QA <img alt="(q_n, a_n)" src="https://www.zhihu.com/equation?tex=%28q_n%2C+a_n%29" /> 为二进制问题，并使用4个手工设计的上下文演示。对于每种推理类型，我们随机抽样10%的推理链生成正面问题，10%生成负面问题。然后，我们通过生成的二进制问题及其之前的问题步骤来重新构造一个新的推理链，并将其添加到数据集中。</p><h3><b>步骤4：多跳问题生成</b></h3><p>现在我们需要生成多跳问题，之前生成的问题链将作为它们的中间推理步骤。对于每个问题链，我们迭代地将中间问题 <img alt="q_i" src="https://www.zhihu.com/equation?tex=q_i" /> 的答案 <img alt="a_i" src="https://www.zhihu.com/equation?tex=a_i" /> 在下一跳问题 <img alt="q_{i+1}" src="https://www.zhihu.com/equation?tex=q_%7Bi%2B1%7D" /> 中替换为 <img alt="[q_i]" src="https://www.zhihu.com/equation?tex=%5Bq_i%5D" />，直到最后一个问题 <img alt="q_n" src="https://www.zhihu.com/equation?tex=q_n" /> 被替换，这表示一个相关的从句。然后，我们利用LLM利用4个手工设计的上下文演示将其重新构造为一个自然的多跳问题。</p><p>经过上述流程，我们构建了一个高质量的2-4跳ODMR数据集，其中包括整体的多跳问题、带有详细QA四元组的分解推理链。凭借生成中的双重检查和可组合性标准，我们自动构建了一个高质量的新数据集。</p><p>图6为我们基于自提示算法自动化构建的 6 种 2-4 条推理链。</p><figure><img class="origin_image zh-lightbox-thumb" src="https://pic3.zhimg.com/v2-506f82d52ed1f99ecd8fc20165c6e50a_b.jpg" width="1080" /></figure><h3><b>2.3 阶段 3：自适应上下文演示构建</b></h3><p>在这个阶段，我们从生成的ODMR数据集中抽取多跳问题作为上下文演示。一些先前的工作已经表明，基于聚类的方法从演示的多样性中受益。我们采用基于聚类的检索方法，为输入问题自适应地抽取上下文演示。首先，通过使用Sentence-BERT编码，所有的问题都被投影到一个高维隐藏空间。假设我们需要 <img alt="n" src="https://www.zhihu.com/equation?tex=n" /> 个上下文演示。对于一个测试问题 <img alt="Q" src="https://www.zhihu.com/equation?tex=Q" />，我们使用k-means 算法将问题嵌入分成 <img alt="n" src="https://www.zhihu.com/equation?tex=n" /> 个簇，并从每个簇中自适应地检索与 <img alt="Q" src="https://www.zhihu.com/equation?tex=Q" /> 余弦相似度最高的问题。对于每个抽样的例子，我们依次连接每一跳的解释，前面加上"Step <img alt="i" src="https://www.zhihu.com/equation?tex=i" />:"，以构建一个推理链。一个完整的思维链如下：</p><figure><img class="origin_image zh-lightbox-thumb" src="https://pic2.zhimg.com/v2-36ad1dc8ce52dcca84127276e03183ad_b.jpg" width="1080" /></figure><h2><b>3. 实验</b></h2><p>我们的研究问题（RQs）是：</p><ul><li><b>RQ1: 与其他仅基于 LLM 的方法相比，SP-CoT 能在我们的四个 ODMR 基准测试中，对 LLMs 有多大的提升？</b></li><li><b>RQ2: SP-CoT 对近期流行的遵循指令的 LLMs 普遍有效吗？</b></li></ul><p>为此，我们对需要复杂多步推理的四个 MHQA 数据集进行了实验，并比较了不同方法在不同 LLMs 上的差异。</p><h3><b>3.1 基准测试和评估指标</b></h3><p>多跳问答（MHQA）数据集旨在通过要求模型阅读多个段落来回答给定问题来测试推理和推理技能。我们选择以下四个 MHQA 数据集：HotpotQA、2WikiMultiHopQA（下简称：2Wiki）、MuSiQue-Ans（下简称：MSQ）和ComplexWebQuestions（下简称：CWebQ）。为了使他们作为 ODMR 基准，我们仅使用每个示例中的问题和答案，不使用其提供的多个段落作为上下文。我们采用精确匹配（EM）和 F1 分数作为评估指标。基于 Karpukhin 等人在  DPR 工作中的评估脚本，我们添加了一个预处理步骤，该步骤忽略“（）”内的内容并通过某些分隔符分割答案字符串以提取多个答案。</p><h3><b>3.2 实验设置</b></h3><p>作为参考，我们使用额外的语料库进行微调方法的实验，这些方法是基于 NQ 数据集的训练分割进行微调的，并且其中大多数采用 Wikipedia dump 作为额外的语料库。我们还测试了我们对最近 LLMs 的检索方法的实现。具体来说，我们使用微调的 DPR 从 Wikipedia 检索前 5 个文档作为上下文，并雇用 LLM 作为 Reader 来根据上下文回答问题。</p><p>除非另有说明，我们会按照之前的工作，使用 Sentence-BERT 进行问题编码。默认的上下文演示数量为 8，这些演示通过每个集群中问题的最大余弦相似度进行采样。</p><p>对于 RQ1，我们采用 ChatGPT（<code>gpt-3.5-turbo-0301</code>）作为 LLM 来进行以下实验。根据 OpenAI，<code>gpt-3.5-turbo-0301</code> 是对 InstructGPT（<code>text-davinci- 003</code>）模型的改进，其性能与<code>text-davinci-003</code> 的推断能力相当。我们在实验中使用了每个数据集的整个开发集。</p><p>对于 RQ2，我们不仅测试 InstructGPT（<code>text-davinci-003</code>），还使用了三个较小规模（13B）的 LLMs：Alpaca、Vicuna 和 WizardLM，这些是在不同大规模遵循指令的数据集上微调的 LLaMA 模型。为了节省计算成本，我们对这四个数据集的子集进行了此实验，随机选择了测试集中的 1000 个样本。</p><h3><b>3.3 实验结果</b></h3><figure><img class="origin_image zh-lightbox-thumb" src="https://pic2.zhimg.com/v2-2c2ca386b2ea95f92f69c6cfb13f0c05_b.jpg" width="1080" /></figure><p>RQ1的主要结果如表1所示。即使有额外的语料库，由于MHQA的固有挑战，基于NQ微调的模型表现不佳。在使用相同的检索模型的情况下，基于检索的方法的性能在很大程度上取决于LLM阅读器。与之前仅基于LLM的工作相比，我们的SP-CoT在平均水平上明显优于Auto-CoT，EM得分高出 <img alt="+6.2" src="https://www.zhihu.com/equation?tex=%2B6.2" />，F1得分高出 <img alt="+6.4" src="https://www.zhihu.com/equation?tex=%2B6.4" />，并超过之前的SOTA方法GENREAD，平均EM得分高出 <img alt="+2.3" src="https://www.zhihu.com/equation?tex=%2B2.3" />，F1得分高出 <img alt="+2.8" src="https://www.zhihu.com/equation?tex=%2B2.8" />。在最具挑战性的基准测试MSQ上，SP-CoT使ChatGPT明显超过了其他仅基于LLM的方法。</p><p>我们注意到，在MSQ上，SP-CoT显著优于GENREAD，证实了为复杂的多跳问题提供高质量CoTs作为上下文示范的有效性。在其他三个数据集上，SP-CoT与GENREAD的性能相当。然而，GENREAD严重依赖于LLMs的生成忠实度，这对小规模的LLMs来说是具有挑战性的。通过将要求苛刻的指令分解为逐步简单的指令，我们的方法更适用于小规模的LLMs，这一点在表2中得到了验证。</p><p>表2展示了RQ2的结果。我们提出的SP-CoT经验证，通过显著提升这四个LLMs在所有四个基准测试上的性能，证明了其普遍有效性。使用SP-CoT，小规模（13B）LLMs的性能可以被提升，与直接提示的LLMs相当，这些LLMs的大小超过了10倍，在不考虑由SP-CoT引出的高质量的中间推理步骤的情况下。</p><figure><img class="origin_image zh-lightbox-thumb" src="https://pic4.zhimg.com/v2-961b6f688345cd9848eacb432bb82443_b.jpg" width="1080" /></figure><h2><b>4. 分析</b></h2><p>在这一部分，我们探讨了抽样方法和示范数量的选择。然后我们检查了由 SP-CoT 引出的中间推理步骤的质量，以及自生成数据的质量。除非另有说明，我们使用 ChatGPT（<code>gpt-3.5-turbo-0301</code>）在 RQ2 设置中提到的相同子集上进行分析。</p><h3><b>4.1 示范抽样的方法</b></h3><p>ICL 的性能在很大程度上取决于示范抽样的质量。我们测试了以下五种策略的有效性：随机抽样（Random）、通过最大余弦相似度全局抽样（Retrieve）、在每个簇中抽样最接近中心的（ClusterCenter）、在每个簇中通过最大余弦相似度抽样（RetrieveInCluster）以及在每个簇中按某种推理类型抽样最相似的 QAs（RetrieveInTypeCluster）。</p><p>输入问题的推理类型由其 <img alt="k" src="https://www.zhihu.com/equation?tex=k" /> 个最近邻的最频繁推理类型确定。如表 3 所示，RetrieveInCluster 是表现最好的策略，这恰恰是我们在之前的实验中采用的策略。</p><figure><img class="origin_image zh-lightbox-thumb" src="https://pic3.zhimg.com/v2-6433e55bdae9fa1254d267a8a06075ea_b.jpg" width="1080" /></figure><h3><b>4.2 示范数量的影响</b></h3><p>提供更多的上下文示范实证上可以改善 ICL 的性能；然而，这也会导致计算成本增加。为此，我们探究了示范数量和所得性能提升之间的权衡。我们报告了 2、4、6、8 和 10 个上下文示范在四个基准上的 EM 和 F1 分数，以及零样本设置中的分数。</p><p>如图 4 所示，当示范数量在 2 到 8 之间时，SP-CoT 的性能随着示范数量的增加而提高；但是，使用 10 个示范并没有带来进一步的性能提升。在我们的主要实验中，我们选择了 8 作为默认的示范数量，在性能和成本之间找到了一个平衡。</p><figure><img class="origin_image zh-lightbox-thumb" src="https://pic2.zhimg.com/v2-6fb06c1a2b56dfbde169b4d2a7fca339_b.jpg" width="1080" /></figure><h3><b>4.3 中间推理质量分析</b></h3><p>鉴于我们提出的 SP-CoT 构建的高质量 CoTs，我们研究了推断过程中生成的中间推理步骤的质量。为了这个分析，我们使用了 MSQ 的开发集，因为它是四个数据集中最具挑战性的，并提供了分解的逐步 QAs。我们比较了 Zero-shot-CoT、Auto-CoT 和 SP-CoT 在推断过程中生成的 CoTs。</p><p>为了公平，我们从所有三种方法都正确回答的 59 个问题中选择了 50 个。首先，我们使用 GPT-4 评估中间推理步骤在清晰度、简洁性、可理解性和直接性上的表现，并分别在 1 到 10 的范围内打分。此外，我们计算了在每种方法的推理步骤中共同出现的中间答案的回忆准确率。</p><p>为了公平，我们只报告了每种方法正确回答的问题的中间答案回忆准确率。如图 5 所示，GPT-4 高度青睐我们的 SP-CoT，其中间答案的回忆准确率接近  50%。这表明 SP-CoT 在清晰度、简洁性、可理解性和直接性方面产生了高质量的推理步骤。</p><figure><img class="origin_image zh-lightbox-thumb" src="https://pic2.zhimg.com/v2-60a8099901f96cba927b0f6d6ff68815_b.jpg" width="1080" /></figure><h2><b>5. 总结</b></h2><p>在这项工作中，我们利用 LLMs 的能力，结合自我提示的 CoTs，来解决开放领域下称为 ODMR 的复杂 MHQA 任务。我们创新的 SP-CoT 不仅通过超越前面的 CoT 提示技术设定了一个新的基准，而且在开放领域的问答中也超越了过去的仅大模型 SOTA 方法。</p><p>SP-CoT 的一个显著特点是其在引导高质量中间推理步骤方面的高效能，以及其在大规模和小规模 LLMs 上的普遍有效性。我们预期我们为 ODMR 设计的创新自我生成流程不仅会成为 SP-CoT 的基础，而且还将为未来的研究铺平道路，促使研究方向转向利用 LLMs 的自我生成能力，由 LLMs 完成，为 LLMs 服务。</p><hr /><p><b>#投 稿 通 道#</b></p><p>如何才能让更多的优质内容以更短路径到达读者群体，缩短读者寻找优质内容的成本呢？ <b>答案就是：你不认识的人。</b></p><p>总有一些你不认识的人，知道你想知道的东西。PaperWeekly 或许可以成为一座桥梁，促使不同背景、不同方向的学者和学术灵感相互碰撞，迸发出更多的可能性。</p><p>PaperWeekly 鼓励高校实验室或个人，在我们的平台上分享各类优质内容，可以是<b>最新论文解读</b>，也可以是<b>学术热点剖析</b>、<b>科研心得</b>或<b>竞赛经验讲解</b>等。我们的目的只有一个，让知识真正流动起来。</p><p><b>来稿标准：</b></p><p>• 文章确系个人<b>原创作品</b>，未曾在公开渠道发表，如为其他平台已发表或待发表的文章，请明确标注</p><p>• 稿件建议以 <b>markdown</b> 格式撰写，文中配图以附件形式发送，要求图片清晰，无版权问题</p><p>• PaperWeekly 尊重原作者署名权，并将为每篇被采纳的原创首发稿件，提供<b>业内具有竞争力稿酬</b>，具体依据文章阅读量和文章质量阶梯制结算</p><p><b>投稿方式：</b></p><p>• 方法一：在PaperWeekly知乎专栏页面点击“投稿”，即可递交文章</p><p>• 方法二：发送邮件至：hr@paperweekly.site ，所有文章配图，请单独在附件中发送</p><p>• 来稿请备注即时联系方式（微信），以便我们在稿件选用的第一时间联系作者</p><p>• 您也可以直接添加小编微信（<b>pwbot02</b>）快速投稿，备注：姓名-投稿</p><h2><b>关于PaperWeekly</b></h2><p>PaperWeekly 是一个推荐、解读、讨论、报道人工智能前沿论文成果的学术平台。如果你研究或从事 AI 领域，欢迎在公众号后台点击<b>「交流群」</b>，小助手将把你带入 PaperWeekly 的交流群里。</p><p><b>加入社区：</b><a class=" wrap external" href="https://link.zhihu.com/?target=http%3A//paperweek.ly/" rel="nofollow noreferrer" target="_blank">http://paperweek.ly</a></p><p><b>微信公众号：PaperWeekly</b></p><p><b>新浪微博：@PaperWeekly</b></p>
]]></content:encoded>
<pubDate>Sun, 26 Nov 2023 12:31:41 GMT</pubDate>
</item>
<item>
<title>开卷翻到毒蘑菇？浅谈大模型检索增强（RAG）的鲁棒性</title>
<link>https://zhuanlan.zhihu.com/p/668347524</link>
<guid>https://zhuanlan.zhihu.com/p/668347524</guid>
<content:encoded><![CDATA[
<p>很久没有写论文 notes 了，近期因为参与对检索增强生成（Retrieval-Augmented Generation, RAG）范式鲁棒性的研究，注意到了近两个月来社区中涌现了一小批关于这个话题的工作，简单梳理以飨读者。</p><h2>何为检索增强：模型可以开卷考</h2><p>纯参数化的大语言模型将其在海量语料上学习到的世界知识存储在模型参数中，虽然已经展现出来强大能力并改变整个 NLP 乃至深度学习社区的研究范式，但纯参数化的模型存在诸多不足：</p><ol><li><b>长尾记忆困难：</b>不能记住所有训练语料中的所有知识，尤其是对低频的长尾知识记忆困难；</li><li><b>容易过时：</b>参数中的知识容易过时（ChatGPT 和 LLaMa肯定不知道周二国足的比分，硬预测的话应该会预测个比三比零更大的数x），更新起来很困难（训练代价且容易造成灾难性遗忘）；</li><li><b>参数太多导致计算代价大：</b>训练和推理代价高昂（虽然有 Scaling Law，但参数量上去之后就没什么人训练甚至部署得起了→_→）。</li></ol><p><br />类似地，人也很难记住所有的知识（除了高考这种几乎纯比拼 memorization的考试之前），很多长尾的冷知识和新知识需要的时候从外部的消息源现查就好了。<br /></p><figure><img class="content_image" src="https://pic1.zhimg.com/v2-b882aa456c9437a753a61cf93545696c_b.jpg" width="300" /><figcaption>全都背下来就会像硬吃记忆面包一样痛苦</figcaption></figure><p><br />同样地，语言模型可以是半参数化的，也就是（参数化的）模型可以外挂一个（非参数化的）语料数据库，推理时以从语料库召回的部分数据为参考组织答案（具体的形式可以是作为额外的上文输入，也可以插在中间的 cross attention 或者输出中），这一范式被称为<b>检索增强生成（Retrieval-Augmented Generation，RAG）,  检索增强的语言模型（Retrieval-Augmented Language Model，RALM）的正式定义是：</b><br /><b>A language model (LM) that uses an external datastore at test time.</b></p><figure><img class="origin_image zh-lightbox-thumb" src="https://pic2.zhimg.com/v2-b2d2dcdf2b0bfcaecd5456aa8df7bbf5_b.jpg" width="913" /><figcaption>以上定义和示意图来自 ACL 2023 的 Tutorial [1]</figcaption></figure><p><br />示意图如上，对用户输入的文本x，我们构造记为 q 的query ，从数据库 D的索引中召回了小一部分相关文档(右侧的小块简报)，模型以其为参考生成最后的输出y。除了缓解以上三个问题（长尾记忆困难、容易过时、参数太多导致计算代价大）之外，还可以起到给模型的回答提供可靠的消息来源、防止模型权重泄露隐私信息等作用，具体的机制和代表性工作可以参见今年 ACL 上陈丹琦老师领衔的<a class=" wrap external" href="https://link.zhihu.com/?target=https%3A//acl2023-retrieval-lm.github.io/" rel="nofollow noreferrer" target="_blank">Tutorial</a> [1]，此处不详细展开。</p><h2><b>检索增强是否一定可靠：开卷翻到了毒蘑菇呢？</b></h2><p>交代完了 RAG 这一大背景，回到今天想聊的鲁棒性的正题。我们知道，人在翻书找长尾知识或者上网冲浪吃新瓜的时候，如果不加审慎的分辨，很容易以讹传讹：</p><figure><img class="content_image" src="https://pic1.zhimg.com/v2-ea066dd958c220221e66ecc0dd857f0c_b.jpg" width="228" /></figure><p>当然，语言模型也不比人高明，如果检索增强的时候召回的是和输入问题<b>无关的内容（噪声干扰</b>），甚至是<b>反事实</b>的 fake news 或者被篡改的百科，模型就会像吃了毒蘑菇一样胡言乱语。</p><figure><img class="content_image" src="https://pic4.zhimg.com/v2-e0990bb711d3f8e6e16ff14d52d44963_b.jpg" width="196" /></figure><p>以下是来自论文[2]的一个检索回无关内容后输出被影响的例子，原本对“德牧能不能进机场”这样的简单的问题，ChatGPT是高度认可小狗同志作为导盲犬的价值的，果断说 yes，但是检索模块召回了一段“老德牧是一类 balabala某种狗的争议性名称”的百科介绍作为额外上文输入后，模型突然对小狗变凶了，说出了“机场不许你入内”这样的负心话。</p><figure><img class="origin_image zh-lightbox-thumb" src="https://pic1.zhimg.com/v2-e568c8789df8f7181142b5ea8d0a6918_b.jpg" width="622" /></figure><p>以下是来自论文[3]的检索到反事实信息造成模型错误输出的例子。对博闻强识的大模型来说，17-18 赛季的欧冠冠军是道简单题，不用检索增强就知道是皇马，但如果有恶意用户某一天编辑了相关的维基百科把答案改成巴萨，模型通过检索模块吃到这样与自身知识冲突的辅助输入就会被忽悠住，人云亦云，复读出错误的答案。</p><figure><img class="content_image" src="https://pic2.zhimg.com/v2-ba8ebf709ee34f7f1757d0fc4e1cd925_b.jpg" width="407" /></figure><h2>如何提高检索增强的可靠性：怎么应对毒蘑菇？</h2><p>综上所述，RAG范式中，语言模型有可能在翻资料的时候误食毒鸡汤里的毒蘑菇，进而见小人、躺板板，胡言乱语误了大事。还好，笔者注意近两个月来社区中涌现了一小批研究来增强模型翻小抄的时候的鲁棒性，本文接下来的部分将介绍其中的五篇新鲜论文。</p><h3><b>SKR: 以自身知识引导检索增强</b></h3><p><b>论文链接[2]:</b> <a class=" wrap external" href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/2310.05002.pdf" rel="nofollow noreferrer" target="_blank">Self-Knowledge Guided Retrieval Augmentation for Large Language Models</a><br /><b>Takeaway: </b>发现RAG 召回的辅助信息不总是有用，甚至可能起负作用，因此设计了名为 SKR （Self-Knowledge Guided Retrieval Augmentation）的框架，对模型本身已知的问题直接生成答案，对未知的问题才调用 RAG 模块。<br /><b>解读：</b>直接看具体方法：</p><figure><img class="origin_image zh-lightbox-thumb" src="https://pic1.zhimg.com/v2-9d6f54ffef6c95e9a2e2dd8351af2b58_b.jpg" width="895" /></figure><ol><li><b>自我知识收集：</b>首先要知道自己知道什么，不知道什么（开始绕口令），因此收集一批有标注的训练集，模型可以直接答对的视为 known，检索增强后才能答对的视为 unknown；</li><li><b>识别是否已知：</b>对输入的测试问题，利用在训练集上构建的分类器识别其是否已知。分类器构建的方式作者试了好几种，可以用大模型本身上下文学习，可以用 RoBERTa小模型训个分类器，也可以用 SimCSE的 embedding为嵌入直接 KNN 分类（实验中 KNN 的性能最好）；</li><li><b>自适应式检索增强：</b>只对第二步中识别为 unknown 的输入进行检索增强，其余输入视为 known，直接回答。</li></ol><p>实验是在一些 QA数据集上做的，LM是InstructGPT 和 ChatGPT，似乎没有详细说明预训练 retriever 是什么模型，结果显示KNN 版本的 SKR与不带检索增强的CoT 以及非自适应的 RAG+CoT类型的基线相比，能取得3%-4%的显著提升。</p><figure><img class="origin_image zh-lightbox-thumb" src="https://pic4.zhimg.com/v2-9cf742835535601a53c9657abee62e33_b.jpg" width="547" /></figure><p>笔者有一个 concern是，上述方法的前二步中，识别 known/unknown的分类器是在和测试样本同分布的训练集上构建的，而且实验中似乎设定是用了完整的训练集（这样一来实际上有信息泄露，与其他的 zero-shot 和 few-shot 方法比较并不公平）。作者也讨论训练集大小的影响，但是有一点避重就轻的感觉，只表示训练集减小到 10% 会导致 2-3 个点的下降，该方法在训练集和测试集不同分布/可用的样本数很少的情况下的有效性还有待确认。</p><h3><b>RECALL: 反事实信息危害极大，现有干预方法难以缓解其风险</b></h3><p><b>论文链接[3]</b>：<a class=" wrap external" href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/2311.08147.pdf" rel="nofollow noreferrer" target="_blank">RECALL: A Benchmark for LLMs Robustness against External Counterfactual Knowledge</a><br /><b>Takeaway: </b>构建了一套名为 RECALL 的 benchmark 来分析大模型对反事实信息输入的鲁棒性，发现现有开源大模型非常容易被反事实的输入误导，prompt engineering 和幻觉缓解领域中的现存方法难以有效解决该问题。<br /><b>解读：</b>与另外几篇工作中，non-relevant contexts 是从正常的大语料库中召回（只是可能与问题本身不太相关，对模型造成干扰）的设定不同，本文聚焦是一种更极端的干扰现象，即反事实信息（counterfactual information），也就是检索召回的内容是与事实恰好相反的假消息。理想情况下，一个明辨是非的模型对不同类输入问题和检索召回内容的处理能力应该是这样的：</p><ol><li>对自己的参数中有明确记忆的问题，即使检索模块的召回的内容与之冲突，也应该坚持原有的正确答案；</li><li>对自己不知道答案的问题，有正确的召回内容时可以以其为参考正确回答，如果召回的内容是错就随缘了x。</li></ol><p>本文首先提出了量化这一能力的一套 benchmark（名为 RECALL），向 EventKG（常识性知识问答）和 UJ （科学性知识问答）这两个阅读理解数据集中注入了反事实信息，在二选一的 QA 和生成式的问答任务上测试了ChatGLM2、LLaMa2、Vicuna、Baichuan2 等四个 6B-13B 规模的开源大模型，其中 QA任务分为两个子集呈现指标，即扰动的时候答案部分被修改（QA-A）和未被修改（QA-NA）。<br />结果显示，选择题形式的EventKG QA任务上，一旦对context的反事实扰动涉及到了答案本身（即答案被篡改为错误选项），模型的 accuracy （下图中的 QA-A Acc）会从 90%+ （图中的第一行"original"）下降到 20% 以下（第二行"edited"），远低于没有检索机制，模型直接回答时的 60%左右（第三行“no”）。相比之下，QA-NA和文本生成的指标下降幅度较小。</p><figure><img class="origin_image zh-lightbox-thumb" src="https://pic1.zhimg.com/v2-1470eab118b2ba5950d0a40e631c9a24_b.jpg" width="732" /></figure><p>为了更精细地量化反事实信息带来的影响，作者额外定义了两个指标：</p><ul><li><b>误导率 M-Rate: </b>选择式 QA中，模型在无上下文输入时原本能答对的问题（即模型预训练阶段记忆住的问题），在接收反事实上下文后回答出错的比例；</li><li><b>错误重复率 R-Rate</b>: 生成任务中，反事实扰动对应的 tokens 在模型的答案中出现的比例。</li></ul><p>结果显示，EventKG 数据集上，四个大模型在 QA-A 设定下的误导率M-Rate高达80%以上，生成设定下反事实信息被复读的比例 R-Rate 也高达85-91%，可见RAG 模块如果召回了包含反事实信息的参考文档，将对模型的可信度造成巨大的危害。</p><figure><img class="origin_image zh-lightbox-thumb" src="https://pic2.zhimg.com/v2-177f21df467d62dbd62fe973ebc22eb5_b.jpg" width="756" /></figure><p>本文比较侧重前面的benchmark 构建与分析部分，本身没有直接提出新方法来增强模型的鲁棒性，而且测试了两种现存方法：</p><ul><li><b>Prompt Engineering: </b>简单粗暴，直接在 prompt 中告诉模型“忽略上下文中的反事实信息”；</li><li><b>DOLA [7]: </b>最近受到关注的一种针对模型幻觉的推理时干预方法，概括地讲是用模型（最后一层）输出的分布减去浅层 hidden states 对应的输出分布做解码。</li></ul><p>出于篇幅考虑此处不拉表格，直接搬运结论：</p><ul><li><b>Prompt Engineering：</b>虽然能提升 QA-A 设定下被扰动时的 accuracy，但对 QA-NA 设定下的 accuracy 反而有损害，有时对生成的质量也有损害；</li><li><b>DOLA: </b>虽然能小幅提升大部分指标，但会导致生成任务中错误复现率 R-Rate 显著上升。</li><li><b>结论：</b>以上两类方法都不能稳定地提升模型对反事实输入的鲁棒性，亟需有可靠的新方法解决这一问题。</li></ul><h3>Training Robust RALMs：数据硬怼，端对端提升鲁棒性</h3><p><b>论文链接[4]:</b> <a class=" wrap external" href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/2310.01558v1.pdf" rel="nofollow noreferrer" target="_blank">Making Retrieval-Augmented Language Models Robust to Irrelevant Context</a><br /><b>解读：</b>本文是篇偏实验分析性质的文章，测试了 NLI过滤召回结果和直接模拟带噪声的召回内容进行训练两类方法。<br />首先，直接用 NLI预训练模型判断召回的文档和问题是否相关来进行过滤，结论是NLI模型的过滤虽然能提升召回信息质量低时模型的鲁棒性，但也会伤及无辜（过滤掉有用的信息），在以 Google 搜索 top-1 为召回内容时总体上是掉点的。<br /> 接下来的方法非常直接暴力，既然 RAG 范式中检索来的内容 可能有噪音，大模型预训练的时候又没见过这种鱼龙混杂的上文，干脆发扬 end-to-end 的精神直接训练。坐着构建了一个 1.5k 样本的训练集，其中包含干净的 context和扰动的 context，希望模型学习到“不论如何都能输出正确答案”的能力。结果确实显示该数据上微调后的 LLaMa2-13B模型在各种 QA任务上，无论是正常的 Google 搜索召回、故意召回排名低的文档（low-rank retrieval），还是随机召回，都能比普通的 RAG显著提升准确率，在low-rank 和 random 的设定下基本和不带 RAG的原模型相当。有一点缺憾是，本文没有讨论这种微调是否影响了模型在其他领域的通用能力，未来或许可以考虑将这种为 RAG鲁棒性设计的数据集加到模型的预训练或者 SFT 阶段中。</p><h3>Chain of Note：适合检索增强的思维链蒸馏</h3><p><b>论文链接[5]:</b> <a class=" wrap external" href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/2311.09210.pdf" rel="nofollow noreferrer" target="_blank">Chain-of-Note: Enhancing Robustness in Retrieval-Augmented Language Models</a><br /><b>解读: </b>将思维链（CoT）方法用于增强 RAG 的鲁棒性，在中间推理过程中输出每一篇召回文档与输入问题的相关性（即对召回内容的 note）和自身对问题的认知，最后总结输出答案。作者用 ChatGPT 构造了一个这种格式的CoT 训练集，将此能力蒸馏到了 LLaMa2 上，显著提升了LLaMa2 带 RAG时的鲁棒性。<br />值得一提的是，另外几篇文章都没有注意 OOD detection 的问题，即当模型本身和召回文档都不掌握回答问题需要的知识时，应该回答 unknown 而不是胡编乱造，本文考虑了此问题（下图第三栏）。<br /></p><figure><img class="origin_image zh-lightbox-thumb" src="https://pic3.zhimg.com/v2-a51571dc57d1fe8cf85c3d3803074556_b.jpg" width="1046" /></figure><h3>Self-RAG：自我求助，自我生成，自我反思</h3><p><b>论文链接[6]:</b> <a class=" wrap external" href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/2310.11511.pdf" rel="nofollow noreferrer" target="_blank">Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection</a><br /><b>Takeaway: </b>本文提出了一个叫 Self-RAG 的框架，方法如其名，希望 LM自主决定对当前输入是否需要召回（而不是像 SKR[2]那样训练一个额外的分类器或像[4]那样借助一个 NLI 模型判断），把召回内容拼接近输入，再生成一段下文，自主判断召回文档是否与输入问题相关、自己借此生成的一段下文是否合理、是否有用，对topk 召回内容进行排序，把 top-1加进最后的输入以尽量生成正确答案。框架如下图右栏所示。</p><figure><img class="origin_image zh-lightbox-thumb" src="https://pic3.zhimg.com/v2-278eac2ecbc85a27a942691e14230f22_b.jpg" width="1066" /></figure><p><b>解读：</b>如上所示，Self-RAG 把要不要进行检索的决定、判断检索召回内容是否与问题相关、检索增强后的输出是否合理有用这几个决定都转化成了 token 预测的形式，文中称为 reflection tokens，整个过程可以用如下算法概括：</p><figure><img class="origin_image zh-lightbox-thumb" src="https://pic2.zhimg.com/v2-0f8f6689aea6c31b4bba6c5bcc366115_b.jpg" width="890" /></figure><p>第 2 行产生的Retrieve决定是否进行检索，如果进行检索的话，各段内容对应的相关程度IsREL、自我支持程度IsSUP、有用程度IsUSE共同组成排序的分数标准。作者把各个维度分别分了几档做离散的预测，如下表所示：</p><figure><img class="origin_image zh-lightbox-thumb" src="https://pic2.zhimg.com/v2-f547d32ec83defa28c52111f87240645_b.jpg" width="857" /></figure><p>以上是方法的骨架，接下来的关键在于如何构造包含reflection tokens 的训练数据来训练 Self-RAG。数据构建的流程略复杂，文中没有给出简洁的流程图，笔者概括如下：</p><ol><li>GPT-4 收集种子数据：对四种类型的reflection tokens，各用 GPT-4 标注 4k-20k 个从开源的 QA 和知识问答数据中收集的样本；</li><li>知识蒸馏，训练 critric model: 在第 1 步的训练数据上微调开源大模型，如LLaMa2-7B，称为 critic model；</li><li>为生成模型生成训练数据：使用上述的 critic model联合检索模块，为最后的生成模型构造模拟整个 Self-RAG 推理过程的训练集（两个例子如下图所示），约 150k 大小；</li><li>训练生成模型：在第 3 步生成的训练数据上训练生成模型，文中为 LLaMa2-7B和 13B，最后推理时只需要该模型，不需要 critic model。</li></ol><figure><img class="origin_image zh-lightbox-thumb" src="https://pic1.zhimg.com/v2-e9fa75ba5b4189367981ca67425935e0_b.jpg" width="1766" /></figure><p>这里笔者存在一个疑问：是否相关、是否自我支持、是否有用这几个客观标准，用 GPT-4标注是合理的，但是否需要检索增强，也就是 上面的Retreive这个 reflection token，是和生成模型本身的能力相关的，GPT-4不需要检索就能回答的问题，可能 LLaMa2 就需要检索，这里这样蒸馏是否合理有待讨论。<br />按下该疑问不表，我们来看目前实现版本的效果，可以发现 Self-RAG 在一系列开放域QA 和生成任务上都能比普通的检索增强 LLaMa2 取得明显提升。<br /></p><figure><img class="origin_image zh-lightbox-thumb" src="https://pic3.zhimg.com/v2-6d4082b7604a89b9f885d1da8f48116a_b.jpg" width="564" /></figure><h2>方法论小结</h2><p>至此，我们已经阐明了大模型检索增强范式的鲁棒性问题，并检视、学习了五篇意图解决该问题的近期工作。整体来看，方法可以分为两类：</p><ol><li><b>自适应检索和过滤：</b>即在检索前加一个模块判断该问题是不是需要检索增强才能回答或判断检索回的内容是否有用，以避免不必要的检索召回内容被输入模型产生干扰，如 SKR [2] 用模型自身的信号在训练数据上额外构建一个分类器，[4]直接使用 NLI模型，Self-RAG [6]从 GPT-4 蒸馏能力，让语言模型自己以预测Retrieve token 的形式判断。实验已证实这类方法能有效地避免无用的召回内容的干扰，坏处是直接删除被判断为无用的内容，可能误伤有用的检索召回内容。</li><li><b>生成时干预：</b>希望即使无用甚至错误的内容被检索回来、输入模型，模型对这样的增强输入依然能凭借自身知识保持鲁棒，如 RECALL [2]的 prompt engineering 或者 Dola干预，[4]的直接构造相应的训练数据进行训练，Chain of Note [5]的思维链蒸馏，Self-RAG [6]的让模型自身判断召回的内容是否有用。 其中只有RECALL [2]是不需要训练的，但未取得明显收益，另外三类都需要依赖 ChatGPT 或 GPT4 这些强大的闭源模型构造训练信号。</li></ol><p>最后，笔者想讨论的一点零碎思考是，以上的各工作基本假定检索模型是固定的（Google API 或者冻结的预训练召回模型），如果把检索模型和 index 的更新也考虑进来，是否能进一步提升整个 RAG系统的鲁棒性？期待看到甚至参与新的相关工作。<br />小文写作于冬日的燕园和万柳，还有许多细节和未来可能的方向未尽讨论，望诸君不吝赐教。<br /><br /><br />参考文献<br />[1] Asai, Akari, et al. "Retrieval-based language models and applications." Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 6: Tutorial Abstracts). 2023.<br />[2] Wang, Yile, et al. "Self-Knowledge Guided Retrieval Augmentation for Large Language Models." arXiv preprint arXiv:2310.05002 (2023).<br />[3] Liu, Yi, et al. "RECALL: A Benchmark for LLMs Robustness against External Counterfactual Knowledge." arXiv preprint arXiv:2311.08147 (2023).<br />[4] Yoran, Ori, et al. "Making Retrieval-Augmented Language Models Robust to Irrelevant Context." arXiv preprint arXiv:2310.01558 (2023).<br />[5] Yu, Wenhao, et al. "Chain-of-Note: Enhancing Robustness in Retrieval-Augmented Language Models." <i>arXiv preprint arXiv:2311.09210</i> (2023).<br />[6] Asai, Akari, et al. "Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection." arXiv preprint arXiv:2310.11511 (2023).<br />[7] Chuang, Yung-Sung, et al. "Dola: Decoding by contrasting layers improves factuality in large language models." <i>arXiv preprint arXiv:2309.03883</i> (2023).</p>
]]></content:encoded>
<pubDate>Thu, 23 Nov 2023 03:27:30 GMT</pubDate>
</item>
<item>
<title>丢弃99%的参数！阿里团队提出语言模型合体术，性能暴涨且无需重新训练和GPU</title>
<link>https://zhuanlan.zhihu.com/p/668152236</link>
<guid>https://zhuanlan.zhihu.com/p/668152236</guid>
<content:encoded><![CDATA[
<p><b>©PaperWeekly 原创 · 作者 |</b> 于乐</p><p><b>单位 | </b>阿里巴巴集团</p><p><b>研究方向 | </b>自然语言处理</p><p><br /></p><figure><img class="origin_image zh-lightbox-thumb" src="https://pica.zhimg.com/v2-289c576bab3507966beba17a7b625666_720w.jpg?source=d16d100b" width="1080" /></figure><p><b>论文题目：</b></p><p>Language Models are Super Mario: Absorbing Abilities from Homologous Models as a Free Lunch</p><p><b>论文链接：</b></p><p><a class=" external" href="http://link.zhihu.com/?target=https%3A//arxiv.org/abs/2311.03099" rel="nofollow noreferrer" target="_blank"><span class="invisible">https://</span><span class="visible">arxiv.org/abs/2311.0309</span><span class="invisible">9</span><span class="ellipsis"></span></a></p><p><b>代码链接：</b></p><p><a class=" external" href="http://link.zhihu.com/?target=https%3A//github.com/yule-BUAA/MergeLM" rel="nofollow noreferrer" target="_blank"><span class="invisible">https://</span><span class="visible">github.com/yule-BUAA/Me</span><span class="invisible">rgeLM</span><span class="ellipsis"></span></a></p><p>试想一下，如果我们能够<b>轻而易举地</b>将具备不同能力的多个模型融合成<b>拥有全部能力的单个模型</b>，那该是多么酷炫的一件事！现在，利用来自阿里团队的研究者们提出的语言模型合体术，我们有望实现这一目标！研究者们能够在<b>无需重新训练和 GPU </b>的情况下将 WizardMath 的能力合并至 WizardLM 中，<b>让 GSM8K 上的零样本准确率从 2.2 飙升至 66.3！</b></p><h2><b>1. 引言</b></h2><p>人类总是通过各种方式（如电影和游戏）来表达获取额外能力的愿望。例如，在《X战警：天启》中，角色可以吸收其他变种人的能力来增强自身实力；在超级马里奥游戏中，主角可以通过吸收游戏中的道具获得扔火球的超能力。在本项工作中，阿里团队的研究者们发现<b>语言模型与天启和超级马里奥类似，也可以通过吸收其他模型来增强自身的能力，且这一过程无需重新训练或 GPU</b>。</p><p><br /></p><figure><img class="content_image" src="https://picx.zhimg.com/v2-2904b3260b729964525eae3d14f00740_720w.jpg?source=d16d100b" width="328" /></figure><p>对于语言模型（LM）而言，有监督式微调（SFT）是一种被广泛采用的策略。SFT 在预训练基模型的基础上，通过微调其参数来获得激发了特定能力的微调模型。显而易见，<b>SFT 带来的效果体现在了模型在 SFT 前后的参数变化中</b>，可以称之为 <b>delta 参数</b>。</p><p>阿里团队的研究者们首先证实 SFT 后的 LMs（无论是基于编码器还是基于解码器的）倾向于学习到<b>大量冗余的</b> <b>delta 参数</b>。研究者们借鉴 Dropout 的思路提出了 DARE（Drop And REscale）来显著降低 delta 参数的冗余性。在将 DARE 应用于拥有 700 亿参数的 LMs 后，可以在<b>维持模型性能的前提下去除多达 99% 的 delta 参数</b>（见图 1(a)）。同时，LMs 拥有的参数越多，它就能容忍越大的。</p><p>进一步地，研究者们通过 DARE 来合并多个同源的 LMs（即从同一个预训练模型微调而来的多个模型）：首先使用 DARE 降低每个模型中的参数冗余性，而后利用现有模型合并方法来获得具有多样能力的单个模型。研究者们将 WizardMath 合并到了 WizardLM 中，实现了 <b>WizardLM 在 GSM8K 上的零样本准确率从 2.2 到 66.3 的飙升</b>（见图 1 (b)）。为了提高工作的可复现性，研究者们开源了一个<b>支持多种类型的 LMs 和常用的模型合并方法的代码库</b>，访问链接为：</p><p><i><a class=" external" href="http://link.zhihu.com/?target=https%3A//github.com/yule-BUAA/MergeLM" rel="nofollow noreferrer" target="_blank"><span class="invisible">https://</span><span class="visible">github.com/yule-BUAA/Me</span><span class="invisible">rgeLM</span><span class="ellipsis"></span></a></i></p><figure><img class="origin_image zh-lightbox-thumb" src="https://picx.zhimg.com/v2-2dc04d2ca59f5914f8409a500cabaebe_720w.jpg?source=d16d100b" width="1080" /><figcaption>▲ 图1部分实验结果</figcaption></figure><h2><b>2. 方法介绍</b></h2><h3><b>2.1 DARE：一种用于消除delta参数冗余性的简单方法</b></h3><p>研究者们提出的 DARE 方法非常简单, 仅由两部分组成: 丢弃和重新缩放, 其工作流程如图2 所示。 <img alt="\boldsymbol{\theta}_{\mathrm{PRE}}" src="https://www.zhihu.com/equation?tex=%5Cboldsymbol%7B%5Ctheta%7D_%7B%5Cmathrm%7BPRE%7D%7D" /> 表示预训练基模型的参数, <img alt="\boldsymbol{\theta}_{\mathrm{SFT}}^t" src="https://www.zhihu.com/equation?tex=%5Cboldsymbol%7B%5Ctheta%7D_%7B%5Cmathrm%7BSFT%7D%7D%5Et" /> 代表在预训练模型的基础上针对任务 <img alt="t" src="https://www.zhihu.com/equation?tex=t" /> 进行 <img alt="\mathrm{SFT}" src="https://www.zhihu.com/equation?tex=%5Cmathrm%7BSFT%7D" /> 得到的模型参数。给定 delta 参数 <img alt="\boldsymbol{\delta}^t=\boldsymbol{\theta}_{\mathrm{SFT}}^t-\boldsymbol{\theta}_{\mathrm{PRE}}" src="https://www.zhihu.com/equation?tex=%5Cboldsymbol%7B%5Cdelta%7D%5Et%3D%5Cboldsymbol%7B%5Ctheta%7D_%7B%5Cmathrm%7BSFT%7D%7D%5Et-%5Cboldsymbol%7B%5Ctheta%7D_%7B%5Cmathrm%7BPRE%7D%7D" />, DARE 首先根据丢弃率 <img alt="p" src="https://www.zhihu.com/equation?tex=p" /> 对 <img alt="\boldsymbol{\delta}^t" src="https://www.zhihu.com/equation?tex=%5Cboldsymbol%7B%5Cdelta%7D%5Et" />进行随机丢弃（将它们的值重置为零）, 然后将剩余的参数乘以 <img alt="1 /(1-p)" src="https://www.zhihu.com/equation?tex=1+%2F%281-p%29" />, 计算过程如下:</p><figure><img class="origin_image zh-lightbox-thumb" src="https://pic1.zhimg.com/v2-e75a20d2014fc99939c6f92c934733de_720w.jpg?source=d16d100b" width="900" /></figure><p>最后, 研究者们将 <img alt="\widehat{\delta}^t" src="https://www.zhihu.com/equation?tex=%5Cwidehat%7B%5Cdelta%7D%5Et" /> 和 <img alt="\boldsymbol{\theta}_{\mathrm{PRE}}" src="https://www.zhihu.com/equation?tex=%5Cboldsymbol%7B%5Ctheta%7D_%7B%5Cmathrm%7BPRE%7D%7D" /> 相加来得到用于推理的参数, 即 <img alt="\boldsymbol{\theta}_{\mathrm{DARE}}^t=\widehat{\boldsymbol{\delta}}^t+\boldsymbol{\theta}_{\mathrm{PRE}}" src="https://www.zhihu.com/equation?tex=%5Cboldsymbol%7B%5Ctheta%7D_%7B%5Cmathrm%7BDARE%7D%7D%5Et%3D%5Cwidehat%7B%5Cboldsymbol%7B%5Cdelta%7D%7D%5Et%2B%5Cboldsymbol%7B%5Ctheta%7D_%7B%5Cmathrm%7BPRE%7D%7D" /> 。研究者们指出重新缩放操作在 DARE 中是极其重要的, 它能够保持模型输出的期望大致不变。后续的实验也展示了该操作的有效性。</p><figure><img class="origin_image zh-lightbox-thumb" src="https://picx.zhimg.com/v2-8f04abef48ff3a9ecee1d0cb15bd038b_720w.jpg?source=d16d100b" width="1080" /><figcaption>▲ 图2 DARE流程图</figcaption></figure><h3><b>2.2 使用DARE进行模型合并</b></h3><p>研究模型合并方法的一个难点在于：对原始的模型参数进行简单的加权平均等运算会产生参数冲突，导致合并得到的模型效果比融合前的模型差。研究者们认为 DARE 具备的大幅降低参数冗余性的能力能天然地克服这一问题，并将 DARE 作为一个通用的预处理技术来有效地合并多个 LMs。</p><p>研究者们首先使用 DARE 来消除每个模型中的冗余 delta 参数以缓解多个模型之间的参数冲突，而后基于现有的模型合并方法整合降低了冗余性的 delta 参数（见图 3）。DARE 能<b>应用于任何现有的模型合并方法</b>，以 Task Arithmetic 方法来举例，DARE 的应用过程可以写为如下公式：</p><figure><img class="origin_image zh-lightbox-thumb" src="https://pic1.zhimg.com/v2-c19bba584ee342d015d1c308c70dce1c_720w.jpg?source=d16d100b" width="900" /></figure><figure><img class="origin_image zh-lightbox-thumb" src="https://picx.zhimg.com/v2-200c7c76ad1edf5959310ad6d296809d_720w.jpg?source=d16d100b" width="900" /></figure><figure><img class="origin_image zh-lightbox-thumb" src="https://pica.zhimg.com/v2-dd3a38c65a01e1905f22e9f230ee6528_720w.jpg?source=d16d100b" width="1080" /><figcaption>▲ 图3 使用DARE进行模型合并的流程图</figcaption></figure><h2><b>3. 实验结果</b></h2><p>研究者们在基于编码器的 LMs（预训练模型为 BERT 和 RoBERTa）和基于解码器的 LMs（预训练模型为 LLaMA，Llama 2，Code Llama）进行了实验。实验用到了 GLUE 中的 8 个数据集，AlpacaEval，GSM8K，MATH，HumanEval 和 MBPP。</p><h3><b>3.1 经过SFT后的LMs中delta参数的冗余性</b></h3><p>在参数丢弃率的不同取值下 LMs 的表现如图 4 和图 5 所示。首先，可以观察到<b>基于编码器和解码器的 LMs 的 delta 参数有非常高的冗余性。大多数情况下，利用 DARE 去掉 90% 甚至 99% 的 delta 参数不会显著降低性能</b>，说明了 LMs 在 SFT 过程中只需要微调极少的参数就能学习到类似于 LoRA 的“低秩结构”。</p><p>其次，<b>LMs 对丢弃率的容忍度随着模型参数量的增加而增加，即模型越大，能够承受的丢弃率越高</b>。最后，值得注意的是，WizardLM-70B 在丢弃率为 0.9 时的表现急剧下降，研究者们猜测可能是指令遵循任务相对比较复杂，需要更多的 delta 参数来实现这一目标，这同时导致规模更大的模型中的参数依赖性变得更强。因此，更高的丢弃率可能会破坏这种依赖关系，导致性能的巨幅下滑。</p><figure><img class="origin_image zh-lightbox-thumb" src="https://pic1.zhimg.com/v2-d97a02eeb32f122f20650ba3466e4164_720w.jpg?source=d16d100b" width="1080" /><figcaption>▲ 图4 基于解码器的LMs在不同丢弃率下的表现</figcaption></figure><figure><img class="origin_image zh-lightbox-thumb" src="https://pic1.zhimg.com/v2-3af452f3d09f4944b75bc40dfde5757d_720w.jpg?source=d16d100b" width="1080" /><figcaption>▲ 图5 基于编码器的LMs在不同丢弃率下的表现</figcaption></figure><h3><b>3.2 利用DARE来合并多个LMs</b></h3><p>研究者们首先使用 DARE 来去除 delta 参数中的冗余性，而后应用现有模型合并方法（包括 Average Merging, Task Arithmetic, Fisher Merging, RegMean 和 TIES-Merging）将多个 LMs 整合为单个 LM，结果如表 1 和图 6 所示。可以发现 DARE 通常能促进基于解码器的 LMs 的合并性能，<b>在某些情况下取得比单个模型更好的表现。DARE 也能提升基于编码器的 LMs 的模型合并效果。</b></p><p>然而，合并的模型大多数情况下仍难以超越单个模型，这与先前研究基于编码器的 LMs 的模型合并工作结论一致。同时，研究者们指出 <b>DARE 带来的提升在基于解码器的 LMs 上更明显</b>。一个可能的原因是，由于模型大小的差异，基于解码器的 LMs 能够容纳更多的能力。</p><figure><img class="origin_image zh-lightbox-thumb" src="https://picx.zhimg.com/v2-f9414448a23da2a6204e377333288c80_720w.jpg?source=d16d100b" width="1080" /><figcaption>▲ 表1 基于解码器的LMs的模型合并表现</figcaption></figure><figure><img class="origin_image zh-lightbox-thumb" src="https://picx.zhimg.com/v2-78bdf42717664ebd4ffed36d727be294_720w.jpg?source=d16d100b" width="1080" /><figcaption>▲ 图6 基于编码器的LMs的模型合并表现</figcaption></figure><h2><b>3.3 对于DARE中各组件的分析</b></h2><p>研究者们验证了 DARE 中重新缩放操作的功能，也将 DARE 中的随机丢弃操作和基于参数量级的剪枝方法进行了比较。由于篇幅限制，仅展示在基于解码器的 LMs 上的实验结果，如图 7 和图 8 所示。<b>实验结果显示 DARE 均取得了优于另外两种方法的效果，验证了 DARE 中两个组件的优越性</b>。</p><figure><img class="origin_image zh-lightbox-thumb" src="https://picx.zhimg.com/v2-a37d031c897a0c62e808dbe1ac061942_720w.jpg?source=d16d100b" width="1080" /><figcaption>▲ 图7 在基于解码器的LMs上是否进行重新放缩的实验结果</figcaption></figure><figure><img class="origin_image zh-lightbox-thumb" src="https://picx.zhimg.com/v2-79f6ae4590feb41810768d70bdd58d2d_720w.jpg?source=d16d100b" width="1080" /><figcaption>▲ 图8 在基于解码器的LMs上与基于参数量级的剪枝方法的比较结果</figcaption></figure><h3><b>3.4 对DARE应用条件的探究</b></h3><p>研究者们还探索了 DARE 的适用条件，通过统计经过 SFT 的 LMs 相较于预训练模型的参数变化范围（见图 9）可以发现，WizardCoder-Python-13B 和 Llama-2-13b 之间 delta 参数的绝对值（通常大于 0.01）比 WizardCoder-Python-13B 和 CodeLlama-13b-Python 之间的绝对值（通常在 0.0002 以内）大几个数量级，导致 DARE 失败。</p><p>对于其他从 Llama-2-13b 进行 SFT 得到的模型，它们 delta 参数的绝对值绝大部分都小于 0.005，使得 DARE 成为一个合适的选择。研究者们得出结论，<b>当 delta 参数的绝对值相对较小（例如小于 0.005）时，DARE 可以很好地发挥作用。否则，DARE 可能会失效</b>。</p><figure><img class="origin_image zh-lightbox-thumb" src="https://pica.zhimg.com/v2-bc561779cbfb10830f62125530c3fcb8_720w.jpg?source=d16d100b" width="849" /><figcaption>▲ 图9 经过SFT的LMs相较于预训练模型产生的参数变化范围</figcaption></figure><p>原始论文中还包含许多其余的实验结果和分析，有兴趣的读者可以查看原文来了解。</p><h2><b>4. 总结</b></h2><p>这篇工作重点围绕 “SFT 产生的 delta 参数存在极强的冗余性”展开研究。研究者们提出 DARE 来显著减少 SFT 所需的 delta 参数的数量，在保证模型性能的前提下可以丢弃 90% 甚至 99% 的 delta 参数。研究者们进一步将 DARE 作为适用于现有模型合并方法的通用预处理技术，将多个同源 LMs 合并成一个具有多样能力的 LM。</p><p>研究者们通过大量的实验证明了 DARE 在减少 delta 参数冗余性和促进模型合并性能上的有效性，同时对 DARE 的工作原理以及 DARE 的适用条件进行了深入分析。研究者们希望这项工作能激励更有效且高效的 SFT 策略设计，并相信 DARE 有潜力成为联邦学习领域中的一项新技术。</p><hr /><p><b>#投 稿 通 道#</b></p><p>如何才能让更多的优质内容以更短路径到达读者群体，缩短读者寻找优质内容的成本呢？ <b>答案就是：你不认识的人。</b></p><p>总有一些你不认识的人，知道你想知道的东西。PaperWeekly 或许可以成为一座桥梁，促使不同背景、不同方向的学者和学术灵感相互碰撞，迸发出更多的可能性。</p><p>PaperWeekly 鼓励高校实验室或个人，在我们的平台上分享各类优质内容，可以是<b>最新论文解读</b>，也可以是<b>学术热点剖析</b>、<b>科研心得</b>或<b>竞赛经验讲解</b>等。我们的目的只有一个，让知识真正流动起来。</p><p><b>来稿标准：</b></p><p>• 文章确系个人<b>原创作品</b>，未曾在公开渠道发表，如为其他平台已发表或待发表的文章，请明确标注</p><p>• 稿件建议以 <b>markdown</b> 格式撰写，文中配图以附件形式发送，要求图片清晰，无版权问题</p><p>• PaperWeekly 尊重原作者署名权，并将为每篇被采纳的原创首发稿件，提供<b>业内具有竞争力稿酬</b>，具体依据文章阅读量和文章质量阶梯制结算</p><p><b>投稿方式：</b></p><p>• 方法一：在PaperWeekly知乎专栏页面点击“投稿”，即可递交文章</p><p>• 方法二：发送邮件至：hr@paperweekly.site ，所有文章配图，请单独在附件中发送</p><p>• 来稿请备注即时联系方式（微信），以便我们在稿件选用的第一时间联系作者</p><p>• 您也可以直接添加小编微信（<b>pwbot02</b>）快速投稿，备注：姓名-投稿</p><h2><b>关于PaperWeekly</b></h2><p>PaperWeekly 是一个推荐、解读、讨论、报道人工智能前沿论文成果的学术平台。如果你研究或从事 AI 领域，欢迎在公众号后台点击<b>「交流群」</b>，小助手将把你带入 PaperWeekly 的交流群里。</p><p><b>加入社区：</b><a class=" wrap external" href="http://link.zhihu.com/?target=http%3A//paperweek.ly/" rel="nofollow noreferrer" target="_blank">http://paperweek.ly</a></p><p><b>微信公众号：PaperWeekly</b></p><p><b>新浪微博：@PaperWeekly</b></p>
]]></content:encoded>
<pubDate>Wed, 22 Nov 2023 03:35:27 GMT</pubDate>
</item>
<item>
<title>ACL2023 Findings | FACTUAL: A Benchmark for Faithful and Consistent Textual Scene Graph Parsing</title>
<link>https://zhuanlan.zhihu.com/p/668085813</link>
<guid>https://zhuanlan.zhihu.com/p/668085813</guid>
<content:encoded><![CDATA[
<p><b>单位 |</b> Monash University, Adobe Research, 武汉大学</p><p><b>研究方向 |</b> 自然语言理解</p><figure><img class="origin_image zh-lightbox-thumb" src="https://picx.zhimg.com/v2-06bf4e4bdacd1dcb76a414dca85762fc_720w.jpg?source=d16d100b" width="1319" /></figure><p><b>论文标题：</b> FACTUAL: A Benchmark for Faithful and Consistent Textual Scene Graph Parsing</p><h3><b>论文链接：</b></h3><a class=" wrap external" href="http://link.zhihu.com/?target=https%3A//arxiv.org/abs/2305.17497" rel="nofollow noreferrer" target="_blank">FACTUAL: A Benchmark for Faithful and Consistent Textual Scene Graph Parsing</a><h3><b>数据和代码链接：</b></h3><a class=" wrap external" href="http://link.zhihu.com/?target=https%3A//github.com/zhuang-li/FactualSceneGraph" rel="nofollow noreferrer" target="_blank">https://github.com/zhuang-li/FactualSceneGraph​github.com/zhuang-li/FactualSceneGraph</a><h2><b>1. 总览</b></h2><p>这是Monash University, Adobe Research和武汉大学联合发表于ACL2023的论文。这篇论文介绍了一个新数据集，FACTUAL，目的是提高视觉-语言任务中场景图解析（Scene Graph Parsing）的准确性（Faithfulness）和一致性（Consistency）。场景图解析是一个简单直接的任务，主要功能是将图片或文本描述转换为对应的语义场景图。在我们的论文中，我们专注于如何提高从文本到场景图的解析效果。例如，如下图所示，“A stone wall surrounds a group of old building.” 被转换为场景图“(wall, has_attribute, stone), (wall, surround, building), (building, has_attribute, group of), (building, has_attribute, old)”。场景图在诸如图像描述生成（image caption generation）、图像搜索（image retrieval）和多模态知识图谱构建（multimodal knowledge graph）等方面有广泛应用。</p><figure><img class="origin_image zh-lightbox-thumb" src="https://pic1.zhimg.com/v2-422b4e1de1b871c335e1267ddb03acd5_720w.jpg?source=d16d100b" width="2308" /></figure><p>由于先前数据集在收集过程中缺乏有效验证，并且对图中元素的定义不够严谨，导致收集到的场景图包含大量噪声，使得训练出的场景图解析器（Scene Graph Parser）效果不理想。为了解决这一问题，我们定义了一种名为FACTUAL-MR的中间语言（Intermediate Representation）。FACTUAL-MR对每个元素都进行了严格的定义，减少了标注者对定义的混淆，并降低了数据收集的难度。由于FACTUAL-MR与场景图具有一一对应的关系，我们通过收集FACTUAL-MR来获得高质量的场景图。最终，这些高质量的场景图数据使我们能够开发出更优秀的场景图解析器，从而提升多种下游视觉-语言任务的性能。</p><p>我们还开发了一个简单易用的[Python包](<a class=" external" href="http://link.zhihu.com/?target=https%3A//github.com/zhuang-li/FactualSceneGraph" rel="nofollow noreferrer" target="_blank"><span class="invisible">https://</span><span class="visible">github.com/zhuang-li/Fa</span><span class="invisible">ctualSceneGraph</span><span class="ellipsis"></span></a>)，用于支持调用我们最新的场景图解析模型以及最新的场景图解析评估方法。自从七月份我们的模型第一版在Huggingface上线以来，已经被下载了4000多次。此外，我们的pip包自上周推出以来也已经被下载了1000多次。</p><h2><b>2. 文章主要贡献：</b></h2><ol><li><b>新颖的中间表示</b>：提出FACTUAL-MR作为一种新的中间语言，确保场景图解析结果的准确性和一致性。</li><li><b>大规模数据</b>：引入了一个包含40,369个示例的大规模数据集，FACTUAL。这个数据集用于训练高质量的场景图解析模型，并进行全面评估，证明FACTUAL数据集显著提升了文本场景图解析的性能。特别值得提到的是，我们的模型在测试集上达到了79%的准确性，远高于之前最通用模型（SPICE Parser）的13%。</li><li><b>SoftSPICE度量</b>：基于常见的场景图相似度度量标准SPICE，我们提出了一种名为SoftSPICE的改进度量方法来计算图相似性。在实验中，SoftSPICE被证明可以显著提升利用场景图的视觉-语言任务的性能。</li></ol><h2>3. 问题描述：</h2><p>当前数据集的标注和模型输出的场景图存在两个主要问题：不准确和不一致。</p><ul><li><b>不准确</b>：这个问题分为场景图的完整性和正确性两个方面。</li><li><b>完整性</b>指的是场景图能否完全传达图片及其文字描述中交叉信息的语义含义。例如，解析器VG-T5的输出漏掉了关键事实，如网球运动员持有网球拍的关系，这表明场景图不完整。</li><li><b>正确性</b>关注于场景图对图片和文字描述信息的语义准确性。数据标注的错误会严重影响模型输出的场景图的正确性。例如，错误地将动词“rest”解释为物体“racket”的属性是一个明显的错误。</li><li><b>不一致</b>：数据集的不一致性主要由语言变化引起。同一语义内容可以以多种方式表达。例如，“(tennis player, hold, tennis racket)”和“(tennis racket, held by, tennis player)”在语义上等价，但主体和客体的顺序不同。不同标注者对任务的理解差异也会导致不一致性。例如，“stone wall”可能被某些人视为一个复合对象，而其他人可能将“stone”视为属性，“wall”视为独立的对象。</li></ul><p>由于先前数据集收集到的场景图含有大量噪声，因此训练出的场景图解析器（Scene Graph Parser）效果不佳。这些问题导致解析器在解析场景时往往无法准确捕获图片和文字描述之间的复杂关系，从而生成的场景图既不完整也不准确。例如，在处理语义复杂或模糊的描述时，解析器可能会产生错误的关系和属性，或者丢失重要的语义信息。这些错误进一步影响了场景图解析器在图像描述生成、图像搜索和多模态知识图谱构建等下游任务中的表现。为解决这些问题，需要一个定义更严格且标注更一致的数据集，以提升场景图解析模型的质量和准确性。</p><h2>4. 方法：</h2><h3><b>数据标注流程</b></h3><ul><li><b>对象和属性定义</b>：FACTUAL-MR准确且严格定义了图中的每个元素，确保标注者对标注问题的理解一致性。</li><li><b>量词、动词和介词注释</b>：为标注者提供了具体标注方案和动词及介词的固定列表，以确保标注一致性。</li><li><b>标注过程</b>：分为初始标注和专家标注者的验证/后处理两个阶段。</li></ul><figure><img class="origin_image zh-lightbox-thumb" src="https://pic1.zhimg.com/v2-54a0a6e8c4979458648646a5ff08cc81_720w.jpg?source=d16d100b" width="458" /></figure><h2>SoftSPICE定义</h2><figure><img class="content_image" src="https://pic1.zhimg.com/v2-46d2d121f40b4c50445d993c0e284e65_720w.jpg?source=d16d100b" width="412" /></figure><ul><li>与传统SPICE使用硬性字符串匹配（hard string matching）来判断图中的每个节点、属性和关系的相似度不同，我们采用基于嵌入（embedding-based）的方法来计算软性相似度（soft similarity），从而最终得到两个图的相似度评分。</li></ul><h2>5. 实验和评估：</h2><p><b>1. 文本场景图解析</b>：</p><p>我们基于T5在FACTUAL-MR数据集上训练了场景图解析模型。随后，我们在我们的测试集上使用了SPICE和Exact Set Match等度量来评估模型的性能。结果显示，我们的模型在效果上显著优于以前使用各种不同方法和不同数据集训练出的所有场景图解析模型。</p><figure><img class="origin_image zh-lightbox-thumb" src="https://pic1.zhimg.com/v2-ec68a5fb79d562ce00f67f72392dbb1b_720w.jpg?source=d16d100b" width="452" /></figure><p>人工评估的结果同样表明，我们的模型优于其他模型。</p><figure><img class="origin_image zh-lightbox-thumb" src="https://picx.zhimg.com/v2-366db7a644b0b4280cc5838be078f652_720w.jpg?source=d16d100b" width="454" /></figure><p><b>2. 图像描述生成评估</b>：我们使用SPICE和SoftSPICE度量来评估模型生成的文本描述质量。我们的新模型显著提高了SPICE和SoftSPICE得分与人类判断的相关性，这显示了我们的模型在下游任务（如图像字幕评估）方面的显著帮助。  </p><figure><img class="origin_image zh-lightbox-thumb" src="https://picx.zhimg.com/v2-27314b6c503d9ce1357d6ef92d124f18_720w.jpg?source=d16d100b" width="457" /></figure><p><b>3. 零样本图像检索</b>：我们的解析器在图像检索任务中证明了其有效性，其性能超越了其他解析器，并且展示了使用结构化信息进行图像检索的优势。</p><figure><img class="origin_image zh-lightbox-thumb" src="https://pic1.zhimg.com/v2-0b0c6486dcdfb28e3a8eb44fec591acd_720w.jpg?source=d16d100b" width="449" /></figure><h3>6. 结论：</h3><p>引入FACTUAL-MR标志着文本场景图解析领域的重大进步。通过提高数据标注的准确性和一致性，该论文解决了现有数据集中的标注不准确和不一致的问题，并显著提高了场景图解析器的效果（Exact Set Match从13%提升至79%）。我们的新模型及SoftSPICE度量也提升了各种下游视觉-语言任务的性能。我们的场景图解析器为图像描述生成，图像检索和其他相关领域的应用铺平了道路。</p>
]]></content:encoded>
<pubDate>Tue, 21 Nov 2023 14:54:54 GMT</pubDate>
</item>
<item>
<title>​EMNLP 2023 findings | 生成式框架下解决输入扰动槽填充任务</title>
<link>https://zhuanlan.zhihu.com/p/667932267</link>
<guid>https://zhuanlan.zhihu.com/p/667932267</guid>
<content:encoded><![CDATA[
<p><b>©PaperWeekly 原创 · 作者 |</b> 回亭风</p><p><b>单位 | </b>北京邮电大学</p><p><b>研究方向 |</b> 自然语言理解</p><p><br /></p><figure><img class="origin_image zh-lightbox-thumb" src="https://picx.zhimg.com/v2-1097e626e41e4e44303544261f1e594b_720w.jpg?source=d16d100b" width="1080" /></figure><p><b>论文标题：</b></p><p>DemoNSF: A Multi-task Demonstration-based Generative Framework for Noisy Slot Filling Task</p><p><b>论文链接：</b></p><p><a class=" external" href="http://link.zhihu.com/?target=https%3A//arxiv.org/abs/2310.10169" rel="nofollow noreferrer" target="_blank"><span class="invisible">https://</span><span class="visible">arxiv.org/abs/2310.1016</span><span class="invisible">9</span><span class="ellipsis"></span></a></p><p><b>代码链接：</b></p><p><a class=" external" href="http://link.zhihu.com/?target=https%3A//github.com/dongguanting/Demo-NSF" rel="nofollow noreferrer" target="_blank"><span class="invisible">https://</span><span class="visible">github.com/dongguanting</span><span class="invisible">/Demo-NSF</span><span class="ellipsis"></span></a></p><h2><b>1. 总览</b></h2><p>真实场景下的对话系统的用户输入通常存在各类扰动，例如人类在对话过程中通常伴有简化、冗余等口语化、非正式的表达。这种自由的表达通常会造成模型性能的严重下降。本文以对话系统中的槽填充任务（Slot Filling）为切入点，探索在输入存在扰动的情况下，生成式模型的表现。</p><h2><b>2. 什么是输入扰动？</b></h2><p>在真实的模型使用场景下，人类与系统交互时的输入是完全“自由”的。以对话系统为例，针对在线对话系统，人类输入可能存在拼写错误；针对语音交互的对话系统，在 ASR 识别过程中可能存在部分词的识别错误；人类自由的表达方式也会存在部分省略、冗余等。</p><p>我们将以上不符合常规语法或存在错误的输入称为输入扰动。在本文中，我们将输入扰动分为三类：字符级（如拼写错误）、词级（如 ASR 识别错误）和句子级（如简化、冗余等），数据集采用 RADDLE 和 SNIPS。对于 RADDLE，我们对五种扰动数据（Typos，Speech Error，Simplification，Verbose，Paraphrase）进行人工标注和校准，对于 SNIPS，我们使用 TextFlint 进行增强，并人工标注并校对。</p><figure><img class="origin_image zh-lightbox-thumb" src="https://pica.zhimg.com/v2-efe1b7b2ef351b1bda1864bfde3369d4_720w.jpg?source=d16d100b" width="1080" /></figure><h2><b>3. 基于多任务演示的生成式框架</b></h2><p>为了解决输入扰动带来的问题，本文提出了一个基于多任务演示的生成式框架，名为 DemoNSF，框架包括三个有噪声的辅助任务，即<b>噪声恢复（NR）</b>、<b>随机掩码（RM）</b>和<b>多噪声分类（HD）</b>，以提高对不同水平的输入扰动的性能。在下游过程中，本文将 SF 任务制定为由噪声任务演示指导的序列到序列生成。</p><h3><b>3.1 问题定义</b></h3><p>给定一个输入 <img alt="X=\{x_{1}, x_{2},..., x_{N}\}" src="https://www.zhihu.com/equation?tex=X%3D%5C%7Bx_%7B1%7D%2C+x_%7B2%7D%2C...%2C+x_%7BN%7D%5C%7D" /> 及其对应的槽类型集 <img alt="S=\{s_{1}, s_{2},..., s_{m}\}" src="https://www.zhihu.com/equation?tex=S%3D%5C%7Bs_%7B1%7D%2C+s_%7B2%7D%2C...%2C+s_%7Bm%7D%5C%7D" />，槽填充任务的目的是提取 <img alt="X" src="https://www.zhihu.com/equation?tex=X" /> 中所有的实体。对于有噪声的槽填充任务，我们将真实场景中的输入扰动过程表示为 <img alt="[(X^{'}, X^{'}=P(X,S))]" src="https://www.zhihu.com/equation?tex=%5B%28X%5E%7B%27%7D%2C+X%5E%7B%27%7D%3DP%28X%2CS%29%29%5D" />，模型的鲁棒性是在扰动测试数据集 <img alt="{(X^{'}, Y^{'})}" src="https://www.zhihu.com/equation?tex=%7B%28X%5E%7B%27%7D%2C+Y%5E%7B%27%7D%29%7D" /> 上进行评估的，但在训练阶段没有访问输入扰动过程 <img alt="P(\cdot)" src="https://www.zhihu.com/equation?tex=P%28%5Ccdot%29" /> 或扰动数据。在本文中 <img alt="D_{clean}" src="https://www.zhihu.com/equation?tex=D_%7Bclean%7D" />、<img alt="D_{aug}" src="https://www.zhihu.com/equation?tex=D_%7Baug%7D" /> 和 <img alt="D_{test}" src="https://www.zhihu.com/equation?tex=D_%7Btest%7D" /> 分别表示干净数据、增强数据和测试数据。</p><h3><b>3.2 噪声辅助任务</b></h3><figure><img class="origin_image zh-lightbox-thumb" src="https://pic1.zhimg.com/v2-ab82a15d3be273b011e05200be7ac231_720w.jpg?source=d16d100b" width="1080" /></figure><p><b>噪声恢复（NR）</b>：给定一个字符级增强输入 <img alt="X^{aug}_{char} = \{x_{1}, x_{2}, ..., x^{aug}_{m}, ..., x_{N}\}" src="https://www.zhihu.com/equation?tex=X%5E%7Baug%7D_%7Bchar%7D+%3D+%5C%7Bx_%7B1%7D%2C+x_%7B2%7D%2C+...%2C+x%5E%7Baug%7D_%7Bm%7D%2C+...%2C+x_%7BN%7D%5C%7D" /> ，其中 <img alt="x^{aug}_{m}" src="https://www.zhihu.com/equation?tex=x%5E%7Baug%7D_%7Bm%7D" /> 表示带有字符级增强的增强标记，NR任务的目标是将 <img alt="X^{aug}_{char}" src="https://www.zhihu.com/equation?tex=X%5E%7Baug%7D_%7Bchar%7D" /> 恢复到相应的干净输入 <img alt="X" src="https://www.zhihu.com/equation?tex=X" />。该任务旨在使模型能够旨在捕获<b>细粒度噪声数据</b>和<b>干净数据</b>之间的<b>映射关系</b>，从而增强模型表示细粒度噪声数据的能力。因此，损失函数可以表述为：</p><figure><img class="origin_image zh-lightbox-thumb" src="https://pica.zhimg.com/v2-77b965caee009a8909023a4c5721c048_720w.jpg?source=d16d100b" width="900" /></figure><p>其中，<img alt="B" src="https://www.zhihu.com/equation?tex=B" /> 和 <img alt="N" src="https://www.zhihu.com/equation?tex=N" /> 分别表示批处理大小和序列长度。</p><p><b>随机掩码（RM）</b>：受BERT中引入的掩码语言建模（MLM）概念的启发，本文提出了随机掩模任务。该任务给定 <img alt="D_{aug}" src="https://www.zhihu.com/equation?tex=D_%7Baug%7D" /> 中的一个输入，用特殊的 <img alt="[MASK]" src="https://www.zhihu.com/equation?tex=%5BMASK%5D" /> 符号随机掩码一个实体，得到 <img alt="X^{aug}_{mask} = \{x^{aug}_{1}, ..., [MASK], ..., x^{aug}_{N}\}" src="https://www.zhihu.com/equation?tex=X%5E%7Baug%7D_%7Bmask%7D+%3D+%5C%7Bx%5E%7Baug%7D_%7B1%7D%2C+...%2C+%5BMASK%5D%2C+...%2C+x%5E%7Baug%7D_%7BN%7D%5C%7D" />。目标是将标记恢复到其原始值。随机掩模过程旨在使模型能够<b>隐式学习扰动数据的槽实体分布</b>。因此，RM任务的损失函数可以定义为：</p><figure><img class="origin_image zh-lightbox-thumb" src="https://pica.zhimg.com/v2-2f8f1e58e16ba72e33ce20d56dc6dba8_720w.jpg?source=d16d100b" width="900" /></figure><p>其中，<img alt="y_{m}" src="https://www.zhihu.com/equation?tex=y_%7Bm%7D" /> 表示原始标记。</p><p><b>多噪声分类（HD）</b>：为了进一步解决粗粒度的输入扰动，本文提出了混合鉴别任务。该任务结合 <img alt="D_{clean}" src="https://www.zhihu.com/equation?tex=D_%7Bclean%7D" /> 和 <img alt="D_{aug}" src="https://www.zhihu.com/equation?tex=D_%7Baug%7D" /> 来创建一个混合数据集，表示为 <img alt="D_{mix}" src="https://www.zhihu.com/equation?tex=D_%7Bmix%7D" />。从 <img alt="D_{mix}" src="https://www.zhihu.com/equation?tex=D_%7Bmix%7D" /> 中随机选择输入，并根据所选的输入是干净的还是有不同程度的扰动来<b>分配不同的标签</b>。如图所示，生成模型可以在区分有扰动和无扰动的输入时考虑全局信息，同时隐式地捕获扰动数据所特有的语义特征。损失函数 <img alt="L_{HD}" src="https://www.zhihu.com/equation?tex=L_%7BHD%7D" /> 与 <img alt="L_{NR}" src="https://www.zhihu.com/equation?tex=L_%7BNR%7D" /> 相同。因此，总体损失函数 <img alt="L" src="https://www.zhihu.com/equation?tex=L" /> 被定义为：</p><figure><img class="origin_image zh-lightbox-thumb" src="https://pic1.zhimg.com/v2-55b4b036f98f6bb101a605cf41a0678d_720w.jpg?source=d16d100b" width="900" /></figure><h3><b>3.3 扰动演示构建</b></h3><p>与之前基于演示的工作不同，本文对于每个输入 <img alt="X" src="https://www.zhihu.com/equation?tex=X" />，我们从 <img alt="D_{aug}" src="https://www.zhihu.com/equation?tex=D_%7Baug%7D" /> 而不是 <img alt="D_{clean}" src="https://www.zhihu.com/equation?tex=D_%7Bclean%7D" /> 中选择示例 <img alt="s" src="https://www.zhihu.com/equation?tex=s" />，以将扰动语义信息纳入模型；对于检索，我们使用<b>SBERT</b>，它独立地为 <img alt="X" src="https://www.zhihu.com/equation?tex=X" /> 和 <img alt="s" src="https://www.zhihu.com/equation?tex=s" /> 生成嵌入表示，并计算它们的相似度得分以对 <img alt="s" src="https://www.zhihu.com/equation?tex=s" /> 进行排序，随后，我们选择top-k个示例来构建带噪声的演示 <img alt=" \hat{X} " src="https://www.zhihu.com/equation?tex=+%5Chat%7BX%7D+" />，并将它们与输入 <img alt="X" src="https://www.zhihu.com/equation?tex=X" /> 连接起来，形成完整的输入 <img alt=" [\hat{X}; X] " src="https://www.zhihu.com/equation?tex=+%5B%5Chat%7BX%7D%3B+X%5D+" />。</p><p>演示模板如下所示:</p><p>“Demonstrations: [Retrieved noisy utterances]. [text span] is [slot type]. Input Utterance: [Original input].”</p><h2><b>4. 实验结果</b></h2><h3><b>4.1 主实验</b></h3><p>表 1 展示了在单一扰动设置下的 DemoNSF 和其他 baseline 的主要结果。可以观察到，DemoNSF 无论是在细粒度的扰动，还是在粗粒度扰动方面均保持了很强的性能，具有显著的优势。这些结果清楚地表明，DemoNSF 有效地捕获了细粒度噪声数据和干净数据之间的映射关系，同时也考虑了泛化到粗粒度全局语义扰动的能力。</p><p>DemoNSF 是一个即插即用的方法，在不同的生成式模型上均可以达到很好的效果。在 BART 和 GPT-2 得到的结果与 baseline 相比性能也有显著提高。通过消融实验的结果进一步证明，该方法在面对扰动时显著增强了生成模型的鲁棒性。</p><figure><img class="origin_image zh-lightbox-thumb" src="https://picx.zhimg.com/v2-fa66c498fc3a4231f770fd374a03501e_720w.jpg?source=d16d100b" width="1080" /></figure><figure><img class="origin_image zh-lightbox-thumb" src="https://picx.zhimg.com/v2-38a1aba02cefdee8c9e7416f374f5f2c_720w.jpg?source=d16d100b" width="1080" /></figure><h3><b>4.2 混合扰动实验</b></h3><p>在真实的对话场景中，混合扰动经常同时出现在一个话语中。为了进一步验证 DemoNSF 在更现实的场景下的有效性，本文进行了混合扰动实验。</p><p>实验结果如表 2 所示，DemoNSF 在所有双扰动中，特别是在细粒度混合扰动中都显著优于其他 baseline。即使有三种扰动的混合场景，DemoNSF 仍能保持更好的性能。</p><figure><img class="origin_image zh-lightbox-thumb" src="https://pica.zhimg.com/v2-437c229a3c9a465f792b92a38d4eff20_720w.jpg?source=d16d100b" width="1080" /></figure><h3><b>4.3 演示示例的影响</b></h3><p>图 3 显示了在单一扰动下不同类型的演示数量的影响。可以发现：</p><p>（1）DemoNSF 在只有两个增强样本的情况下表现出显著的性能增益，而随着样本数量的增加，其性能会严重下降。这可能是因为多样化的增强实例可以帮助模型明确融合有噪声的语义分布，而超过一定阈值的样本多样性甚至可能带来额外的噪声。</p><p>（2）随着数量的增加，clean 演示只带来轻微的改善，这表明 clean 样本只提供一些任务一般信息（例如实体分布、槽-值映射）来进行提示。</p><p>（3）从混合数据池中检索到的演示显示出稳定的性能增益，进一步证实了噪声语义分布与任务一般信息之间的相互促进，并为基于提示的生成模型的鲁棒性提供了指导。</p><figure><img class="origin_image zh-lightbox-thumb" src="https://pica.zhimg.com/v2-69a630fe60704246d9ca3e1588311c4a_720w.jpg?source=d16d100b" width="1080" /></figure><h3><b>4.4 ChatGPT评估</b></h3><p>为了进一步验证我们提出的噪声示例策略的有效性，我们在 ChatGPT 和 Text-davinci-003 上进行评估实验。结果如下：</p><figure><img class="origin_image zh-lightbox-thumb" src="https://picx.zhimg.com/v2-0abf65b663c76f2206ebe055fb860ffd_720w.jpg?source=d16d100b" width="1080" /></figure><p>可以看出，无论是 Text-davinci-003 还是 ChatGPT，在扰动槽填充任务上表现都不尽人意。与传统的召回 clean 示例相比，增强示例和混合示例的策略在扰动任务的性能上有明显改善，这也证明了我们的示例策略在解决输入扰动问题中的有效性。</p><p>除此之外，从结果中可以看出，两种演示的策略在细粒度扰动上有显著的提升，但是在粗粒度中改善并不明显。即拼接示例这种上下文学习的方式更适配于细粒度扰动，至于如何大幅改善粗粒度扰动下的生成式模型的性能，是未来研究工作的一项挑战。</p><h2><b>5. 结论</b></h2><p>在本文中，我们提出了一个统一的基于多任务演示的生成框架的噪声槽填充任务，引入了三种新的有噪声的辅助任务和一种有噪声的演示构造策略，旨在从显式和隐式两个层面学习扰动的语义结构。在两个基准上的实验证明了 DemoNSF 的有效性，进一步的分析为生成框架的实际应用提供了经验指导。</p><p>但是，我们的工作还是存在其局限性，在训练前阶段引入三种新的噪声辅助任务，这可能比传统方法消耗更多的 GPU 内存和计算时间。此外，我们的方法主要集中于插槽填充任务。然而，我们相信有可能将我们的工作扩展到其他场景，如小样本和零样本的场景。我们也将它作为我们未来的研究以及改善的目标，欢迎大家多多留言多多交流。</p><hr /><p><b>#投 稿 通 道#</b></p><p>如何才能让更多的优质内容以更短路径到达读者群体，缩短读者寻找优质内容的成本呢？ <b>答案就是：你不认识的人。</b></p><p>总有一些你不认识的人，知道你想知道的东西。PaperWeekly 或许可以成为一座桥梁，促使不同背景、不同方向的学者和学术灵感相互碰撞，迸发出更多的可能性。</p><p>PaperWeekly 鼓励高校实验室或个人，在我们的平台上分享各类优质内容，可以是<b>最新论文解读</b>，也可以是<b>学术热点剖析</b>、<b>科研心得</b>或<b>竞赛经验讲解</b>等。我们的目的只有一个，让知识真正流动起来。</p><p><b>来稿标准：</b></p><p>• 文章确系个人<b>原创作品</b>，未曾在公开渠道发表，如为其他平台已发表或待发表的文章，请明确标注</p><p>• 稿件建议以 <b>markdown</b> 格式撰写，文中配图以附件形式发送，要求图片清晰，无版权问题</p><p>• PaperWeekly 尊重原作者署名权，并将为每篇被采纳的原创首发稿件，提供<b>业内具有竞争力稿酬</b>，具体依据文章阅读量和文章质量阶梯制结算</p><p><b>投稿方式：</b></p><p>• 方法一：在PaperWeekly知乎专栏页面点击“投稿”，即可递交文章</p><p>• 方法二：发送邮件至：hr@paperweekly.site ，所有文章配图，请单独在附件中发送</p><p>• 来稿请备注即时联系方式（微信），以便我们在稿件选用的第一时间联系作者</p><p>• 您也可以直接添加小编微信（<b>pwbot02</b>）快速投稿，备注：姓名-投稿</p><h2><b>关于PaperWeekly</b></h2><p>PaperWeekly 是一个推荐、解读、讨论、报道人工智能前沿论文成果的学术平台。如果你研究或从事 AI 领域，欢迎在公众号后台点击<b>「交流群」</b>，小助手将把你带入 PaperWeekly 的交流群里。</p><p><b>加入社区：</b><a class=" wrap external" href="http://link.zhihu.com/?target=http%3A//paperweek.ly/" rel="nofollow noreferrer" target="_blank">http://paperweek.ly</a></p><p><b>微信公众号：PaperWeekly</b></p><p><b>新浪微博：@PaperWeekly</b></p>
]]></content:encoded>
<pubDate>Tue, 21 Nov 2023 02:21:26 GMT</pubDate>
</item>
</channel>
</rss>