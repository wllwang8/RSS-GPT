<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
<channel>
<title>凡心的知乎动态</title>
<link>https://www.zhihu.com/people/vaxin/activities</link>


<item>
<title>凡心赞同了文章: 2024年大模型Agent tuning关键技术Fireact, Agent-FLAN, AgentOhana, Agent LUMOS, STE, ETO,MoE, DebateGPT等</title>
<link>https://zhuanlan.zhihu.com/p/690012170</link>
<guid>https://zhuanlan.zhihu.com/p/690012170</guid>
<content:encoded><![CDATA[
<div> agent, 训练, 数据结构, 长序列, 多智能体<br />
<br />
总结:<br />
本文探讨了Agents的优化技术，包括数据处理、训练和长序列优化。涵盖了对数据结构复杂性的挑战，以及通过多种方法对agents的不同能力进行微调，如tool use, reflection, collaboration, debate等。讨论了通过fine tuning、contrastive learning、memory retrieval等方式提升agents的性能，以及长序列训练下的挑战和解决方案。不断优化agents的技术将为未来进一步探索这一领域提供重要基础。 <div>
<p>现在的agent的技术分为prompt engineering和训练两种方式，prompt engineering的方向发展的比较快（例如autogen，metagpt，Qwen-Agent， model-scope agent, langchain-agents, crewAI），而训练则发展比较缓慢，主要原因是数据结构比较复杂，不容易获得高质量的数据，另外，模型到了一定规模以后，不需要针对agent的范式训练，就能直接进行agent应用，但对于小模型，如果没有经过训练，本身是不具备agent相关的能力的。下面我就介绍一下小模型的agent tuning相关的技术，agent涵盖了memory，tool use，reasoning，reflexion（能提升准确率，但会消耗更多的token，推理时间长，并不高效），collaboration，debate（最小化幻觉和谬论）等能力。</p><h2>为什么需要Agent tuning？</h2><ul><li>目前的agents的方法需要prompt示例进行行为约束，而prompt示例会在推理阶段占用很多的token数目，会增加推理的成本，还对模型的长序列能力提出了更高的要求。</li><li>目前的agents能力来源于大模型自身的zero-shot，instruction following等智能涌现能力，虽然效果好，但推理成本很高。</li><li>现有的LLM远未达到可靠的工具使用性能：GPT-4（OpenAI，2023）的正确率为60.8%，而专门针对工具使用进行微调的ToolLLaMAv2（Qin et al.，2024）仅为37.3%。</li><li>多智能体的协同优化通过fine tuning获得还存在盲区，但目前主流的agents应用框架都集成了多智能体的协作和debate的能力。</li></ul><p>目前agents还是一片蓝海，好多工作需要我们去探索，接下来我从agent tuning的角度来介绍一下这些tool use， reflection，memory，collaboration, debate等能力怎么通过微调训练获得，其中tool using的文章比较多，大多是数据处理方面的工作，训练则是会混合一些通用的数据集来一起进行训练，这是为了克服灾难性遗忘的问题，有的则引入了一些负样本，然后做类似DPO的训练，多智能体的训练文章不多，主要对角色扮演的能力进行拆分训练。</p><figure><img class="origin_image zh-lightbox-thumb" src="https://pic3.zhimg.com/v2-6243dc569f163247e3cd92912dedbbe2_1440w.jpg" /></figure><h2>Agents的tool能力训练</h2><h3>AgentTuning: Enabling Generalized Agent Abilities for LLMs</h3><p>Agent Instruction和AgentTuning概述如下图所示。Agent Instruction的构建，包括指令生成、轨迹交互和轨迹过滤器。AgentLM是使用AgentInstruction和通用域指令的混合进行微调的。<i>AgentInstruct的任务有6个，AlfWorld，WebShop，Mind2Web，Knowledge Graph，Operating System和Database。ALFWorld, WebShop, Mind2Web, 和Knowledge Graph有现成的数据集，就直接进行了数据划分，Operating System 和Database任务则使用Task Derivation和Self-Instruct来构造对应的instructions。</i></p><figure><img class="origin_image zh-lightbox-thumb" src="https://pic3.zhimg.com/v2-1d19f1247bc7b8cae48fc0ca73853f5e_1440w.jpg" /></figure><p>训练方式是混合训练，其中D-general表示的是general dataset，D-agent表示的是AgentInstruct数据集,两个数据集通过一定的混合比率进行训练：</p><figure><img class="origin_image zh-lightbox-thumb" src="https://pic3.zhimg.com/v2-cac606a224fe931050014f9902d96c5e_1440w.jpg" /></figure><p><i>论文使用的是AgentLM-7B, 13B, and 70B（基座模型是Llama 2 chat模型），其中AgentLM-70B能够在未见过的agents任务上跟GPT-3.5相当.</i></p><figure><img class="origin_image zh-lightbox-thumb" src="https://pic3.zhimg.com/v2-2438b53567b92d74beef67412781108a_1440w.jpg" /></figure><p>注意：论文提到仅仅在agents任务上进行训练，模型在未知任务上的表现很差。</p><p>论文链接：</p><a class=" external" href="https://arxiv.org/abs/2310.12823" rel="nofollow noreferrer" target="_blank"><span class="invisible">https://</span><span class="visible">arxiv.org/abs/2310.1282</span><span class="invisible">3</span><span class="ellipsis"></span></a><p>代码链接：</p><a class=" external" href="https://github.com/THUDM/AgentTuning" rel="nofollow noreferrer" target="_blank"><span class="invisible">https://</span><span class="visible">github.com/THUDM/AgentT</span><span class="invisible">uning</span><span class="ellipsis"></span></a><h3>Fireact: Toward language agent fine-tuning</h3><p>如下图，虽然LLM agents和语言模型微调都是热门话题，但它们的交叉点研究不足。这项工作迈出了第一步，展示了微调LMs用于agents用途的多重优势，并为LLM agent微调提出了各种新问题。</p><figure><img class="origin_image zh-lightbox-thumb" src="https://pic3.zhimg.com/v2-6d4e79024aa77c0af1ff14a9c385d7ee_1440w.jpg" /></figure><p>FireAct使用的还是ReAct这种thought-action-observation的这种范式，不仅构造了ReAct数据，还有Reflextion和CoT的数据，然后使用的LoRA进行的微调。如下图（a） 在微调过程中，大型LM（例如GPT-4）基于来自不同数据集的问题和来自不同方法的提示生成任务解决轨迹。然后将成功的轨迹转换为ReAct格式，以微调较小的LM。（b） 在推理过程中，微调的LM可以在没有少量提示的情况下操作，并且可以隐式地选择一种提示方法来完成具有灵活长度的ReAct轨迹，以适应不同的问题复杂性。例如，一个简单的问题可以只使用一轮思维-行动观察来解决，而不需要使用工具。</p><figure><img class="origin_image zh-lightbox-thumb" src="https://pic1.zhimg.com/v2-9903d9c214a591f74bc6084a1c886c30_1440w.jpg" /></figure><p>作者还探讨了什么时候直接prompting，什么时候做fine tune，对于一些已知的问题，数据可以使用prompting和chatgpt4的方法进行获取，直接进行fine tuning能够提升模型的推理速度（不需要few shot prompting），更好的泛化能力，如果任务是新的，则prompting的方法更占优势。</p><p>论文给了一些实验，微调后的Llama-2-13B可以优于所有GPT-3.5提示方法（<i>Input-Output prompting</i>，IO；<i>Chain-of-thought</i>，CoT；ReAct）。这意味着微调小型开源LMs的表现可能优于更强的商业LMs。由于微调的LMs不需要few shot examples，因此它们的推理变得更高效，尤其是对于迭代累积上下文的agent应用程序，发现推理时间减少了70%。</p><p>论文链接：</p><a class=" external" href="https://arxiv.org/abs/2310.05915" rel="nofollow noreferrer" target="_blank"><span class="invisible">https://</span><span class="visible">arxiv.org/abs/2310.0591</span><span class="invisible">5</span><span class="ellipsis"></span></a><p>代码链接：</p><a class=" external" href="https://fireact-agent.github.io/" rel="nofollow noreferrer" target="_blank"><span class="invisible">https://</span><span class="visible">fireact-agent.github.io</span><span class="invisible">/</span><span class="ellipsis"></span></a><h3><i>Learning From Failure: Integrating Negative Examples when Fine-tuning Large Language Models as Agents</i></h3><figure><img class="origin_image zh-lightbox-thumb" src="https://pic1.zhimg.com/v2-dcb28965da22bc169d95e9ad281c5638_1440w.jpg" /></figure><p>如图所示，该方法可以分为四个阶段，具体如下：(1) data collection, (2) data cleaning, (3) negative-aware reformatting, and (4) finetuning.</p><p>Data collection： 对于每项任务，我们都会获得初始问题和相应的基本事实答案作为种子数据。然后，我们使用LLM生成三次轨迹，每次都具有不同的温度（0.2、0.5和0.7）。这使我们能够在试验各种正负样本收集策略的同时，收集各种正样本。在本文中，我们使用GPT-3.5-1106来生成高质量的轨迹。</p><p>Data cleaning：某些负样本有利于微调模型，而其他样本可能有害。因此，数据清理在我们的方法中起着至关重要的作用。考虑到任务之间的可变性，我们采用了特定任务的数据清理标准和方法，在每个任务的实验设置中详细介绍了这些标准和方法。</p><p>Negative-aware reformatting：在agents调整过程中区分正样本和负样本有助于教模型区分成功和不成功的结果。我们将其定义为隐式对比学习，以帮助模型从成功和失败中学习。因此，在微调之前，我们将不同的提示连接到正轨迹和负轨迹，以将它们与模型区分开来。</p><p>Fine-tuning：如下图，绿色的部分会被计算在loss内，正负样本放在一个prompt里面。论文使用了LLaMa-2-Chat 7B/13B模型，训练了2个epoch，batchsize 64, 4xA100 GPU， DeepSpeed ZeRO 3 stage。</p><figure><img class="origin_image zh-lightbox-thumb" src="https://pic1.zhimg.com/v2-e237026cb004672b886db97462e986bc_1440w.jpg" /></figure><p>Inference:使用正样本的prompt进行推理即可。</p><p>论文链接：</p><a class=" external" href="https://arxiv.org/abs/2402.11651" rel="nofollow noreferrer" target="_blank"><span class="invisible">https://</span><span class="visible">arxiv.org/abs/2402.1165</span><span class="invisible">1</span><span class="ellipsis"></span></a><h3>Agent-FLAN: Designing Data and Methods of Effective Agent Tuning for Large Language Models</h3><figure><img class="origin_image zh-lightbox-thumb" src="https://pic2.zhimg.com/v2-f9c5e04c3a95d2b53dd97ad589b3cb5d_1440w.jpg" /></figure><p>上图显示的是Agent-FLAN和其他agent-tuning方法的比较，在论文中，作者有如下的三个发现：</p><ul><li>大多数智能体训练数据都与格式遵循（format following）和一般推理(general reasoning)纠缠在一起，导致与模型的原始预训练语言领域，即自然会话发生了显著偏离。</li><li>通过沿着基本能力方面明确地分解训练数据，每个损失都表现出不同的收敛曲线，表明LLM的agents任务所需能力的学习速度不同。</li><li>现有的方法主要集中在专业agent能力上，忽略了幻觉效应(如下图a是格式幻觉，b是action幻觉)在模型输出中的普遍性和重要性。</li></ul><figure><img class="origin_image zh-lightbox-thumb" src="https://pic4.zhimg.com/v2-2fdadc287b80fde196e9e29510a434ff_1440w.jpg" /></figure><p>然后作者是怎么改进的呢？由于预训练没有出现过json数据以及ReAct的数据，因此ReACT做了如下图的转换，把数据变成了聊天的形式。</p><figure><img class="origin_image zh-lightbox-thumb" src="https://pic1.zhimg.com/v2-51a620989acec834cd74ea9bdfc40c40_1440w.jpg" /></figure><p>推理和理解是最有益的混合，然后是检索和指令：当推理和理解数据的比例减少到50%时，最终性能分别下降1.1/0.3分。通过减少检索和指令跟随的数量，性能几乎不会受到影响，甚至会得到改善。这样的现象也启发我们缩小混合权重搜索空间，并根据每种能力的损失曲线大大减少训练token。另外，为了克服幻觉，我们插入了两种不同类型的负样本：（1）没有提供工具，用户对工具的查询请求（2）提供的工具，用户对正常对话的查询请求。通过明确的监督，不仅教会模型how，而且教会模型何时充当agent。</p><p>论文链接：</p><a class=" external" href="https://arxiv.org/abs/2403.12881" rel="nofollow noreferrer" target="_blank"><span class="invisible">https://</span><span class="visible">arxiv.org/abs/2403.1288</span><span class="invisible">1</span><span class="ellipsis"></span></a><p>代码链接：</p><a class=" external" href="https://github.com/InternLM/Agent-FLAN" rel="nofollow noreferrer" target="_blank"><span class="invisible">https://</span><span class="visible">github.com/InternLM/Age</span><span class="invisible">nt-FLAN</span><span class="ellipsis"></span></a><h3>AgentOhana: Design Unified Data and Training Pipeline for Effective Agent Learning</h3><p>AgentHana的工作流程如下。同构多轮数据格式旨在整合来自不同数据源的异构轨迹。<i>AgentRater</i>然后评估和过滤agent轨迹。最后，流式数据加载器能够集成各种数据集，并将数据随机馈送到分布式训练过程中。</p><figure><img class="origin_image zh-lightbox-thumb" src="https://pic1.zhimg.com/v2-ef4a63164328ded6045e9f6a7dd4f594_1440w.jpg" /></figure><p>该工作流构建了一个同质的JSON字典格式来封装每个轨迹的所有相关内容。具体来说，我们的格式包含了所有重要元素，如存储初始用户查询的<i>user query</i>、识别相应模型的<i>model name</i>以及记录可用模型性能分数的<i>score</i>。这些元素可用于区分模型，并有助于开发用于成对样本的训练，如DPO、<i>self-reward </i>和AI feedback LLM。此外，将辅助轨迹信息或特定注释保存到<i>other information中</i>，为进一步分析或模型改进计划提供参考。</p><p>为了增强对多回合agent轨迹信息的保存和分析，提出了一个结构化的步骤定义，该定义捕获了每个交互回合的细节。一个步骤包括三个主要组成部分：intput、output和next observation。input组件整合了当前提示和过去交互的历史记录，作为交互的综合上下文。output组件捕获模型的预测，详细说明其决策和计划。next observation组件记录环境的反馈，这对反馈回路和系统自适应至关重要。</p><figure><img class="origin_image zh-lightbox-thumb" src="https://pic1.zhimg.com/v2-b10d1f2966c6380b52963c07af8d9644_1440w.jpg" /></figure><p><i>AGENTRATER： agent轨迹代表一个复杂的数据子集，不同于一般和直接的训练数据。虽然像Alpaca这样的数据集以单回合为特征，而LMSYS Chat包括平均两回合左右的对话，但这些通常包含更简单的交互模式。DialogStudio确实提供了多回合对话的例子，这些例子主要局限于用户和系统之间的对话，缺乏与外部环境的互动。除了agents轨迹复杂度高以外，agent轨迹的质量评估也是一个挑战，因为reward反应的是结果，而不是agents的轨迹生成的质量。为了解决这些问题，AgentRater</i>的prompt template，其中开源模型（例如，Mistral）或封闭世界API（例如，ChatGPT）将根据标准对整个agent轨迹进行评级，然后分配0-5的分数。</p><figure><img class="origin_image zh-lightbox-thumb" src="https://pic4.zhimg.com/v2-0635976ad62c7578cf9a990f0ad8aacf_1440w.jpg" /></figure><p>模型：<i>XLAM-v0.1，基座模型是Mixtral-8x7B-Instruct-v0.1，8 Nvidia H100 GPUs， QLoRA进行训练。</i></p><p><i>论文链接：</i></p><a class=" external" href="https://arxiv.org/abs/2402.15506" rel="nofollow noreferrer" target="_blank"><span class="invisible">https://</span><span class="visible">arxiv.org/abs/2402.1550</span><span class="invisible">6</span><span class="ellipsis"></span></a><p>代码链接：</p><a class=" external" href="https://github.com/SalesforceAIResearch/xLAM" rel="nofollow noreferrer" target="_blank"><span class="invisible">https://</span><span class="visible">github.com/SalesforceAI</span><span class="invisible">Research/xLAM</span><span class="ellipsis"></span></a><h3><i>Agent LUMOS: Unified and Modular Training for Open-Source Language Agents</i></h3><p>LUMOS的总体框架如下。LUMOS使用56K高质量的训练标注进行训练。我们提出了两种智能体训练和推理公式，LUMOS-O和LUMOS-I。LUMOS-O是一种高效的公式（formulation），能够进行一次推理；LUMOS-I是一种自适应公式（formulation），可帮助agent根据执行反馈进行灵活规划。我们展示了A-OKVQA和Mind2Web中的两个LUMOS-I运行示例。</p><figure><img class="origin_image zh-lightbox-thumb" src="https://pic1.zhimg.com/v2-587df80aa3167e29743437548e772774_1440w.jpg" /></figure><p>对于各种复杂的交互式任务，常见的解决方案包括：（1）将任务分解为一系列子目标，（2）将子目标转换为具体行动，（3）执行这些行动。该过程对应于planning、grounding，execution模块。</p><p><i>Planning Module (PM).该模块旨在将复杂的任务分解为一系列用自然语言表示的高级子目标。例如，诸如“她手中的设备来自哪个国家？”这样的多模式问题需要两个子目标：（1）识别她手中设备的品牌；（2） 回答设备品牌的国家/地区。所设计的子目标有助于以可解释和工具不可知的方式将复杂任务分解为低级操作。计划模块旨在方便调试和学习新的任务计划，而不会影响其他模块。</i></p><p><i>Grounding Module (GM).该模块将PM生成的高级子目标转换为低级可执行操作。例如，GM将子目标“查询Lowell Sherman的生存期”转换为一个或多个操作，如KnowledgeQuery(LowellSherman)和QA([R2], Query:“What is theliving period of Lowell Sherman?”)。这里，R2指的是先前检索到的可能有助于回答查询的知识。接地模块可以很容易地进行定制，以在不影响计划模块的情况下学习新动作。</i></p><p><i>Execution Module (EM). 执行模块（EM）是一个实现接地模块生成的动作并获得执行结果的程序。它部署了各种现成的工具，包括API、神经模型和虚拟模拟器（virtual simulations）。例如，执行模块可以调用维基百科或谷歌搜索API来完成KnowledgeQuery操作。</i></p><p>训练：把所有的多轮标注的数据送入decoder-only模型，对于user promp tokens使用了binary masking，不计算loss，模型使用的是LLAMA-2-7B和LLAMA-2-13B作为planning和grounding模型，并且使用了CoT和Agent training两种方法。</p><p>代码链接：</p><a class=" external" href="https://github.com/allenai/lumos" rel="nofollow noreferrer" target="_blank"><span class="invisible">https://</span><span class="visible">github.com/allenai/lumo</span><span class="invisible">s</span><span class="ellipsis"></span></a><p>论文链接：</p><a class=" external" href="https://arxiv.org/html/2311.05657v2" rel="nofollow noreferrer" target="_blank"><span class="invisible">https://</span><span class="visible">arxiv.org/html/2311.056</span><span class="invisible">57v2</span><span class="ellipsis"></span></a><p class="ztext-empty-paragraph"><br /></p><h3><i>LLMs in the Imaginarium: Tool Learning through Simulated Trial and Error</i></h3><p><i>该论文提出了STE(simulated trial and error)，STE的示例如下，在探索（exploration）阶段，LLM与工具交互，并通过试错逐步收集工具使用经验。具体而言，a）在每次试验中，LLM想象</i>与目标工具相关的看似合理的场景，与该工具迭代交互以完成用户查询，并最终对试验进行自我反思；b） 由最近的试验轨迹组成的短期记忆鼓励从细粒度的成功和失败中学习，并更深入地探索API；c） 对过去粗粒度试错经验的长期记忆可以在很长的时间范围内保持渐进式学习。在开发（exploitation）阶段，探索经验被提炼成一组工具使用示例，用于ICL或微调。</p><figure><img class="origin_image zh-lightbox-thumb" src="https://pic3.zhimg.com/v2-64ca53c406fbb9cb5ae59e1792956b3e_1440w.jpg" /></figure><p>作者在实践中有如下的发现：</p><ul><li>现有的LLM远未达到可靠的工具使用性能：GPT-4（OpenAI，2023）的正确率为60.8%，而专门针对工具使用进行微调的ToolLLaMAv2（Qin et al.，2024）仅为37.3%。</li><li>事实证明，在ICL和微调设置下，STE对于使用工具增强LLM非常有效。STE将Mistral-Instrument-7B的工具使用能力提高到76.8%（绝对值提高了46.7%），使其优于ICL的GPT-4。</li><li>在实践中，不断添加新的工具。微调（有或没有STE）带来了灾难性遗忘的挑战，学习新工具可能会导致LLM失去现有的工具使用能力或通用语言能力。我们证明，一个简单的经验回放策略可以在很大程度上缓解这个问题，使模型能够不断学习新工具，同时保留其先前获得的技能。</li></ul><figure><img class="origin_image zh-lightbox-thumb" src="https://pic3.zhimg.com/v2-3fee0b0d0bf3c4cf39bfdb31a6d276a6_1440w.jpg" /></figure><p>训练：使用了LLama-2-Chat-7B和Mistral-Instruct-7B两个模型，<i>4 NVIDIA A100/A6000 GPUs，2048 tokens，为了克服灾难性遗忘，使用了continue training。</i>作者也说了这个工作的局限性：</p><p>1.<i>Iterative improvement.目前使用强模型进行探索（exploration），使用较小的弱模型进行开采（exploitation）。勘探开发（exploration-exploitation）也可以像之前的工作一样反复进行，随着模型能力的提高，对强大模型的依赖可以逐渐减少（例如，仅作为评估者）。</i></p><p>2.<i>Compositional tool use &amp; planning. 在工具使用的背景下，另一个重要的能力是编写/规划多个工具调用来完成复杂的查询，这是我们在这里关注的正交方向。最近的工作表明，LLM的核心能力是通过预训练编码和引出的，而不是通过微调/对齐注入的，这表明可能不需要大量的数据准备来适应复杂的工具使用，这与我们的重点不同，在我们的重点中，随着从工具端获得信息，总是需要广泛的学习和探索。</i></p><p>3.<i>Larger memory capacity beyond context limit. augmented memory的容量受到LLM的上下文长度的限制。有不同种类的方法可以用于进一步放大存储器，例如使用额外的检索模块或具有更多层次/压缩的memory表示。</i></p><p>4.<i>Tool unlearning? 在我们探索不断学习新工具的同时，遗忘问题也很重要，因为工具可能会不断卸载/过时。知识遗忘通常是一个具有挑战性的问题，可能有一些特定的设计支持更容易的工具遗忘，例如ToolkenGPT（Hao et al.，2021），它允许即插即用的适应，同时实现大规模示例的学习。</i></p><p>5.<i>Limitations of example-based fine-tuning. 最后，基于示例的工具学习方法也存在固有的局限性，特别是当不使用工具时，仅通过positive的工具使用示例来教授模型的困难。改善这一问题的一些潜在方法是结合negative examples（例如，使用contrastive objectives）或将API的这些部分与基于例子的训练结合起来。</i></p><p><i>论文链接：</i></p><a class=" external" href="https://arxiv.org/abs/2403.04746" rel="nofollow noreferrer" target="_blank"><span class="invisible">https://</span><span class="visible">arxiv.org/abs/2403.0474</span><span class="invisible">6</span><span class="ellipsis"></span></a><p>代码链接：</p><a class=" external" href="https://github.com/microsoft/simulated-trial-and-error" rel="nofollow noreferrer" target="_blank"><span class="invisible">https://</span><span class="visible">github.com/microsoft/si</span><span class="invisible">mulated-trial-and-error</span><span class="ellipsis"></span></a><h3><i>Trial and Error: Exploration-Based Trajectory Optimization for LLM Agents</i></h3><p><i>Exploration-based Trajectory Optimization (ETO)概览如图所示，</i>从通过<i>behavioral cloning</i>训练的基本LLM agent开始，我们的方法允许agent迭代收集失败轨迹，并通过不断从对比的失败-成功轨迹对中学习来更新其策略。具体地址，首先使用基于SFT的行为克隆来构建基本agent，在探索阶段，该基础agent与目标环境交互以执行一组给定任务并接收来自环境的反馈。我们从基本agent中收集失败的轨迹，并将它们与之前为这些任务收集的专家轨迹配对。随后，我们应用DPO损失来利用这些对比轨迹对微调LLM策略，从而不断改进agent。ETO可以通过收集以前ETO调优agent的失败案例扩展到多轮。</p><figure><img class="origin_image zh-lightbox-thumb" src="https://pic3.zhimg.com/v2-30a31bce1cd0710584b8686b37db72ca_1440w.jpg" /></figure><p>在behavior cloning阶段使用的是auto-regressive的损失：</p><figure><img class="origin_image zh-lightbox-thumb" src="https://pic4.zhimg.com/v2-afa62273a747969380e6844303266473_1440w.jpg" /></figure><p>从失败的轨迹中学习的损失是：</p><figure><img class="origin_image zh-lightbox-thumb" src="https://pic3.zhimg.com/v2-402bb26d043df828396719519e1b67f6_1440w.jpg" /></figure><p>训练的算法流程如下，训练过程可以在一个迭代的探索-训练循环中进行表述。在ETO的探索阶段，agent探索环境以收集故障轨迹。在训练阶段，agent从“failure-success”轨迹对中学习对比信息，以更新策略。</p><figure><img class="origin_image zh-lightbox-thumb" src="https://pic2.zhimg.com/v2-46bfc8c69352957a7be5029279151135_1440w.jpg" /></figure><p>以下是一个示例，感受一下SFT微调出来的agent和ETO调出来的agent的差别：</p><figure><img class="origin_image zh-lightbox-thumb" src="https://pic4.zhimg.com/v2-7fa2ba981b95e4182966e0ea5a65d10f_1440w.jpg" /></figure><p>局限性：1.ETO通过假设agent从一开始就产生错误的动作，简化了<i>failure-success</i>轨迹的比较。然而，在现实情况下，agent可能会从某个中间步骤开始执行不正确的操作。如果我们能够识别agent何时进行了糟糕的动作（比如a3），那么我们应该在a&gt;3时收集剩余动作的专家轨迹。不幸的是，大多数当前环境都不包含这样的信息，这使得进行行动层面或流程层面的奖励建模具有挑战性。一个潜在的解决方案是使用GPT-4来识别不良动作并构建细粒度的对比轨迹数据。2.这项工作主要集中在为特定的agent任务开发专门的LLMagent，对强广义agent的构建进行了有限的探索。</p><p>论文链接：</p><a class=" external" href="https://arxiv.org/abs/2403.02502" rel="nofollow noreferrer" target="_blank"><span class="invisible">https://</span><span class="visible">arxiv.org/abs/2403.0250</span><span class="invisible">2</span><span class="ellipsis"></span></a><p>代码链接：</p><a class=" external" href="https://github.com/Yifan-Song793/ETO" rel="nofollow noreferrer" target="_blank"><span class="invisible">https://</span><span class="visible">github.com/Yifan-Song79</span><span class="invisible">3/ETO</span><span class="ellipsis"></span></a><h2>长序列相关的论文</h2><p>现在长序列要么就是简单粗暴的搞数据集，买更多的卡，要么就是做一些结构的调整，前者太耗费资源了，这里选了几篇有代表性的探索一下长序列是怎么做的。</p><h3>Yi: Open Foundation Models by 01.AI</h3><p>注意力机制使用的是Grouped-Query Attention(GQA)，Yi-6B，Yi-34B都使用了GQA（减少训练和推理的成本），激活函数使用的是SwiGLU，post-attention layer把4h的activation大小降低到了8/3h，Position Embedding则使用的是RoPE，然后base 模型使用的是4k上下文长度进行训练，然后再10B 的长token上做post train。预训练的数据预处理使用的是层级过滤，去重，去噪等等，对齐策略用的是DPO，训练框架使用的是DeepSpeed和Megatron。</p><p>启发：这个Yi模型的长序列的训练和结构设计值得借鉴，比如使用GQA，在4k长度的上下文进行训练，再在10B长的token做post train。</p><p>论文链接：</p><a class=" external" href="https://arxiv.org/abs/2403.04652" rel="nofollow noreferrer" target="_blank"><span class="invisible">https://</span><span class="visible">arxiv.org/abs/2403.0465</span><span class="invisible">2</span><span class="ellipsis"></span></a><h3>Mixtral of Experts</h3><p>Mixtral 8x7B是一个具有开放权重的稀疏专家混合模型（SMoE），在Apache 2.0下获得许可。Mixtral在大多数基准测试中都优于Llama 2 70B和GPT-3.5。由于Mixtral只对每个token使用其参数的子集，因此在低批量时可以实现更快的推理速度，在大批量时可以获得更高的吞吐量。Mixtral是一个稀疏的专家混合网络。它是一个解码器的模型，其中前馈块从一组8个不同的参数中选取。在每一层，对于每一个token，路由器网络选择其中两个组（“专家”）来处理token，并将其输出相加。这项技术增加了模型的参数数量，同时控制了成本和延迟，因为模型每个令牌只使用总参数集的一小部分。</p><figure><img class="origin_image zh-lightbox-thumb" src="https://pic3.zhimg.com/v2-717ae396a42c619be2e9fb9d30632e5e_1440w.jpg" /></figure><p>启发：这个对于小模型的结构设计很有帮助，降低了参数量，提升了推理速度。</p><p>论文链接：</p><a class=" external" href="https://arxiv.org/abs/2401.04088" rel="nofollow noreferrer" target="_blank"><span class="invisible">https://</span><span class="visible">arxiv.org/abs/2401.0408</span><span class="invisible">8</span><span class="ellipsis"></span></a><h3>InternLM2 Technical Report</h3><figure><img class="origin_image zh-lightbox-thumb" src="https://pic4.zhimg.com/v2-6bc1a02d2ca48be3b0cef010c909a6db_1440w.jpg" /></figure><p>如图所示，修改了模型结构实现方式（本质上没变），使得tp并行更方便，使用了Grouped-Query Attention(GQA)，可以在提升长上下文的推理速度，减少显存的消耗。</p><p>对于长序列的数据的处理：<i>Data Filtering Pipeline</i>旨在过滤出低质量的长文本数据。它包括三个阶段：a）长度选择，一个基于规则的过滤器，选择超过32K字节的数据样本；b） 统计过滤器（<i>Statistical Filters</i>），利用统计特征来识别和删除异常数据；c） 困惑过滤器（<i>Perplexity filters</i>），利用困惑的差异来评估文本片段之间的连贯性，过滤出具有分散注意力的上下文的样本。注意，为长上下文训练选择的所有数据都是标准预训练语料库的子集，这意味着在预训练期间将至少学习两次长上下文数据。</p><figure><img class="origin_image zh-lightbox-thumb" src="https://pic2.zhimg.com/v2-2ebf6c873ae32121e0e4eaa3fa2b00d1_1440w.jpg" /></figure><p>预训练流程：用于预训练1.8B、7B和20B模型的token总数在2.0T到2.6T之间，预训练过程由三个不同的阶段组成。在第一阶段，使用了长度不超过4k的预训练语料库。在第二阶段，包含了50%的长度不超过32k的预训练语料库。在第三阶段，我们使用了特定能力的增强数据。在每个阶段，混合了英文、中文和代码的数据。</p><p>为了做长序列的训练，RoPE的基数从50000调整到了100 0000，使用的是<i>flash attention和InternEvo，长序列的训练性能只下降了40%。</i></p><p><i>Long-Context Finetuning：使用了book data和github code DS-1000的长序列数据。</i></p><figure><img class="origin_image zh-lightbox-thumb" src="https://pic4.zhimg.com/v2-9ba27695caee8b561baf3ebcc84c12d3_1440w.jpg" /></figure><p><i>Tool-Augmented LLMs：</i></p><p><i>General Tool Calling：采用了ChatML格式的修改版本，通过引入“environment”角色来启用通用工具调用。这种修改在聊天场景中共享相同的格式，但在采用agent时为模型提供了更清晰的信号。此外，我们定义了两个特定的关键字来支持人工智能agent的不同用途，即代码解释器（&lt;|interpret|&gt;）和外部插件（&lt;|plugin|&gt;）。这使我们能够采用统一的流媒体格式，可以处理各种类型的插件扩展和人工智能环境，同时与通用聊天兼容。 下图是一个示例格式：</i></p><figure><img class="origin_image zh-lightbox-thumb" src="https://pic4.zhimg.com/v2-0d278955477ee0e40c92501d4e2d511f_1440w.jpg" /></figure><p><i>Code Interpreter： 还通过使用工具学习中描述的相同模式，将Python代码解释器视为一种特殊工具，增强了InternetLM2 Chat通过代码解释器解决数学问题的能力。我们采用了推理交织编码（RICO）策略，并以迭代的硬示例挖掘方式构建数据，如InternLM Math中所述。</i></p><p><i>论文链接：</i></p><a class=" external" href="https://arxiv.org/abs/2403.17297" rel="nofollow noreferrer" target="_blank"><span class="invisible">https://</span><span class="visible">arxiv.org/abs/2403.1729</span><span class="invisible">7</span><span class="ellipsis"></span></a><h3>LongRoPE: Extending LLM Context Window Beyond 2 Million Tokens</h3><figure><img class="origin_image zh-lightbox-thumb" src="https://pic1.zhimg.com/v2-84cc4d0b47c733d0ab7a6f0130c62028_1440w.jpg" /></figure><p>LongRoPE基于三个关键创新。首先，LongRoPE充分利用了位置插值中的多维非均匀性。它根据标记位置为每个RoPE维度的RoPE旋转角度确定有效的重新缩放因子。由于识别重缩放因子的搜索空间随着目标扩展比呈指数级扩展，LongRoPE引入了一种具有两种优化技术的进化搜索算法，以提高搜索效率。</p><p>然后，LongRoPE利用一种高效、渐进的扩展策略来实现2048k上下文窗口，而无需对长度极长的文本进行直接微调，而这些文本很少且几乎不可用。该策略首先在预训练的LLM上搜索256k的长度，并在此长度下对其进行微调。然后，由于我们的非均匀位置插值允许在非微调设置中进行8×扩展，我们在微调扩展LLM上进行第二次搜索新的RoPE重缩放因子。这最终实现了LLaMA2和Mistral的2048k上下文窗口。</p><p>最后，为了缓解原始（较短）上下文窗口的性能下降，LongRoPE继续调整扩展LLM上的RoPE重新缩放因子。与从256k扩展到2048k类似，我们使用搜索算法在256k微调LLM上向下扩展到4k和8k上下文窗口，以减少位置插值。在推理过程中，如果序列长度小于8k，我们用搜索到的重缩放因子更新RoPE。</p><p>论文链接：</p><a class=" external" href="https://arxiv.org/abs/2402.13753" rel="nofollow noreferrer" target="_blank"><span class="invisible">https://</span><span class="visible">arxiv.org/abs/2402.1375</span><span class="invisible">3</span><span class="ellipsis"></span></a><h2>多智能体训练</h2><h3>DebateGPT: Fine-tuning Large Language Models with Multi-agent Debate Supervision</h3><p>DebateGPT的框架如下，（左）从Alpaca数据集中采样5K指令，并使用多智能体辩论的修改版本来改进生成的响应。这包括询问agents的置信度得分，总结其他agents的回答，并清理最终答案。（右）收集问答对，并使用OpenAI FineTuning API对GPT-3.5进行微调。</p><figure><img class="origin_image zh-lightbox-thumb" src="https://pic4.zhimg.com/v2-f8ebe7cd2f66cda1794d581b5b7ed797_1440w.jpg" /></figure><p>直接微调多智能体模型有一定的挑战，首先，尽管已经注意到增加agents和debate轮次的数量可以提高准确性，但大型语言模型中对最大上下文窗口的内在约束阻碍了agents数量和辩论持续时间的可扩展性。其次，目前的多智能体辩论配置也可能得出错误的结论，主要是因为智能体缺乏有效的机制来适当评估和整合其参与者的见解和观点。最后，产生的答案通常带有无关的以辩论为中心的短语，如“在最后一轮”或“其他agents”，这些短语对模型的改进（refinement）没有帮助。鉴于这些限制，作者提出了三个具体的修改来应对每一个挑战，从而提高多智能体辩论范式的有效性。</p><p>Summarization: 为了处理预训练语言模型中对最大上下文窗口的约束，引入了一个摘要模型，该模型在每轮辩论中总结来自其他LLM agent的响应。具体来说，在第r轮辩论中，首先收集上一轮辩论中所有其他agents的回复。对于 agent ak，将收集的响应Ar−1’k={a1，···ak−1，ak+1，··，an}发送到摘要模型，以合并这些响应，并提供给定问题的简短而清晰的答案，其中n是agents的数量。在实验中，使用另一个GPT-3.5作为summarizer，并使其与辩论agents不同。这一变化使我们能够使用更多的agents和辩论回合来获得更准确的数据，用于微调下游模型。</p><p>Confidence Scores: 为了增强主体有效评估和整合自身和其他主体反应的能力，在多主体辩论程序中集成了置信度评分机制。每个参与的agent都会被提示生成一个置信度分数，该分数在1到100的范围内。该分数反映了agents对其自身反应的准确性和可靠性的确定程度。置信度较低的答案对最终答案的影响较小。这些置信度得分有助于更统一的评估，同时整合来自不同agents的见解。</p><p>Cleaning: 为了提高最终答案的一致性，并删除通过多智能体辩论生成的无关文本，引入了一个清理模型，用于清理上一轮辩论的响应。还在提示中为该模型提供了四个GPT-4响应示例，以帮助它生成简洁一致的文本。使用单独的GPT-3.5作为清洁器，使其与debate agent和摘要模型不同。然后将清理后的数据用于微调下游模型。</p><p>下面是一个示例，取自AlpacaEval数据集，GPT-3.5算错了，DebateGPT-3.5和GPT-4算对了，论文还给了很多示例，有需要可以多看看论文。</p><figure><img class="origin_image zh-lightbox-thumb" src="https://pic1.zhimg.com/v2-c78f5ead176405c8f38805bba997e6e4_1440w.jpg" /></figure><p>论文链接：</p><a class=" external" href="https://openreview.net/forum?id=ChNy95ovpF" rel="nofollow noreferrer" target="_blank"><span class="invisible">https://</span><span class="visible">openreview.net/forum?</span><span class="invisible">id=ChNy95ovpF</span><span class="ellipsis"></span></a><h3><i>CMAT: A Multi-Agent Collaboration Tuning Framework for Enhancing Small Language Models</i></h3><p>如下图，用户将任务分配给assistant，助手根据短期和长期记忆生成SQL命令：短期记忆提供轨迹历史中的即时上下文，而自我反思输出存储为长期记忆。在环境中执行SQL命令之前，检查程序会验证这些命令的正确性。</p><figure><img class="origin_image zh-lightbox-thumb" src="https://pic2.zhimg.com/v2-d426f76df74c8546adb8070664ceb325_1440w.jpg" /></figure><p>论文工作重点是设计和实现多智能体LLM调整框架，通过不同agent角色（即User、Assistant、Checker）之间的协作通信和任务完成，提高复杂系统中的决策质量、可控性和效率。它强调使用基于预先训练的数据集（如AgentBench）的监督微调技术，如LoRA和PTuning，并通过环境交互和记忆更新进行进一步调整，类似于从人类反馈中强化学习（RLHF）的过程。有兴趣的可以看看下面的算法流程，其实就是把能力进行了拆分，通过协作完成一些复杂的任务。</p><figure><img class="origin_image zh-lightbox-thumb" src="https://pic1.zhimg.com/v2-c21aac4825073e4de2eeae452769dc18_1440w.jpg" /></figure><p>论文链接：</p><a class=" external" href="https://arxiv.org/abs/2404.01663" rel="nofollow noreferrer" target="_blank"><span class="invisible">https://</span><span class="visible">arxiv.org/abs/2404.0166</span><span class="invisible">3</span><span class="ellipsis"></span></a><h2>memory相关的论文</h2><p>memory模块主要是面临的context length超出的问题，需要对context windows进行一定的策略优化，其中一个就是memory retrieval的功能，retrieval的效果对整个模型的性能有很大的影响，因此也有一些微调retrieval模型的方法，有兴趣可以参考下面的文章：</p><p class="ztext-empty-paragraph"><br /></p><a class="internal" href="https://zhuanlan.zhihu.com/p/684899047">是念：2024年大模型最快的应用落地技术-Embedding向量优化Matryoshka，Nomic Embed，JINA EMBEDDINGS 2，ColBERTv2，BGE M3，GRIT</a><h2>总结</h2><p>本文主要是对Agents的一些能力怎么优化做了一些探索，主要是数据层面，训练层面的一些认知，包括agents的轨迹，负样本轨迹，fine tuning，对比学习，context windows的memory retrieval的优化，长序列的优化，包括预训练，SFT的长序列优化技巧，MoE架构提升速度等等，目前agents tuning还是一片蓝海，欢迎大家来发挥自己的优势完善这个方向。</p>
]]></content:encoded>
<pubDate>Thu, 25 Apr 2024 07:12:49 GMT</pubDate>
</item>
<item>
<title>凡心赞同了文章: RAG与Long-Context之争—没必要争</title>
<link>https://zhuanlan.zhihu.com/p/688983758</link>
<guid>https://zhuanlan.zhihu.com/p/688983758</guid>
<content:encoded><![CDATA[
<div> RAG, Long-Context, 检索, 大模型, 计算资源<br />
<br />
总结: Liu Cong NLP讨论了RAG与Long-Context之争，指出它们并不冲突，分别是粗排和精排。 RAG依赖检索系统从数据库中筛选相关内容，而Long-Context将所有文本输入到大模型中。随着大模型支持长度增长，RAG可用性提高。但长内容需求可能带来成本问题。是否需要全文Cache也需谨慎考虑。最终，智能仍取决于模型本身。不要极端，世界非非黑即白，关注NLP工作站获取更多信息。 <div>
<h2>写在前面</h2><p>大家好，我是刘聪NLP。</p><p>随着大模型可以支持的上下文(Context)长度越来越长，网上（好几个群里都在聊这个话题，也来聊几句）竟然出现了RAG与Long-Context之争，是真没必要。。。主要是两者不冲突，并不是非A即B、非B即A的关系。</p><p>个人观点：<b>如果类比做检索系统的话，RAG应该算作粗排，而Long-Context可以算是精排。</b> RAG的本质是通过用户问题从数据库/知识库中找到相关片段内容，再利用大模型查找或总结出答案。Long-Context的本质是将所有文本内容全部灌入到大模型中，用户问一个问题，再利用大模型查找或总结出答案。</p><p>本质区别在于外部知识如何给到大模型，以及给多少到大模型。</p><p>这也是大家的所“争”之处，因为大模型可以接受的长度越长，那么检索的重要性就会降低，<b>对检索效果的依赖就会降低</b>，也就是为什么有人持观点，未来没有RAG，只有Long-Context。但大模型随着输入长度越长，<b>KV Cache所占资源就越多</b>，成本也会剧增，这也是为什么有人持观点，未来也会有RAG。</p><p>那么未来是多远呢？如果未来只是5年，我觉得RAG一定会存在；如果未来是AGI，那么也许不需要RAG。</p><h2>为什么RAG是粗排，Long-Context是精排</h2><p><b>从计算量角度来看</b>，目前RAG是靠检索系统来进行相关内容过滤，一般采用ES、向量匹配等方法，可以理解计算量较小，也就是文本之间交互较少；而Long-Context相当于用户Query与文本交互时，利用了整个大模型参数，即通过Transformer的每一层Attention来定位相关文本片段。</p><p><b>从文本选择角度来看</b>，Long-Context可以类比成人为已经确定了问题的相关内容，而RAG需要借助检索手段来确定问题所涉及的相关内容，有一定的信息损失。</p><h2>大模型上下文支持的越长，对RAG越友好</h2><p>RAG是目前大模型落地最快速、最有效、最安全的方式。但RAG依然存在很多问题，例如：文档在切段的过程中会将文本原始语义切割、检索匹配存在准确率的问题。但当大模型Context可以支持更长时，RAG可以对文档不切割甚至少切割，检索可以召回更多内容，Top10不够用我们就来Top100。</p><p>那么，</p><ul><li>如果你站边RAG，那么大模型即使支持了Long-Context，它也是大模型；</li><li>如果你站边Long-Context，那么RAG就是在大模型支持无限Context前的过滤手段。</li></ul><p>争与不争的核心问题在于数据库/知识库中内容长度是否会超出大模型所能支持的最大长度。</p><p>但检索越好、大模型支持长度越长，对于目前大模型落地、大模型可持续发展都是必不可少的，<b>we are famlily</b>！！！</p><h2>Long-Context会打击到一些RAG的场景，但不是全部</h2><p>不可否认，Long-Context会打击到一些RAG的场景。</p><p>主要就是数据库/知识库内容不超过大模型最大长度的场景，例如：涉及场景中仅有几篇文档作为参考资料，完全不需要检索，<b>直接暴力解法都给大模型就完事儿了</b>。</p><p>但如果超出大模型最大长度的内容，不用检索过滤，请问阁下如何应对。</p><p>很多极端的人说可能就不存在领域大模型了，但仔细想想，即使Gemini支持1M Token、Kimi-Chat支持200w字，也就是400多个PDF(假设一个PDF平均有5k字)，垂直领域(不抬杠，这里特指一些大领域)数据何至于此呀。</p><p>还有，在场景中针对权限的判定，没必要用大模型来判断，完全可以进行域隔离就行，那么也无需将所有文本给到大模型（节省计算资源）。</p><p>PS: 说个题外话，貌似人一辈子也就产生0.3B左右的Tokens（之前群友讨论过这个话题），是不是当模型支持最大长度达到300M时，可以快速复制一个人了。不用微调，直接给库。</p><h2>你能部署1M Token的大模型服务吗</h2><p>KV Cache的计算方式是4*Batch Size*模型层数L*注意力维度H*KV Cache长度，要硬支持1M长度的话，确实有些可怕。</p><p>当然目前有一些优化方法，滑动窗口、Cache量化等等等（欢迎大家补充），但即使这样由于大模型参数规模较大，显存占用也是很可怕的。并且滑动窗口貌似与感觉与retrieval差不多，都是有损失的。那么若可以接受损失，那么为什么不能接受retrieval。</p><p>当然前面聊的内容都没有考虑成本问题，但回归现实，请问有哪些厂家有能力部署支持1M Token的大模型模型服务，或者说部署这样一个模型的性价比如何？</p><p>对于ToC，大模型服务由大模型公司进行部署，成本由大模型公司承担，那么考虑的就是如何盈利的问题（当然可以为了梦想不考虑）。</p><p>对于ToB，会遇到一个很现实的问题，用了这么多显卡，能来什么？Long-Context相较于RAG来说，能否带来精度的提高，可以带来多少提高，是否会增加延迟，相同显卡情况下部署更大的模型+RAG是否更有性价比。</p><p>如果你遇到一个1张T4部署大模型的需求，就知道显示是多么残酷。<b>并不是抬杠，只是我在路上走，目标活下去。</b></p><h2>如果你相信AGI，Cache所有文本就不是梦</h2><p>在技术飞速发展的时代，那么未来有一天，用较小的代价来Cache所有的文本也许不是梦。</p><p>但真的有必要吗？</p><p>Long-Context和RAG本质是都是让大模型找到更好的答案，真正智能还是要靠模型本身。接受更长的上下文可以侧面反应模型的智能，但模型的智能并不仅仅是接受更长的上下文。</p><h2>写在最后</h2><p>做人不用太极端，世界上也并非是非黑即白，条条大路都通AGI。</p><hr /><p>欢迎多多关注公众号「NLP工作站」，<a class=" wrap external" href="https://mp.weixin.qq.com/s/oqJZsxHgs6qDuYHQB6KyJA" rel="nofollow noreferrer" target="_blank">加入交流群</a>，交个朋友吧，一起学习，一起进步！</p><p>我们的口号是“生命不止，学习不停”！</p><p><b>往期回顾</b></p><p><a class="internal" href="https://zhuanlan.zhihu.com/p/686515415">刘聪NLP：自我蒸馏方法-减轻大模型微调过程中的灾难性遗忘</a></p><p><a class="internal" href="https://zhuanlan.zhihu.com/p/686042639">刘聪NLP：Yi技术报告细节分享</a><br /><a class="internal" href="https://zhuanlan.zhihu.com/p/679354000">刘聪NLP：大模型增量预训练新技巧-防止模型灾难性遗忘</a><br /><a class="internal" href="https://zhuanlan.zhihu.com/p/676589001">刘聪NLP：如何提高LLMs的文本表征(Text Embedding)能力?</a><br /><a class="internal" href="https://zhuanlan.zhihu.com/p/675928711">刘聪NLP：DEITA-大模型指令微调的数据高效筛选方法</a><br /><a class="internal" href="https://zhuanlan.zhihu.com/p/673405755">刘聪NLP：1-2B参数规模大模型使用心得及模型汇总</a><br /><a class="internal" href="https://zhuanlan.zhihu.com/p/671183709">刘聪NLP：大模型微调技巧 | 高质量指令数据筛选方法-MoDS</a><br /><a class="internal" href="https://zhuanlan.zhihu.com/p/662947944">刘聪NLP：大模型下载使我痛苦</a><br /><a class="internal" href="https://zhuanlan.zhihu.com/p/662024086">刘聪NLP：大模型微调技巧-在Embedding上加入噪音提高指令微调效果</a><br /><a class="internal" href="https://zhuanlan.zhihu.com/p/658392609">刘聪NLP：通义千问-Qwen技术报告细节分享</a><br /><a class="internal" href="https://zhuanlan.zhihu.com/p/658128530">刘聪NLP：如何从数据集中自动识别高质量的指令数据-IFD指标的使用</a><br /><a class="internal" href="https://zhuanlan.zhihu.com/p/656570703">刘聪NLP：BaiChuan2技术报告细节分享&amp;个人想法</a><br /><a class="internal" href="https://zhuanlan.zhihu.com/p/648798461">刘聪NLP：领域大模型-训练Trick&amp;落地思考</a><br /><a class="internal" href="https://zhuanlan.zhihu.com/p/648327222">刘聪NLP：千Star-大模型LLM微调项目-更新</a><br /><a class="internal" href="https://zhuanlan.zhihu.com/p/644671690">刘聪NLP：Llama2技术细节&amp;开源影响</a><br /><a class="internal" href="https://zhuanlan.zhihu.com/p/642611747">刘聪NLP：垂直领域大模型的一些思考及开源模型汇总</a><br /><a class="internal" href="https://zhuanlan.zhihu.com/p/642117003">刘聪NLP：如何评估大模型-LLMs的好坏？</a><br /><a class="internal" href="https://zhuanlan.zhihu.com/p/636488690">刘聪NLP：大模型流水线并行（Pipeline）实战</a><br /><a class="internal" href="https://zhuanlan.zhihu.com/p/630265131">刘聪NLP：支持多模态的ChatGLM模型-VisualGLM-6B</a><br /><a class="internal" href="https://zhuanlan.zhihu.com/p/625934485">刘聪NLP：大模型时代-不进则退</a><br /><a class="internal" href="https://zhuanlan.zhihu.com/p/620885226">刘聪NLP：大模型LLM-微调经验分享&amp;总结</a><br /><a class="internal" href="https://zhuanlan.zhihu.com/p/605331104">刘聪NLP：ChatGPT-所见、所闻、所感</a><br /><a class="internal" href="https://zhuanlan.zhihu.com/p/527366495">刘聪NLP：ACL2022 | DCSR：一种面向开放域段落检索的句子感知的对比学习方法</a><br /><a class="internal" href="https://zhuanlan.zhihu.com/p/523865674">刘聪NLP：ACL2022 | NoisyTune：微调前加入少量噪音可能会有意想不到的效果</a><br /><a class="internal" href="https://zhuanlan.zhihu.com/p/518146549">刘聪NLP：总结|Prompt在NER场景的应用</a><br /><a class="internal" href="https://zhuanlan.zhihu.com/p/509647368">刘聪NLP：PERT：一种基于乱序语言模型的预训练模型</a><br /><a class="internal" href="https://zhuanlan.zhihu.com/p/406512290">刘聪NLP：常用预训练语言模型（PTMs）总结</a></p>
]]></content:encoded>
<pubDate>Thu, 11 Apr 2024 09:55:32 GMT</pubDate>
</item>
<item>
<title>凡心回答了问题: 为什么美国一次又一次引领创新领域的革命？</title>
<link>https://www.zhihu.com/question/644674017/answer/3417085368</link>
<guid>https://www.zhihu.com/question/644674017/answer/3417085368</guid>
<content:encoded><![CDATA[
<p>就让他们领先嘛，为什么啥事儿非要争先呢，就你们喜欢卷。让别人趟坑嘛，等他们从坑里爬出来了，我们再抄，哦不，再学习嘛，这样可以少走好多弯路，避免浪费，大不了晚二十年进入供铲注仪嘛！对不对啊，同志们？</p><p>哦，对了，刚刚恰巧看到这篇文章，也不知道哪个大傻子写的：</p><h2>未有天才之前</h2><p>我自己觉得我的讲话不能使诸君有益或者有趣，因为我实在不知道什么事，但推托拖延得太长久了，所以终于不能不到这里来说几句。</p><p>我看现在许多人对于文艺界的要求的呼声之中，要求天才的产生也可以算是很盛大的了，这显然可以反证两件事：一是中国现在没有一个天才，二是大家对于现在的艺术的厌薄。天才究竟有没有？也许有着罢，然而我们和别人都没有见。倘使据了见闻，就可以说没有；不但天才，还有天才得以生长的民众。</p><p>天才并不是自生自长在深林荒野里的怪物，是由可以使天才生长的民众产生，长育出来的，所以没有这种民众，就没有天才。有一回拿破仑过Alps山，说，“我比Alps山还要高！”这何等英伟，然而不要忘记他后面跟着许多兵；倘没有兵，那只有被山那面的敌人捉住或者赶回，他的举动，言语，都离了英雄的界线，要归入疯子一类了。所以我想，在要求天才的产生之前，应该先要求可以使天才生长的民众。——譬如想有乔木，想看好花，一定要有好土；没有土，便没有花木了；所以土实在较花木还重要。花木非有土不可，正同拿破仑非有好兵不可一样。</p><p>然而现在社会上的论调和趋势，一面固然要求天才，一面却要他灭亡，连预备的土也想扫尽。举出几样来说：</p><p>其一说是“整理国故”。自从新思潮来到中国以后，其实何尝有力，而一群老头子，还有少年，却已丧魂失魄的来讲国故了。他们说，“中国自有许多好东西，都不整理保存，倒去求新，正如放弃祖宗遗产一样不肖。”抬出祖宗来说法，那自然是极威严的，然而我总不信在旧马褂未曾洗净叠好之前，便不能做一件新马褂。就现状而言，做事本来还随各人的自便，老先生要整理国故，当然不妨去埋在南窗下读死书，至于青年，却自有他们的活学问和新艺术，各干各事，也还没有大妨害的，但若拿了这面旗子来号召，那就是要中国永远与世界隔绝了。倘以为大家非此不可，那更是荒谬绝伦！我们和古董商人谈天，他自然总称赞他的古董如何好，然而他决不痛骂画家，农夫，工匠等类，说是忘记了祖宗：他实在比许多国学家聪明得远。</p><p>其一是“崇拜创作”。从表面上看来，似乎这和要求天才的步调很相合，其实不然，那精神中，很含有排斥外来思想、异域情调的分子，所以也就是可以使中国和世界潮流隔绝的。许多人对于托尔斯泰，屠格涅夫，陀思妥夫斯基的名字，已经厌听了，然而他们的著作，为什么译到中国来？眼光囚在一国里，听谈彼得和约翰就生厌，定须张三李四才行，于是创作家出来了，从实说，好的也离不了刺取点外国作品的技术和神情，文笔或者漂亮，思想往往赶不上翻译品，甚者不宁加上些传统思想，使他适合于中国人的老脾气，而读者却已为他所牢笼，于是眼界便渐渐地狭小，几乎要缩进旧圈套里去。作者和读者互相为因果，排斥异流，抬上国粹，哪里会有天才产生？即使产生了，也是活不下去的。</p><p>这样的风气的民众是灰尘，不是泥土，在他这里长不出好花和乔木来！</p><p>还有一样是恶意的批评。大家的要求批评家的出现，也由来已久了，到目下就出了许多批评家。可惜他们之中很有不少是不平家，不象批评家，作品才到面前，便恨恨地磨墨立刻写出很高明的结论道，“唉，幼稚得很。中国要天才！”到后来，连并非批评家也这样叫喊了，他是听来的。其实即使天才，在生下来的时候的第一声啼哭，也和平常的儿童的一样，决不会就是一首好诗。因为幼稚，当头加以戕贼，也可以萎死的。我亲见几个作者，都被他们骂得寒噤了。那些作者大约自然不是天才，然而我的希望是便是常人也留着。</p><p>恶意的批评家在嫩苗的地上驰马，那当然是十分快意的事；然而遭殃的是嫩苗——平常的苗和天才的苗。幼稚对于老成，有如孩子对于老人，决没有什么耻辱；作品也一样，起初幼稚，不算耻辱的。因为倘不遭了戕贼，他就会生长，成熟，老成；独有老衰和腐败，倒是无药可救的事！我以为幼稚的人，或者老大的人，如有幼稚的心，就说幼稚的话只为自己要说而说，说出之后，至多到印出之后，自己的事就完了，对于无论打着什么旗子的批评都可以置之不理的！</p><p>就是在座的诸君，料来也十之九愿有天才的产生罢，然而情形是这样，不便产生天才难，单是有培养天才的泥土也难。我想，天才大半是天赋的；独有这培养天才的泥土，似乎大家都可以做。做土的功效，比要求天才还切近；否则，纵有成千成百的天才，也因为没有泥土，不能发达，要像一碟子绿豆芽。</p><p>做土要扩大了精神，就有收纳新潮，脱离旧套，能够容纳，了解那将产生的天才；又要不怕做小事业，就是能创作的自然是创作，否则翻译，介绍，欣赏，读，看，消闲都可以。以文艺来消闲，说来似乎有些可笑，但究竟较胜于戕贼也。</p><p>泥土和天才比，当然是不足齿数的，然不是坚苦卓绝者，也怕不容易做；不过事在人为，比空等天赋的天才有把握。这一点，是泥土的伟大的地方，也是反有大希望的地方。而且也有报酬，譬如好花从泥土里出来，看的人固然欣然的赏鉴，泥土也可以欣然的赏鉴，正不必花卉自身，这才心旷神怡的——假如当作泥土也有灵魂的说。</p>
]]></content:encoded>
<pubDate>Sun, 03 Mar 2024 08:31:25 GMT</pubDate>
</item>
<item>
<title>凡心赞同了回答: 为什么很多聪明的人，却一辈子在社会最底层？</title>
<link>https://www.zhihu.com/question/314284424/answer/2961133505</link>
<guid>https://www.zhihu.com/question/314284424/answer/2961133505</guid>
<content:encoded><![CDATA[
<p>有段时间，网上盛传张亮和杨国福两大麻辣烫巨头的创始人是亲戚，张亮原本在舅舅杨国福的店里打工，学到技术以后出去单干，创立了张亮麻辣烫。</p><p>这个说法后来被张亮亲自辟谣了，两人确实有一些关系，但杨国福并不是他的舅舅，而是没有血缘关系的“姑家表姐夫”，并且在事业上也没有什么交集，但做麻辣烫确实是受杨国福的启发。</p><p>这事儿有意思的一点在于，杨国福在事业上并没有提携过张亮，从公开内容分析，两人大概率是属于没有走动的远房亲戚，但张亮就因为“姑家表姐夫”的成功示范，从一家苍蝇馆子开始做起，创立了属于自己的事业。</p><p>这个示范效应的威力还远不止如此，在杨国福和张亮的带动下，两人的老家哈尔滨宾县，发展成了麻辣烫之都，诞生了大大小小几十个麻辣烫品牌，大街上随便一个不起眼的门店，都有可能是连锁品牌的发源地。</p><p>在宾县麻辣烫行业最火热的时候，宾县街头任何一个野心勃勃的年轻人，今天可能还是麻辣烫的消费者，明天就可能进入麻辣烫行业， 从而改变自己的命运。做出这种选择的年轻人当然是聪明的，但客观来说，这种改变人生的选择，更多的是因为他生在宾县，近距离看见了成功。</p><p>因为人们对“<b>远方的成功</b>”往往是迟钝的，但对“<b>附近的成功</b>”却非常敏感，比如杭州那个长的像外星人的小个子成功了，你不会起心动念，但要是隔壁二傻子做成了一件事，你自然会觉得“<b>我也可以</b>”。</p><p>所以，很多产业都有地域上的聚集性，比如曹县的汉服，灌云县的内衣、昌乐县的吉他，最初可能是某个当地人的天才或意外（后者的可能性更大），在这个行业获得了成功，然后吸引了更多人的加入，最后形成了产业集群。如果你生在某个产业集中地，那么选择进入这个产业就是你的“<b>决策舒适区</b>”，就像你生在山东曹县，就比出生在河南驻马店更有可能去做汉服生意。</p><p>人不能想象自己没见过的东西，也自然难以跳脱环境的桎梏去做人生规划。</p><p>当宾县的年轻人选择麻辣烫的时候，与此同时，沈阳可能也有一个聪明、勤劳、有野心，愿意从小生意做起的青年，但他的选择大概率是卖烤鸡架，几年以后，宾县的年轻人可能已经开出了连锁店，沈阳的年轻人却只能在深夜收摊以后，在网上发出灵魂一问“为什么我这么聪明，却始终在社会底层？”</p><p>人的命运本质上是“<b>生活环境的延展</b>”，几个初始参数的不同，最终的结果可能天差地别。</p><p>B站有一个叫“非洲飞哥”的up主，80后、农村人、只上过大专，底层打工人专属配置，但飞哥的命运在婚后第三年改变了，这一年飞哥有了女儿，养家的重担让他极其渴望赚钱，飞哥的原话是“前后一年都在想怎么挣到钱，压力特别大，越想越慌”。</p><p>很多底层年轻人都会经历这样的阶段，也许是因为像飞哥那样要养家，也许是因为极度渴望自我实现，但局限于认知和人脉，他们往往会尝试一个被做烂的生意，或者学一门存在了几十年的手艺，这样的选择自然不会改变一个人的命运。</p><p>但飞哥最后的选择是去非洲打工，因为他有一个在非洲务工超过十年的父亲。</p><p>接下来就是一个很俗套的成功故事，飞哥先是从普通工人做起，在非洲熬了几年之后，发现了当地运输业的商机，于是跟别人合伙搞运输，一年就挣到了一套学区房的钱，有了钱就有了时间，闲暇之余拍了一些展现非洲风土人情的视频，又赶上了短视频的风口。</p><p>看过飞哥视频的人，恐怕都不能否认飞哥是个聪明人，但飞哥这种程度的聪明，任何一个村子都能找到十个，但“去非洲打工”这个选项，根本不会出现在他们的选择列表里面，因为非洲距离他们的生活太遥远了，不但是地理位置上的遥远，更是人际关系的遥远。所以不论非洲怎样“<b>广阔天地，大有作为</b>”，跟他们都是没有关系的。</p><p>社会学有一个“<b>六度分隔理论</b>”，即：你和任何一个陌生人之间所间隔的人不会超过六个，也就是最多通过六个人你就能够认识任何一个陌生人。在六度分隔的基础上，有学者提出了三度分隔理论，既：相距三度之内是强连接，强连接可以引发人的行为。相距超过三度是弱连接，而弱连接只能传递信息。</p><p>从这个角度来说，不论是飞哥因为父亲的缘故去非洲，还是张亮因为“姑家表姐夫”的启发做麻辣烫，本质上都是“<b>强连接</b>”引发的选择。</p><p>所以，一个碌碌无为的中年人回溯过去二十年，也不必为自己马不停蹄地错过了各种机会而感到懊悔。对于大多数人来说，不论是外贸的火爆，还是互联网的发展，都只是“<b>远方传递过来的信息</b>”，而不是“<b>眼前可供选择的选项</b>”，抓不住这些机会是再正常不过的事情。</p><p>尤其是青年时期认知还不成熟的时候，在决定命运的几个关键节点，那条通往成功的路没有出现在你的选择列表里，那么泯然众人是大概率会发生事。</p><p>还有一种更坏的情况，就是被周围信赖的人提供了错误的建议。<b>很多事情年纪到了才能深刻理解它，但又不得不在很年轻的时候做出决定</b>，以至于很多出身底层的年轻人，<b>被认知低下的人给出的建议，草草决定了命运的走向。</b></p><p>比如刘强东就是这样，东哥是宿迁的高考状元，本来清华北大随便进，但高中老师告诉他，如果想要回家乡当县长，带领家乡人民致富，应该选择人民大学社会学系。不得不说东哥高中老师的认知相当“<b>朴素</b>”，想当然地认为人大社会学系是离人民公仆最近的专业。</p><p>然而，听从了老师建议的东哥，进大学第一天梦想就破灭了，学长告诉他人大毕业是不能直接当官的，社会学系更是就业率倒数第二的专业。</p><p>一个年轻人，即使拥有能考进清北的智商，但在青年时期对世界缺乏认知的时候，被信赖的人提供了错误的建议，那么这个坑也是很难绕开的。</p><p>当然，东哥后来努力从“坑”里爬了出来，用另一种方式为家乡做了贡献。不过这个过程其实也很值得分析一下。</p><p>大家都知道被坑的东哥后来自学编程，给别人开发软件赚到了一些钱，大四的时候拿着自己的积蓄，又从父母亲戚那里借了一些钱去开了饭店，饭店虽然赔钱了，但也为后来创业积累了经验。</p><p>对于东哥这种学霸来说，自学编程是很正常的，做生意也算不上奇怪，但在大学还没毕业的时候，就敢借钱去开饭店，这对于一个90年代的农村做题家而言，人生道路的切换也未免过于“<b>丝滑</b>”了。尤其是东哥的父母居然愿意借钱给一个大学还没毕业的孩子去做生意，就更不符合常理了，大多数出身底层的父母的生存逻辑往往是求稳的，大学还没毕业就想着做生意，正常来说只会被斥责为“不务正业”。</p><p>但了解东哥的家族史以后，就会明白这种选择其实并不突兀，东哥的家族从曾祖父开始就在京杭运河一带做生意，祖父也有自己的船队，到父亲这一辈才开始没落，也就是说东哥家其实是有“<b>富人记忆</b>”的，也有做生意的积淀和传承，这一点从他父母在改开以后，就马上辞去工作去做跑船的个体户就可以看出来。因此东哥在掉坑里以后，把经商作为自我实现的另一条途径，可以说再正常不过了。</p><p>综上所述，我们可以明白两个残酷的事实：</p><p>1、大多数人成功的原因，无非是因为他能近距离接触到成功，知道成功长什么样子，然后才能模仿成功；</p><p>2、人的“主观能动性”是有限度的，你能做出的选择，实际上都是环境给你的，如果环境没有给你相关的选项，就很难找到通往成功的切入点；</p><p>所以，如果有什么人生建议的话，那就是：</p><p><b>1、家贫走他乡，离开当前生活的环境；</b></p><p><b>2、去看得见成功的地方，复制成功；</b></p><p>在这个所谓的信息时代，人们其实都是听闻成功，但看不到成功的全貌。不说远方的成功，就是附近的成功也很难看到，举个例子来说，你天天上班，但你知道你所在公司的供应链，产品细节，获客手段吗？</p><p>不知道也不能怪你，因为生意就是这么设计的，如果所有人都知道生意的全貌，那老板的生意就不要做了。</p><p>但在新兴行业，员工往往有全流程参与的机会，从而一窥生意的全貌，比如电商行业刚兴起的时候，电商运营离职单干的概率特别高，就是因为运营这个职位清晰地知道，从选品到获客的所有环节，所以后来做电商的老板，哪怕多招一些人，也要尽量拆分这个职位。</p><p>如果你还年轻，在面对离自己生活环境很远的新兴事物的时候，不要在心理上隔绝它，不妨大胆一些，让它成为自己的选项，给自己一些犯错的机会，也许能在这个固化的时代，找到突破阶层的缝隙。</p><h3>弄了个公众号：<b>奇迹年</b>。</h3><p><b>分享一些我比你先知道的事。</b></p><p><b>很多事情，很多道理，年纪到了都能会都能懂，但社会比的是谁先懂，早懂和晚懂是不一样的命运。</b></p><p><b>以下是我的其它回答，希望能对你有帮助。</b></p><p><a class="internal" href="https://www.zhihu.com/question/487990120/answer/2295005139">如何从底层杀出？</a>-3.2W赞</p><p><a class="internal" href="https://www.zhihu.com/answer/2693655379">人是怎么废掉的？-1.8万赞</a></p>
]]></content:encoded>
<pubDate>Fri, 02 Feb 2024 09:51:07 GMT</pubDate>
</item>
<item>
<title>凡心收藏了文章: [新西兰远程工作] Web 高级全栈/Node.js/React.js/TypeScript/Ruby on Rails，工作地点随意，无加班 996</title>
<link>https://zhuanlan.zhihu.com/p/480567067</link>
<guid>https://zhuanlan.zhihu.com/p/480567067</guid>
<content:encoded><![CDATA[
<p>4.14 update: 已经招到合适的小伙伴，明年见。</p><p>===</p><p>有没有朋友看着眼熟？一年过去了，我又来招人了，老帖子：<a class=" external" href="https://www.v2ex.com/t/763563" rel="nofollow noreferrer" target="_blank"><span class="invisible">https://www.</span><span class="visible">v2ex.com/t/763563</span><span class="invisible"></span></a></p><h2>关于我们</h2><p>我们是一家在新西兰的公司，主要产品是一个 SaaS 平台（官网：<a class=" wrap external" href="https://www.mycsp.io/" rel="nofollow noreferrer" target="_blank">MyCSP | The all-in-one platform for Microsoft CSP!</a>），方便用户管理和购买微软产品以及提供一些云服务，目前主要业务在新西兰地区。在前年疫情期间，公司转型全员远程办公，同时我们在扩张业务到全球其他地方，所以目前开始进行海外开发人员招聘。</p><h2>工作要求</h2><p>简单的说：JS 全栈开发。</p><ul><li>精通 JS 和 Node.js （3 年+开发经验）</li><li>熟悉 TypeScript</li><li>熟悉 React.js 以及 Antd</li><li>认识或者用过 PostgreSQL 、Redis 等</li><li>认识或者用过 DevOps 、CICD 、K8S 等最好是 Azure 平台（有做运维的，不需要做运维，但是了解更好）</li><li>能用英文进行沟通面试，具备一定的听说读写能力，因为是英文的工作环境</li><li>了解或者有 Nest.js 经验，全新 Node 应用将会基于这个框架</li><li>会用 Ruby on Rails 优先，没有经验愿意学也可以，也不算太难</li></ul><h2>工作模式</h2><ul><li>全远程办公，不限制你的位置，比较灵活自由。每周最多工作 40 小时，没有加班和 996，多陪陪家人。适合想要自由生活、逃离北上广、喜欢到处旅游的人群。今年老板刚回乌克兰。。。</li><li>基本不开会。全部例会只有每天不到 15 分钟的 Standup 和每两周一次大概 2 小时的 Sprint Planning 。</li><li>合同工，至少签约两周起，优先选择长期合作，因为我们是一个长期维护的 SaaS 的平台，不是短期的小项目，因此倾向于每周 40 小时的全职远程办公，只不过以合同工的形式签约。提交工作时间表的方式统计工时。</li></ul><h2>薪资待遇和招聘流程</h2><p>去年我们第一次招聘不知道薪资情况所以原贴没说，后来招到了两位靠谱小伙伴，今年确认了下薪资范围大概在税前时薪 160 - 240 人民币。按照一周 40 小时，一年 52 周算，税前月收入平均 27733 - 41600 之间。公司用纽币跨境汇款，自己结汇，然后需要自己负责处理税务、社保、医保等等，然后是合同工的合同，按照时薪付费，所以请假是没有薪资的，可以自己估算一下。</p><p>简单的说，对于一线城市中大厂的薪资待遇的程序员应该没有吸引力。但是对于在二三线老家或者希望有 Work life balance 的程序员来说，还是比较好的。</p><p>如果有兴趣，请按照下面格式要求替换信息并且发送简历，提醒一下，这也是一个筛选方式，去年没有仔细看按照要求发的我都筛掉了。</p><p>格式：</p><ul><li>Subject: Hi there, I am [your English name], this is my resume</li><li>Body: 英文版的目前情况简介，包括现在是否在工作，以及其他的合作方式和每周工作时间，期望薪资（ RMB 即可），以及你想要问的问题等等</li><li>Attachment: PDF 或者 Word 文档，英文简历</li><li>发送到 rain.lei[at]<a class=" wrap external" href="http://umbrellar.com/" rel="nofollow noreferrer" target="_blank">umbrellar.com</a></li></ul><p>早发早安排面试，面试也是远程，由于老板目前人在乌克兰有时差，所以会在国内中午附近面试，需要露脸，具体时间、方式再协商。</p><h2>常见问题</h2><h2>可否先兼职一段时间再决定是否签长期合同？</h2><p>可以。我也跟老板反馈了一下，很多朋友可能在职有想法，但是直接辞职签约也有很大的风险和担心，所以最好的方式是可以先利用晚上和周末能兼职一下，比如一周先只做 20 个小时左右，然后磨合一段时间双方再做决定。</p><p>老板表示只要是人才，都可以协商，关键看水平。</p><h2>有时差怎么办？</h2><p>我们目前是一个比较小的团队，所以比较灵活，我们可以调整例会的时间来符合大家的作息。</p><p>本信息有效期暂定 2022 年 5 月 20 日，目前刚刚启动面试流程，疯狂收简历安排面试中，有兴趣的朋友赶紧准备投递，名额有限，招满就没了。</p>
]]></content:encoded>
<pubDate>Fri, 15 Dec 2023 15:30:35 GMT</pubDate>
</item>
<item>
<title>凡心赞同了回答: 女生为什么总是被造黄谣?</title>
<link>https://www.zhihu.com/question/578342113/answer/2944123469</link>
<guid>https://www.zhihu.com/question/578342113/answer/2944123469</guid>
<content:encoded><![CDATA[
<p>最近这个黄谣。前几天有个广州地铁女子露出的瓜，在各种群里面疯传，都在调侃这是“主人的任务”。</p><figure><img class="origin_image zh-lightbox-thumb lazy" src="https://pic1.zhimg.com/v2-55d4c68f844bdf85080bc9c4c5cf1448_1440w.jpg" /></figure><p class="ztext-empty-paragraph"><br /></p><figure><img class="content_image lazy" src="https://pic2.zhimg.com/v2-62a00f9245605010b0940722ce692afd_1440w.jpg" /></figure><p class="ztext-empty-paragraph"><br /></p><figure><img class="origin_image zh-lightbox-thumb lazy" src="https://pic4.zhimg.com/v2-abfb41adadd9dfa85017538476a66b2f_1440w.jpg" /></figure><p class="ztext-empty-paragraph"><br /></p><figure><img class="origin_image zh-lightbox-thumb lazy" src="https://pic2.zhimg.com/v2-0d66edc5ac6b4e9cdd4d9747e5f6ae05_1440w.jpg" /></figure><p>然而实际上呢？</p><p>人家是小红书博主，去年7月就发了的照片。</p><figure><img class="origin_image zh-lightbox-thumb lazy" src="https://pic1.zhimg.com/v2-ccad614f7acc3579a23e7cbe6ec158c0_1440w.jpg" /></figure><p>让人恶意p成全裸，导致好多人一看到她照片就说，“这不是地铁不穿衣服的那个吗”。</p><p>真的很无语。</p>
]]></content:encoded>
<pubDate>Thu, 14 Dec 2023 08:59:00 GMT</pubDate>
</item>
</channel>
</rss>